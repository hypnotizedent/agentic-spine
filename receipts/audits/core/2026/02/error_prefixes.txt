ops/commands/start.sh:113:  # GitHub is mirror-only; divergence is WARN-only (D62 reports it).
bin/generate-scaffold.sh:156:# FIRST ACTIONS (must pass or STOP):
bin/generate-scaffold.sh:162:# If ANY fail: STOP. Fix spine core before doing any other work.
ops/plugins/session/bin/session-start:81:  *) echo "ERROR: unknown lane '$LANE'. Use: scan | apply | coordinator"; exit 1 ;;
ops/plugins/session/bin/session-start:158:    echo "WARNING: apply-owner.lock already held by: $existing"
surfaces/verify/d115-ha-ssot-baseline-freshness.sh:10:ERRORS=0
surfaces/verify/d115-ha-ssot-baseline-freshness.sh:11:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d115-ha-ssot-baseline-freshness.sh:19:  echo "D115 FAIL: $ERRORS check(s) failed"
surfaces/verify/d115-ha-ssot-baseline-freshness.sh:77:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d115-ha-ssot-baseline-freshness.sh:78:  echo "D115 FAIL: $ERRORS check(s) failed"
ops/commands/cap.sh:96:        echo "ERROR: yq required for YAML parsing"
ops/commands/cap.sh:118:        echo "ERROR: Unknown capability: $name"
ops/commands/cap.sh:146:        echo "ERROR: Unknown capability: $name"
ops/commands/cap.sh:459:                echo "ERROR: requires cycle detected: ${name} -> ${req}"
ops/commands/cap.sh:488:        echo "STOP: precondition failed: ${precond_name} (exit=$precond_rc)" | tee "$output_file" >/dev/null
ops/commands/cap.sh:512:            echo "POST-ACTION WARN: ${post_action} failed (non-blocking)"
ops/commands/ai.sh:27:  echo "ERROR: REPO_ROOT not set and not in a git repo." >&2
ops/commands/ai.sh:39:die() { echo -e "${RED}ERROR:${NC} $1" >&2; exit 1; }
ops/commands/ai.sh:126:      echo "# WARNING: $path not found" >&2
surfaces/verify/github-actions-gate.sh:10:  echo "WARN: github-actions-status not present (capability not yet merged)"
ops/commands/close.sh:86:      echo "  WARN: branch exists but NOT merged into main — keeping: $BRANCH_NAME"
surfaces/verify/d89-rag-reindex-quality-contract-lock.sh:27:ERRORS=0
surfaces/verify/d89-rag-reindex-quality-contract-lock.sh:28:err() { echo "  $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d89-rag-reindex-quality-contract-lock.sh:83:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d89-rag-reindex-quality-contract-lock.sh:84:  fail "$ERRORS governance errors found"
ops/commands/hooks.sh:38:      echo "status: WARN (core.hooksPath is not .githooks)"
ops/commands/hooks.sh:43:      echo "pre-commit: WARN ($hook_file missing or not executable)"
ops/commands/hooks.sh:56:      echo "pre-commit: WARN (missing or not executable): $hook_file"
ops/commands/hooks.sh:65:    echo "ERROR: unknown subcommand: $cmd" >&2
surfaces/verify/d113-coordinator-health-probe.sh:56:  RESULTS+=("Z2M: WARN (state=$z2m_state)")
surfaces/verify/d113-coordinator-health-probe.sh:68:    RESULTS+=("SLZB-06MU ethernet: WARN (state=$slzb_state)")
surfaces/verify/d113-coordinator-health-probe.sh:85:    RESULTS+=("TubesZB: WARN (ESPHome unavailable)")
surfaces/verify/d106-media-port-collision-lock.sh:10:ERRORS=0
surfaces/verify/d106-media-port-collision-lock.sh:11:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d106-media-port-collision-lock.sh:50:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d106-media-port-collision-lock.sh:51:  echo "D106 FAIL: $ERRORS check(s) failed"
surfaces/verify/ledger-reconcile.sh:11:# Outputs: PASS (all clean) or WARN with specific counts for each issue
surfaces/verify/ledger-reconcile.sh:38:    echo "WARN: Loop scopes directory not found: $LOOP_SCOPES_DIR"
surfaces/verify/ledger-reconcile.sh:43:    echo "WARN: Ledger file not found: $LEDGER_FILE"
surfaces/verify/ledger-reconcile.sh:48:    echo "WARN: Receipts directory not found: $RECEIPTS_DIR"
surfaces/verify/ledger-reconcile.sh:53:    echo "WARN: Inbox directory not found: $INBOX_DIR"
surfaces/verify/ledger-reconcile.sh:132:    echo "WARN - Ledger reconciliation issues:"
surfaces/verify/d55-secrets-runtime-readiness-lock.sh:6:# Groups secrets readiness checks behind one high-signal STOP.
ops/commands/preflight.sh:57:    parity_status="WARN"
ops/commands/preflight.sh:58:    parity_detail="WARN: D62 not present/executable"
ops/commands/preflight.sh:73:    worktree_status="WARN"
ops/commands/preflight.sh:74:    worktree_detail="WARN: D48 not present/executable"
ops/commands/preflight.sh:123:      echo "  hooks: WARN (core.hooksPath is not .githooks)"
ops/commands/preflight.sh:129:	        echo "  hooks: WARN (.githooks/pre-commit missing or not executable)"
ops/commands/preflight.sh:153:      echo "  pack: WARN (selected domain brief unavailable; non-blocking)"
ops/commands/preflight.sh:160:    echo "  available: WARN (could not load domain registry)"
ops/commands/preflight.sh:165:  echo "  available: WARN (missing certifier executable: $DRIFT_CERTIFIER)"
ops/commands/preflight.sh:171:  cat <<'STOP'
ops/commands/preflight.sh:173:║ STOP: PREFLIGHT BLOCKERS DETECTED                         ║
ops/commands/preflight.sh:177:STOP
surfaces/verify/backup_audit.sh:25:    echo "ERROR: Cannot connect to $PVE_HOST"
ops/plugins/media/bin/media-backup-create:156:ERRORS=0
ops/plugins/media/bin/media-backup-create:159:  backup_vm_configs "download-stack" "${DOWNLOAD_CONFIGS[@]}" || ERRORS=$((ERRORS + 1))
ops/plugins/media/bin/media-backup-create:163:  backup_vm_configs "streaming-stack" "${STREAMING_CONFIGS[@]}" || ERRORS=$((ERRORS + 1))
ops/plugins/media/bin/media-backup-create:166:if [[ $ERRORS -gt 0 ]]; then
ops/plugins/media/bin/media-backup-create:168:  echo "FAIL: $ERRORS VM(s) had backup errors"
surfaces/verify/d62-git-remote-parity-lock.sh:8:#   canonical work. If github diverges, we WARN (no-fail) so the mirror can be
surfaces/verify/d62-git-remote-parity-lock.sh:9:#   repaired, but we do not STOP.
surfaces/verify/d62-git-remote-parity-lock.sh:14:#   - If github remote exists, best-effort fetch and WARN if it diverges.
surfaces/verify/d62-git-remote-parity-lock.sh:22:warn() { echo "WARN: $*" >&2; }
surfaces/verify/d62-git-remote-parity-lock.sh:48:# GitHub mirror parity is best-effort (WARN-only).
ops/plugins/media/bin/media-stack-restart:158:ERRORS=0
ops/plugins/media/bin/media-stack-restart:164:  restart_vm_stack "$vm" || ERRORS=$((ERRORS + 1))
ops/plugins/media/bin/media-stack-restart:167:if [[ $ERRORS -gt 0 ]]; then
ops/plugins/media/bin/media-stack-restart:169:  echo "FAIL: $ERRORS VM(s) had errors"
surfaces/verify/d56-agent-entry-surface-lock.sh:6:# Groups the common "agent entry/read surfaces out of sync" checks behind one STOP.
ops/commands/verify.sh:8:  echo "ERROR: verify surface missing: $V" >&2
ops/plugins/media/bin/media-health-check:64:  echo "STOP: binding not found: $BINDING" >&2
surfaces/verify/d93-tenant-storage-boundary-lock.sh:9:ERRORS=0
surfaces/verify/d93-tenant-storage-boundary-lock.sh:10:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d93-tenant-storage-boundary-lock.sh:18:  echo "D93 FAIL: $ERRORS check(s) failed"
surfaces/verify/d93-tenant-storage-boundary-lock.sh:77:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d93-tenant-storage-boundary-lock.sh:78:  echo "D93 FAIL: $ERRORS check(s) failed"
ops/plugins/media/bin/media-metrics-today:78:  echo "STOP: RADARR_API_KEY not present in injected environment." >&2
ops/plugins/media/bin/media-metrics-today:85:  echo "STOP: radarr endpoint missing in binding: $SERVICES_BINDING" >&2
ops/plugins/media/bin/media-metrics-today:102:  echo "STOP: radarr ping failed: ${RADARR_BASE}/ping" >&2
ops/plugins/media/bin/media-metrics-today:121:    print(f"STOP: {msg}", file=_sys.stderr)
ops/commands/ready.sh:73:  echo "STOP (exit 2): Infisical auth is NOT hydrated in this terminal."
surfaces/verify/d109-media-compose-config-match-lock.sh:11:ERRORS=0
surfaces/verify/d109-media-compose-config-match-lock.sh:12:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d109-media-compose-config-match-lock.sh:84:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d109-media-compose-config-match-lock.sh:85:  echo "D109 FAIL: $ERRORS check(s) failed"
ops/commands/loops.sh:161:        echo "ERROR: Scope file not found: $scope_file" >&2
ops/commands/loops.sh:177:        echo "ERROR: Scope file not found: $scope_file" >&2
surfaces/verify/d64-git-remote-authority-warn.sh:5:# D64: Git remote authority WARN (no-fail)
surfaces/verify/d64-git-remote-authority-warn.sh:10:# This script MUST exit 0 (WARN-only).
surfaces/verify/d64-git-remote-authority-warn.sh:16:MAX_WARN="${GIT_AUTHORITY_WARN_MAX:-5}"
surfaces/verify/d64-git-remote-authority-warn.sh:32:    echo "WARN: GitHub-authored merge detected: ${short} ${subject} (${date})"
surfaces/verify/d64-git-remote-authority-warn.sh:33:    echo "WARN: Policy: PRs/merges must happen on Gitea; see docs/governance/GIT_REMOTE_AUTHORITY.md"
surfaces/verify/d64-git-remote-authority-warn.sh:35:    if (( warned >= MAX_WARN )); then
ops/commands/pr.sh:97:  echo "STOP: missing remote 'origin' (required)" >&2
ops/commands/pr.sh:102:  echo "WARN: github forge selected; this bypasses Gitea CI"
ops/commands/pr.sh:107:    echo "WARN: github remote not configured; skipping mirror push"
surfaces/verify/replay-test.sh:114:        echo "ERROR: No baseline found. Run without --compare first."
surfaces/verify/d90-rag-reindex-runtime-quality-gate.sh:2:# TRIAGE: Run rag.reindex.remote.verify when session is STOPPED to catch false-green parity.
surfaces/verify/d90-rag-reindex-runtime-quality-gate.sh:5:# Enforces that a STOPPED reindex session has clean completion quality.
surfaces/verify/d90-rag-reindex-runtime-quality-gate.sh:13:# - If session is STOPPED:
surfaces/verify/d90-rag-reindex-runtime-quality-gate.sh:30:warn() { echo "D90 WARN: $*" >&2; }
surfaces/verify/d90-rag-reindex-runtime-quality-gate.sh:73:# Session is STOPPED - check quality gates
surfaces/verify/d90-rag-reindex-runtime-quality-gate.sh:74:ERRORS=0
surfaces/verify/d90-rag-reindex-runtime-quality-gate.sh:75:err() { echo "  ERROR: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d90-rag-reindex-runtime-quality-gate.sh:78:echo "Session '$TMUX_SESSION' is STOPPED — checking quality gates..."
surfaces/verify/d90-rag-reindex-runtime-quality-gate.sh:84:failed_uploads="$(ssh "${SSH_ARGS[@]}" "$TARGET" "awk -v s='$run_start_line' 'NR>=s && /ERROR: Upload failed/{c++} END{print c+0}' '$REMOTE_LOG' 2>/dev/null || echo 0")"
surfaces/verify/d90-rag-reindex-runtime-quality-gate.sh:129:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d90-rag-reindex-runtime-quality-gate.sh:130:  fail "$ERRORS quality gate(s) failed — run rag.reindex.remote.verify for details"
surfaces/verify/d91-aof-product-foundation-lock.sh:10:ERRORS=0
surfaces/verify/d91-aof-product-foundation-lock.sh:11:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d91-aof-product-foundation-lock.sh:124:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d91-aof-product-foundation-lock.sh:125:  echo "D91 FAIL: $ERRORS check(s) failed"
surfaces/verify/d110-media-ha-duplicate-audit-lock.sh:11:ERRORS=0
surfaces/verify/d110-media-ha-duplicate-audit-lock.sh:12:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d110-media-ha-duplicate-audit-lock.sh:14:warn() { echo "  WARN: $*" >&2; }
surfaces/verify/d110-media-ha-duplicate-audit-lock.sh:26:  echo "D110 WARN: ha.addons.yaml missing (run ha.addons.snapshot)"
surfaces/verify/d110-media-ha-duplicate-audit-lock.sh:55:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d110-media-ha-duplicate-audit-lock.sh:56:  echo "D110 FAIL: $ERRORS HA add-on overlap(s) require action"
surfaces/verify/d58-ssot-freshness-lock.sh:93:      echo "  WARN: $basename_doc (authoritative) missing last_verified" >&2
ops/plugins/media/bin/media-nfs-verify:141:ERRORS=0
ops/plugins/media/bin/media-nfs-verify:148:    ERRORS=$((ERRORS + 1))
ops/plugins/media/bin/media-nfs-verify:156:    ERRORS=$((ERRORS + 1))
ops/plugins/media/bin/media-nfs-verify:162:  if [[ "$ERRORS" -gt 0 ]]; then
ops/plugins/media/bin/media-nfs-verify:180:    --argjson errors "$ERRORS" \
ops/plugins/media/bin/media-nfs-verify:209:  if [[ $ERRORS -gt 0 ]]; then
ops/plugins/media/bin/media-nfs-verify:211:    echo "FAIL: $ERRORS NFS mount issue(s) detected"
ops/plugins/media/bin/media-nfs-verify:219:exit $ERRORS
surfaces/verify/d57-infra-identity-cohesion-lock.sh:6:# Groups infra placement/identity checks behind one STOP.
surfaces/verify/foundation-gate.sh:15:warn() { echo "WARN: $*"; }
surfaces/verify/d32-codex-instruction-source-lock.sh:60:  echo "D32 WARN: latest recorded session was seeded by legacy AGENTS (pre-cutover)" >&2
ops/plugins/ms-graph/bin/graph-token-exec:34:  ERROR="$(printf '%s' "$RESPONSE" | python3 -c 'import sys,json; d=json.load(sys.stdin); print(d.get("error_description", d.get("error","unknown")))' 2>/dev/null || echo "unknown")"
ops/plugins/ms-graph/bin/graph-token-exec:35:  echo "FAIL: Azure AD token acquisition failed: $ERROR" >&2
surfaces/verify/d66-mcp-parity-gate.sh:20:warn() { echo "D66 WARN: $*" >&2; }
surfaces/verify/d25-secrets-cli-canonical-lock.sh:10:#   - Exit 0 on PASS (may emit WARN lines).
surfaces/verify/d25-secrets-cli-canonical-lock.sh:17:warn() { echo "WARN $*"; }
surfaces/verify/d97-surface-readonly-contract-lock.sh:9:ERRORS=0
surfaces/verify/d97-surface-readonly-contract-lock.sh:10:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d97-surface-readonly-contract-lock.sh:19:  echo "D97 FAIL: $ERRORS check(s) failed"
surfaces/verify/d97-surface-readonly-contract-lock.sh:119:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d97-surface-readonly-contract-lock.sh:120:  echo "D97 FAIL: $ERRORS check(s) failed"
ops/plugins/share/bin/share-publish-preview:153:  echo "WARNING: $BLOCKED file(s) contain denylist patterns. Review before publishing."
surfaces/verify/d88-rag-remote-reindex-governance-lock.sh:22:STOP_SCRIPT="$ROOT/ops/plugins/rag/bin/rag-reindex-remote-stop"
surfaces/verify/d88-rag-remote-reindex-governance-lock.sh:31:ERRORS=0
surfaces/verify/d88-rag-remote-reindex-governance-lock.sh:32:err() { echo "  $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d88-rag-remote-reindex-governance-lock.sh:72:for s in "$START_SCRIPT" "$STATUS_SCRIPT" "$STOP_SCRIPT"; do
surfaces/verify/d88-rag-remote-reindex-governance-lock.sh:131:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d88-rag-remote-reindex-governance-lock.sh:132:  fail "$ERRORS governance errors found"
surfaces/verify/d83-proposal-queue-health-lock.sh:21:warn() { echo "  WARN: $1" >&2; }
surfaces/verify/d69-vm-creation-governance-lock.sh:25:warn() { echo "  WARN: $1" >&2; }
surfaces/verify/updates_verify.sh:13:warn() { echo "${RULE_PREFIX}-${1} WARN: ${2}"; }
surfaces/verify/backup_verify.sh:34:ERRORS=0
surfaces/verify/backup_verify.sh:35:WARNINGS=0
surfaces/verify/backup_verify.sh:64:            echo "WARNING: VM $VMID - Backup is ${AGE_HOURS}h old (max: ${MAX_AGE}h)"
surfaces/verify/backup_verify.sh:77:        ERRORS=$((ERRORS + 1))
surfaces/verify/backup_verify.sh:87:        ERRORS=$((ERRORS + 1))
surfaces/verify/backup_verify.sh:89:        WARNINGS=$((WARNINGS + 1))
surfaces/verify/backup_verify.sh:101:    echo "WARNING: Storage over 80% - consider pruning or expanding"
surfaces/verify/backup_verify.sh:102:    WARNINGS=$((WARNINGS + 1))
surfaces/verify/backup_verify.sh:107:echo "Errors: $ERRORS"
surfaces/verify/backup_verify.sh:108:echo "Warnings: $WARNINGS"
surfaces/verify/backup_verify.sh:110:if [ "$ERRORS" -gt 0 ]; then
surfaces/verify/backup_verify.sh:114:elif [ "$WARNINGS" -gt 0 ]; then
surfaces/verify/backup_verify.sh:116:    echo "RESULT: WARNING - Non-critical issues found"
surfaces/verify/d103-streamdeck-config-lock.sh:25:    echo "WARN: tracked config (sha:$TRACKED_SHA) differs from runtime (sha:$RUNTIME_SHA)"
surfaces/verify/d85-gate-registry-parity-lock.sh:20:ERRORS=0
surfaces/verify/d85-gate-registry-parity-lock.sh:21:err() { echo "  $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d85-gate-registry-parity-lock.sh:83:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d85-gate-registry-parity-lock.sh:84:  fail "$ERRORS parity errors found"
surfaces/verify/d36-legacy-exception-hygiene-lock.sh:75:WARN_THRESHOLD=$((48 * 60 * 60))  # 48 hours in seconds
surfaces/verify/d36-legacy-exception-hygiene-lock.sh:86:WARN_COUNT=0
surfaces/verify/d36-legacy-exception-hygiene-lock.sh:106:        echo "  WARN: cannot parse expires_at for $label: $expires_at"
surfaces/verify/d36-legacy-exception-hygiene-lock.sh:119:    if (( time_remaining < WARN_THRESHOLD )); then
surfaces/verify/d36-legacy-exception-hygiene-lock.sh:121:        echo "  WARN: exception '$label' expires in ${hours_remaining}h (at $expires_at)"
surfaces/verify/d36-legacy-exception-hygiene-lock.sh:122:        WARN_COUNT=$((WARN_COUNT + 1))
surfaces/verify/d36-legacy-exception-hygiene-lock.sh:131:if (( WARN_COUNT > 0 )); then
surfaces/verify/d36-legacy-exception-hygiene-lock.sh:132:    echo "D36 PASS: exception hygiene enforced (${WARN_COUNT} warning(s) - near expiry)"
surfaces/verify/d107-media-nfs-mount-lock.sh:11:ERRORS=0
surfaces/verify/d107-media-nfs-mount-lock.sh:12:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d107-media-nfs-mount-lock.sh:90:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d107-media-nfs-mount-lock.sh:91:  echo "D107 FAIL: $ERRORS check(s) failed"
surfaces/verify/drift-gate.sh:12:#   WARN_POLICY=advisory (default) — warnings reported, exit 0
surfaces/verify/drift-gate.sh:13:#   WARN_POLICY=strict             — warnings escalate to FAIL (exit 1)
surfaces/verify/drift-gate.sh:27:WARN_COUNT=0
surfaces/verify/drift-gate.sh:31:warn(){ echo "WARN $*"; WARN_COUNT=$((WARN_COUNT + 1)); }
surfaces/verify/drift-gate.sh:81:WARN_POLICY="${WARN_POLICY:-$RESOLVED_WARN_POLICY}"
surfaces/verify/drift-gate.sh:107:    # Preserve advisory WARN lines (if any), but drop PASS noise from scripts.
surfaces/verify/drift-gate.sh:108:    if grep -q '^WARN' "$tmp" 2>/dev/null; then
surfaces/verify/drift-gate.sh:109:      grep '^WARN' "$tmp" 2>/dev/null || true
surfaces/verify/drift-gate.sh:1185:if [[ "$WARN_POLICY" == "strict" && "$WARN_COUNT" -gt 0 ]]; then
surfaces/verify/drift-gate.sh:1189:  if [[ "$WARN_COUNT" -gt 0 ]]; then
surfaces/verify/drift-gate.sh:1190:    echo "DRIFT GATE: PASS ($WARN_COUNT warning(s) — review WARN lines above)"
surfaces/verify/drift-gate.sh:1197:if [[ "$WARN_COUNT" -gt 0 ]]; then
surfaces/verify/drift-gate.sh:1198:  echo "  WARNINGS: $WARN_COUNT gate(s) reported warnings (policy=$WARN_POLICY)"
surfaces/verify/d99-ha-token-freshness.sh:41:    echo "WARN: HA API returned unexpected HTTP $HTTP_CODE"
surfaces/verify/monitoring_verify.sh:72:    echo -e "${YELLOW}WARN:${NC} $id - incomplete endpoint entry"
surfaces/verify/d87-rag-workspace-contract-lock.sh:18:ERRORS=0
surfaces/verify/d87-rag-workspace-contract-lock.sh:19:err() { echo "  $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d87-rag-workspace-contract-lock.sh:95:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d87-rag-workspace-contract-lock.sh:96:  fail "$ERRORS contract parity errors found"
surfaces/verify/d52-udr6-gateway-assertion.sh:18:ERRORS=0
surfaces/verify/d52-udr6-gateway-assertion.sh:52:    ERRORS=$((ERRORS + 1))
surfaces/verify/d52-udr6-gateway-assertion.sh:57:    ERRORS=$((ERRORS + 1))
surfaces/verify/d52-udr6-gateway-assertion.sh:61:  ERRORS=$((ERRORS + 1))
surfaces/verify/d52-udr6-gateway-assertion.sh:71:    ERRORS=$((ERRORS + 1))
surfaces/verify/d52-udr6-gateway-assertion.sh:75:  ERRORS=$((ERRORS + 1))
surfaces/verify/d52-udr6-gateway-assertion.sh:85:    ERRORS=$((ERRORS + 1))
surfaces/verify/d52-udr6-gateway-assertion.sh:89:  ERRORS=$((ERRORS + 1))
surfaces/verify/d52-udr6-gateway-assertion.sh:97:    ERRORS=$((ERRORS + 1))
surfaces/verify/d52-udr6-gateway-assertion.sh:102:    ERRORS=$((ERRORS + 1))
surfaces/verify/d52-udr6-gateway-assertion.sh:107:    ERRORS=$((ERRORS + 1))
surfaces/verify/d52-udr6-gateway-assertion.sh:111:[[ "$ERRORS" -eq 0 ]] && exit 0 || exit 1
surfaces/verify/d86-vm-operating-profile-parity-lock.sh:24:ERRORS=0
surfaces/verify/d86-vm-operating-profile-parity-lock.sh:25:err() { echo "  $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d86-vm-operating-profile-parity-lock.sh:110:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d86-vm-operating-profile-parity-lock.sh:111:  fail "$ERRORS parity errors found"
ops/plugins/monolith/bin/monolith-git-status:19:  echo "ERROR: repo not found: $repo_dir" >&2
ops/plugins/monolith/bin/monolith-git-status:24:  echo "ERROR: not a git repo (missing .git): $repo_dir" >&2
surfaces/verify/d38-extraction-hygiene-lock.sh:67:    echo "  WARN: SERVICE_REGISTRY.yaml not found (skipping utility sprawl check)"
surfaces/verify/d38-extraction-hygiene-lock.sh:121:    echo "  WARN: STACK_REGISTRY.yaml not found (skipping stack binding check)"
ops/plugins/agent/bin/agent-route:13:  echo "ERROR: agents.registry.yaml not found at $REGISTRY" >&2
ops/plugins/agent/bin/agent-route:18:  echo "ERROR: yq is required but not found in PATH" >&2
ops/plugins/agent/bin/agent-route:75:  echo "ERROR: no agent found for input: $input" >&2
ops/plugins/agent/bin/agent-route:101:  echo "ERROR: agent $agent_id referenced in routing_rules but not found in agents[]" >&2
surfaces/verify/check-secret-expiry.sh:12:WARNING_THRESHOLD="${WARNING_THRESHOLD:-60}"
surfaces/verify/check-secret-expiry.sh:174:          elif [[ "$days" -ge "$WARNING_THRESHOLD" ]]; then
surfaces/verify/check-secret-expiry.sh:176:              echo -e "  ${YELLOW}WARNING${NC}: $key - $days days old"
surfaces/verify/check-secret-expiry.sh:200:    echo -e "Warning (>$WARNING_THRESHOLD days): ${YELLOW}$warning_count${NC}"
surfaces/verify/check-secret-expiry.sh:274:  echo "    \"warning\": $WARNING_THRESHOLD,"
surfaces/verify/check-secret-expiry.sh:313:  WARNING_THRESHOLD   Days before warning (default: 60)
surfaces/verify/check-secret-expiry.sh:327:  WARNING_THRESHOLD=30 check-secret-expiry.sh
surfaces/verify/secrets_verify.sh:78:        echo -e "${YELLOW}WARN:${NC} $project - missing project ID"
surfaces/verify/secrets_verify.sh:106:        echo -e "${YELLOW}WARN:${NC} Pattern '$pattern' found in files (review manually):"
surfaces/verify/secrets_verify.sh:118:    echo -e "${YELLOW}WARN:${NC} $SECRETS_FOUND pattern(s) need manual review"
surfaces/verify/d92-ha-config-version-control.sh:12:ERRORS=0
surfaces/verify/d92-ha-config-version-control.sh:13:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d92-ha-config-version-control.sh:19:  echo "D92 FAIL: $ERRORS check(s) failed"
surfaces/verify/d92-ha-config-version-control.sh:53:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d92-ha-config-version-control.sh:54:  echo "D92 FAIL: $ERRORS check(s) failed"
surfaces/verify/d94-policy-runtime-enforcement-lock.sh:9:ERRORS=0
surfaces/verify/d94-policy-runtime-enforcement-lock.sh:10:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d94-policy-runtime-enforcement-lock.sh:19:  echo "D94 FAIL: $ERRORS check(s) failed"
surfaces/verify/d94-policy-runtime-enforcement-lock.sh:116:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d94-policy-runtime-enforcement-lock.sh:117:  echo "D94 FAIL: $ERRORS check(s) failed"
surfaces/verify/d114-ha-automation-stability.sh:39:  echo "WARN: could not parse automation entities from HA API response"
surfaces/verify/d114-ha-automation-stability.sh:48:  echo "WARN: automation count mismatch (actual=$ACTUAL_COUNT, expected=$EXPECTED_COUNT)"
surfaces/verify/d53-change-pack-integrity-lock.sh:16:ERRORS=0
surfaces/verify/d53-change-pack-integrity-lock.sh:21:  ERRORS=$((ERRORS + 1))
surfaces/verify/d53-change-pack-integrity-lock.sh:27:  ERRORS=$((ERRORS + 1))
surfaces/verify/d53-change-pack-integrity-lock.sh:45:    ERRORS=$((ERRORS + 1))
surfaces/verify/d53-change-pack-integrity-lock.sh:68:      ERRORS=$((ERRORS + 1))
surfaces/verify/d53-change-pack-integrity-lock.sh:73:[[ "$ERRORS" -eq 0 ]] && exit 0 || exit 1
surfaces/verify/api-preconditions.sh:9:command -v yq >/dev/null 2>&1 || { echo "ERROR: yq missing"; exit 1; }
surfaces/verify/api-preconditions.sh:34:  echo "STOP: API preconditions rule violated"
ops/plugins/release/bin/spine-release-zip:21:  echo "STOP: git not installed" >&2
ops/plugins/release/bin/spine-release-zip:25:  echo "STOP: zip not installed (macOS: brew install zip)" >&2
ops/plugins/release/bin/spine-release-zip:37:  echo "STOP: working tree dirty; commit first to produce a sharable artifact" >&2
ops/plugins/docker/bin/docker-compose-pull:14:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/docker/bin/docker-compose-pull:81:  -o "LogLevel=ERROR"
ops/plugins/rag/bin/rag-reindex-remote-verify:19:warn() { echo "WARN: $*" >&2; }
ops/plugins/rag/bin/rag-reindex-remote-verify:47:SESSION_MUST_STOP="$(yq -r '.completion.session_must_be_stopped // true' "$QUALITY_BINDING")"
ops/plugins/rag/bin/rag-reindex-remote-verify:61:ERRORS=0
ops/plugins/rag/bin/rag-reindex-remote-verify:62:err() { echo "  ERROR: $*" >&2; ERRORS=$((ERRORS + 1)); }
ops/plugins/rag/bin/rag-reindex-remote-verify:67:SESSION_STATE="STOPPED"
ops/plugins/rag/bin/rag-reindex-remote-verify:68:if [[ "$SESSION_MUST_STOP" == "true" ]]; then
ops/plugins/rag/bin/rag-reindex-remote-verify:71:    err "Session $TMUX_SESSION is still RUNNING (expected STOPPED)"
ops/plugins/rag/bin/rag-reindex-remote-verify:73:    ok "Session is STOPPED"
ops/plugins/rag/bin/rag-reindex-remote-verify:83:failed_uploads="$(ssh "${SSH_ARGS[@]}" "$TARGET" "awk -v s='$run_start_line' 'NR>=s && /ERROR: Upload failed/{c++} END{print c+0}' '$REMOTE_LOG' 2>/dev/null || echo 0")"
ops/plugins/rag/bin/rag-reindex-remote-verify:265:if [[ "$ERRORS" -gt 0 ]]; then
ops/plugins/rag/bin/rag-reindex-remote-verify:266:  fail "$ERRORS quality gate(s) failed"
surfaces/verify/d108-media-health-endpoint-parity-lock.sh:11:ERRORS=0
surfaces/verify/d108-media-health-endpoint-parity-lock.sh:12:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d108-media-health-endpoint-parity-lock.sh:55:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d108-media-health-endpoint-parity-lock.sh:56:  echo "D108 FAIL: $ERRORS check(s) failed"
surfaces/verify/d100-vm-ip-parity-lock.sh:10:ERRORS=0
surfaces/verify/d100-vm-ip-parity-lock.sh:12:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d100-vm-ip-parity-lock.sh:18:    echo "D100 FAIL: $ERRORS check(s) failed"
surfaces/verify/d100-vm-ip-parity-lock.sh:27:  echo "D100 FAIL: $ERRORS check(s) failed"
surfaces/verify/d100-vm-ip-parity-lock.sh:32:  echo "D100 FAIL: $ERRORS check(s) failed"
surfaces/verify/d100-vm-ip-parity-lock.sh:39:  echo "D100 FAIL: $ERRORS check(s) failed"
surfaces/verify/d100-vm-ip-parity-lock.sh:48:  echo "D100 FAIL: $ERRORS check(s) failed"
surfaces/verify/d100-vm-ip-parity-lock.sh:53:  echo "D100 FAIL: $ERRORS check(s) failed"
surfaces/verify/d100-vm-ip-parity-lock.sh:65:  echo "D100 FAIL: $ERRORS check(s) failed"
surfaces/verify/d100-vm-ip-parity-lock.sh:77:  echo "D100 FAIL: $ERRORS check(s) failed"
surfaces/verify/d100-vm-ip-parity-lock.sh:133:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d100-vm-ip-parity-lock.sh:134:  echo "D100 FAIL: $ERRORS check(s) failed ($CHECKED VMs checked)"
ops/plugins/rag/bin/anythingllm-tune:17:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/rag/bin/anythingllm-tune:76:  -o "LogLevel=ERROR"
ops/plugins/secrets/bin/secrets-binding:40:  echo "STATUS: STOP (binding incomplete)"
surfaces/verify/cloudflare-drift-gate.sh:52:LEAK1="$(grep -nE '^\s*echo\s+.*\$CLOUDFLARE' "$CF_SCRIPT" 2>/dev/null | grep -v 'not present\|missing\|STOP\|>&2' || true)"
ops/plugins/docker/bin/docker-compose-up:14:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/docker/bin/docker-compose-up:81:  -o "LogLevel=ERROR"
ops/plugins/secrets/bin/secrets-auth-load:7:# STOP semantics: 2 = operator action required
ops/plugins/secrets/bin/secrets-auth-load:9:  echo "STOP: $*" >&2
ops/plugins/mint/bin/intake-validate:6:# STOP=2 on preconditions.
ops/plugins/mint/bin/intake-validate:18:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/mint/bin/intake-validate:63:  echo "ERROR: curl failed (rc=$curl_rc)"
ops/plugins/mint/bin/migrate-dryrun:7:# STOP=2 on preconditions.
ops/plugins/mint/bin/migrate-dryrun:19:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/mint/bin/migrate-dryrun:33:SSH_OPTS=(-o ConnectTimeout=5 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o LogLevel=ERROR)
ops/plugins/mint/bin/migrate-dryrun:53:    echo "ERROR: psql query failed (rc=$pg_rc)"
ops/plugins/docker/bin/docker-compose-logs:4:# WARNING: logs may contain secrets. This command is receipt-producing; use minimal tail.
ops/plugins/docker/bin/docker-compose-logs:16:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/docker/bin/docker-compose-logs:122:  -o "LogLevel=ERROR"
ops/plugins/docker/bin/docker-compose-status:6:# STOP=2 on preconditions (missing binding, yq, etc.)
ops/plugins/docker/bin/docker-compose-status:29:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/docker/bin/docker-compose-status:212:    -o "LogLevel=ERROR"
ops/plugins/infra/bin/infra-docker-host-status:16:stop() { echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/infra/bin/infra-docker-host-status:42:  -o LogLevel=ERROR
surfaces/verify/tests/d94-test.sh:54:    validates_via: "WARN_POLICY variable"
ops/plugins/surface/bin/surface-readonly-audit:17:ERRORS=0
ops/plugins/surface/bin/surface-readonly-audit:40:        ERRORS=$((ERRORS + 1))
ops/plugins/surface/bin/surface-readonly-audit:52:    echo "    WARNING: mutating access declared!"
ops/plugins/surface/bin/surface-readonly-audit:53:    ERRORS=$((ERRORS + 1))
ops/plugins/surface/bin/surface-readonly-audit:58:echo "Summary: $TOTAL surfaces, $EXISTING existing, $PLANNED planned, $ERRORS errors"
ops/plugins/surface/bin/surface-readonly-audit:60:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d95-version-compat-matrix-lock.sh:9:ERRORS=0
surfaces/verify/d95-version-compat-matrix-lock.sh:10:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d95-version-compat-matrix-lock.sh:18:  echo "D95 FAIL: $ERRORS check(s) failed"
surfaces/verify/d95-version-compat-matrix-lock.sh:106:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d95-version-compat-matrix-lock.sh:107:  echo "D95 FAIL: $ERRORS check(s) failed"
ops/plugins/rag/bin/rag-reindex-remote-status:69:  echo "session: STOPPED"
ops/plugins/rag/bin/rag-reindex-remote-status:73:failed="$(ssh "${SSH_ARGS[@]}" "$TARGET" "awk '/ERROR: Upload failed/{c++} END{print c+0}' '$REMOTE_LOG' 2>/dev/null || echo 0")"
ops/plugins/rag/bin/rag-reindex-remote-status:82:    failed="$(ssh "${SSH_ARGS[@]}" "$TARGET" "awk '/ERROR: Upload failed/{c++} END{print c+0}' '$effective_log' 2>/dev/null || echo 0")"
ops/plugins/rag/bin/rag-reindex-remote-status:89:failed="$(ssh "${SSH_ARGS[@]}" "$TARGET" "awk -v s='$run_start_line' 'NR>=s && /ERROR: Upload failed/{c++} END{print c+0}' '$effective_log' 2>/dev/null || echo 0")"
ops/plugins/host/bin/host-macbook-bootstrap:37:ERRORS=0
ops/plugins/host/bin/host-macbook-bootstrap:45:    *) echo "ERROR: unknown argument: $1" >&2; exit 1 ;;
ops/plugins/host/bin/host-macbook-bootstrap:116:        rg) install_brew_formula ripgrep || ((ERRORS++)) ;;
ops/plugins/host/bin/host-macbook-bootstrap:117:        *)  install_brew_formula "$tool" || ((ERRORS++)) ;;
ops/plugins/host/bin/host-macbook-bootstrap:137:      ((ERRORS++))
ops/plugins/host/bin/host-macbook-bootstrap:152:      ((ERRORS++))
ops/plugins/host/bin/host-macbook-bootstrap:187:  install_brew_cask superwhisper || ((ERRORS++))
ops/plugins/host/bin/host-macbook-bootstrap:193:    brew install espanso 2>/dev/null || { echo "  FAIL espanso"; ((ERRORS++)); }
ops/plugins/host/bin/host-macbook-bootstrap:199:  install_brew_cask maccy || ((ERRORS++))
ops/plugins/host/bin/host-macbook-bootstrap:229:echo "errors: $ERRORS"
ops/plugins/host/bin/host-macbook-bootstrap:230:if [[ "$ERRORS" -gt 0 ]]; then
ops/plugins/secrets/bin/secrets-identity-rbac-status:136:  echo "status: WARN"
ops/plugins/host/bin/host-claude-entrypoint-lock:27:fail() { echo "claude-entrypoint-lock ERROR: $*" >&2; }
ops/plugins/host/bin/host-claude-entrypoint-lock:169:    echo "claude-entrypoint-lock WARN: $FINDINGS finding(s) (status mode, non-blocking)"
ops/plugins/host/bin/host-streamdeck-reload:11:  echo "ERROR: Plist not found: $PLIST" >&2
ops/plugins/secrets/bin/secrets-inventory-status:16:command -v yq >/dev/null 2>&1 || { echo "STOP: missing dependency: yq" >&2; exit 2; }
ops/plugins/secrets/bin/secrets-inventory-status:20:  echo "status: STOP"
ops/plugins/secrets/bin/secrets-inventory-status:29:  echo "status: STOP"
ops/plugins/secrets/bin/secrets-inventory-status:38:  echo "status: STOP"
ops/plugins/secrets/bin/secrets-credentials-parity:16:#   2 STOP (missing bindings/deps)
ops/plugins/secrets/bin/secrets-credentials-parity:25:stop() { echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/secrets/bin/secrets-credentials-parity:137:    -o "LogLevel=ERROR"
ops/plugins/aof/bin/contract-read-check.sh:27:    *) echo "ERROR: unknown arg: $1" >&2; usage; exit 2 ;;
ops/plugins/aof/bin/contract-read-check.sh:38:[[ -f "$CONTRACT_FILE" ]] || { echo "ERROR: contract not found: $CONTRACT_FILE" >&2; exit 1; }
ops/plugins/secrets/bin/secrets-projects-status:20:command -v yq >/dev/null 2>&1 || { echo "status: STOP"; echo "reason: missing_dep_yq"; exit 2; }
ops/plugins/secrets/bin/secrets-projects-status:21:command -v jq >/dev/null 2>&1 || { echo "status: STOP"; echo "reason: missing_dep_jq"; exit 2; }
ops/plugins/secrets/bin/secrets-projects-status:22:command -v curl >/dev/null 2>&1 || { echo "status: STOP"; echo "reason: missing_dep_curl"; exit 2; }
ops/plugins/secrets/bin/secrets-projects-status:26:  echo "status: STOP"
ops/plugins/secrets/bin/secrets-projects-status:34:  echo "status: STOP"
ops/plugins/secrets/bin/secrets-projects-status:44:  echo "status: STOP"
ops/plugins/secrets/bin/secrets-projects-status:67:[[ -x "$INFISICAL_AGENT" ]] || { echo "status: STOP"; echo "reason: infisical_agent_not_found"; exit 2; }
ops/plugins/secrets/bin/secrets-projects-status:72:  echo "status: STOP"
ops/plugins/secrets/bin/secrets-projects-status:89:  echo "status: STOP"
ops/plugins/secrets/bin/secrets-projects-status:98:  echo "status: STOP"
ops/plugins/secrets/bin/secrets-projects-status:106:    ERROR_MSG="$(echo "$LIVE_BODY" | jq -r '.message // "api error"')"
ops/plugins/secrets/bin/secrets-projects-status:108:    ERROR_MSG="non_json_response"
ops/plugins/secrets/bin/secrets-projects-status:110:  echo "status: STOP"
ops/plugins/secrets/bin/secrets-projects-status:112:  echo "error: $ERROR_MSG"
ops/plugins/secrets/bin/secrets-projects-status:117:  echo "status: STOP"
ops/plugins/secrets/bin/secrets-projects-status:127:  echo "status: STOP"
ops/plugins/mint/bin/deploy-status:7:# STOP=2 on preconditions.
ops/plugins/mint/bin/deploy-status:21:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/mint/bin/deploy-status:29:SSH_OPTS=(-o ConnectTimeout=5 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o LogLevel=ERROR)
ops/plugins/mint/bin/seeds-query:5:# STOP=2 on preconditions.
ops/plugins/mint/bin/seeds-query:21:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/mint/bin/seeds-query:68:SSH_OPTS=(-o ConnectTimeout=5 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o LogLevel=ERROR)
ops/plugins/mint/bin/seeds-query:90:  echo "ERROR: query failed (rc=$rc)"
ops/plugins/secrets/bin/secrets-status:2:# secrets.status — prove secrets plumbing is either configured or STOP
ops/plugins/secrets/bin/secrets-status:37:    echo "WARN: infisical_projects missing/unreadable"
ops/plugins/secrets/bin/secrets-status:40:  echo "WARN: SERVICE_REGISTRY.yaml missing"
ops/plugins/secrets/bin/secrets-status:60:# If secret missing -> STOP (fail) so agents cannot proceed into API work
ops/plugins/secrets/bin/secrets-status:62:  echo "STATUS: STOP (secrets not configured)"
ops/plugins/docker/bin/docker-compose-down:14:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/docker/bin/docker-compose-down:78:  -o "LogLevel=ERROR"
surfaces/verify/verify-identity.sh:68:WARN=0
surfaces/verify/verify-identity.sh:91:    ((++WARN))
surfaces/verify/verify-identity.sh:92:    status="WARN"
surfaces/verify/verify-identity.sh:111:      WARN) echo -e "  ${YELLOW}⚠${NC} $name: $message" ;;
surfaces/verify/verify-identity.sh:137:    ((++WARN))
surfaces/verify/verify-identity.sh:138:    status="WARN"
surfaces/verify/verify-identity.sh:146:      WARN) echo -e "  ${YELLOW}⚠${NC} $name: $message" ;;
surfaces/verify/verify-identity.sh:202:  echo "  \"summary\": {\"pass\": $PASS, \"fail\": $FAIL, \"warn\": $WARN},"
surfaces/verify/verify-identity.sh:218:  echo -e "  Summary: ${GREEN}$PASS passed${NC} | ${RED}$FAIL failed${NC} | ${YELLOW}$WARN warnings${NC}"
ops/plugins/infra/bin/infra-relocation-parity:14:fail() { echo "ERROR: $*" >&2; exit 1; }
ops/plugins/aof/bin/validate-environment.sh:22:    *) echo "ERROR: unknown arg: $1" >&2; usage; exit 2 ;;
ops/plugins/aof/bin/validate-environment.sh:26:command -v yq >/dev/null 2>&1 || { echo "ERROR: yq is required"; exit 1; }
ops/plugins/aof/bin/validate-environment.sh:29:WARN=0
ops/plugins/aof/bin/validate-environment.sh:30:err() { echo "ERROR: $*" >&2; ERR=$((ERR + 1)); }
ops/plugins/aof/bin/validate-environment.sh:31:warn() { echo "WARN: $*" >&2; WARN=$((WARN + 1)); }
ops/plugins/aof/bin/validate-environment.sh:74:  echo "VALIDATION FAILED: $ERR error(s), $WARN warning(s)"
ops/plugins/aof/bin/validate-environment.sh:78:if [[ "$STRICT" -eq 1 && "$WARN" -gt 0 ]]; then
ops/plugins/aof/bin/validate-environment.sh:79:  echo "VALIDATION FAILED (strict): 0 errors, $WARN warning(s)"
ops/plugins/aof/bin/validate-environment.sh:83:echo "VALIDATION PASSED: 0 errors, $WARN warning(s)"
ops/plugins/mint/bin/modules-health:8:# STOP=2 on preconditions.
ops/plugins/mint/bin/modules-health:20:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/mint/bin/modules-health:51:SSH_OPTS=(-o ConnectTimeout=5 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o LogLevel=ERROR)
ops/plugins/mint/bin/modules-health:124:  echo "STOP (2): component '$FILTER' not found"
surfaces/verify/d61-session-loop-traceability-lock.sh:18:warn() { echo "  WARN: $1" >&2; }
ops/plugins/cloudflare/bin/cloudflare-dns-status:14:  echo "STOP: CLOUDFLARE_API_TOKEN missing" >&2
ops/plugins/cloudflare/bin/cloudflare-dns-status:38:        print(f"- {name}: STOP (dns_records request failed: {exc})")
ops/plugins/cloudflare/bin/cloudflare-tunnel-ingress-status:33:      [[ $# -ge 2 ]] || { echo "STOP: --tunnel requires NAME" >&2; exit 2; }
ops/plugins/cloudflare/bin/cloudflare-tunnel-ingress-status:42:      echo "STOP: unknown argument: $1" >&2
ops/plugins/cloudflare/bin/cloudflare-tunnel-ingress-status:57:[[ -n "${CLOUDFLARE_API_TOKEN:-}" ]] || { echo "STOP: CLOUDFLARE_API_TOKEN missing" >&2; exit 2; }
ops/plugins/cloudflare/bin/cloudflare-tunnel-ingress-status:58:[[ -n "${CLOUDFLARE_ACCOUNT_ID:-}" ]] || { echo "STOP: CLOUDFLARE_ACCOUNT_ID missing" >&2; exit 2; }
ops/plugins/cloudflare/bin/cloudflare-tunnel-ingress-status:62:  [[ -f "$BINDING_FILE" ]] || { echo "STOP: binding missing: $BINDING_FILE" >&2; exit 2; }
ops/plugins/cloudflare/bin/cloudflare-tunnel-ingress-status:82:[[ -n "${TUNNEL_ID:-}" ]] || { echo "STOP: tunnel id not found (env CLOUDFLARE_TUNNEL_ID unset; binding missing name '$TUNNEL_NAME')" >&2; exit 2; }
ops/plugins/cloudflare/bin/cloudflare-tunnel-ingress-status:96:    print("STOP: Cloudflare API returned success=false", file=sys.stderr)
ops/plugins/aof/bin/aof-policy-show.sh:22:  echo "ERROR: resolve-policy.sh not found"
ops/plugins/aof/bin/aof-policy-show.sh:29:warn_policy="${RESOLVED_WARN_POLICY:-advisory}"
ops/plugins/cloudflare/bin/cloudflare-domain-routing-diff:34:      [[ $# -ge 2 ]] || { echo "STOP: --tunnel requires NAME" >&2; exit 2; }
ops/plugins/cloudflare/bin/cloudflare-domain-routing-diff:43:      echo "STOP: unknown argument: $1" >&2
ops/plugins/cloudflare/bin/cloudflare-domain-routing-diff:50:[[ -f "$REGISTRY_FILE" ]] || { echo "STOP: missing registry: $REGISTRY_FILE" >&2; exit 2; }
ops/plugins/cloudflare/bin/cloudflare-domain-routing-diff:51:command -v yq >/dev/null 2>&1 || { echo "STOP: missing dependency: yq" >&2; exit 2; }
ops/plugins/cloudflare/bin/cloudflare-domain-routing-diff:61:[[ -n "${CLOUDFLARE_API_TOKEN:-}" ]] || { echo "STOP: CLOUDFLARE_API_TOKEN missing" >&2; exit 2; }
ops/plugins/cloudflare/bin/cloudflare-domain-routing-diff:62:[[ -n "${CLOUDFLARE_ACCOUNT_ID:-}" ]] || { echo "STOP: CLOUDFLARE_ACCOUNT_ID missing" >&2; exit 2; }
ops/plugins/cloudflare/bin/cloudflare-domain-routing-diff:66:  [[ -f "$BINDING_FILE" ]] || { echo "STOP: binding missing: $BINDING_FILE" >&2; exit 2; }
ops/plugins/cloudflare/bin/cloudflare-domain-routing-diff:86:[[ -n "${TUNNEL_ID:-}" ]] || { echo "STOP: tunnel id not found (env CLOUDFLARE_TUNNEL_ID unset; binding missing name '$TUNNEL_NAME')" >&2; exit 2; }
ops/plugins/rag/bin/rag:41:log_err() { echo "ERROR: $*" >&2; }
ops/plugins/rag/bin/rag:140:  log_err "STOP: ANYTHINGLLM_API_KEY is not set."
ops/plugins/rag/bin/rag:224:    log_err "STOP: workspace '${slug}' does not exist on AnythingLLM (${ANYTHINGLLM_URL})."
ops/plugins/rag/bin/rag:239:      log_err "STOP: workspace '${slug}' not found (API returned empty workspace object)."
ops/plugins/secrets/bin/secrets-exec:36:# --- preconditions (STOP if unmet) ---
ops/plugins/secrets/bin/secrets-exec:42:    echo "STOP: secrets.binding not OK (exit $rc)"
ops/plugins/secrets/bin/secrets-exec:47:    echo "STOP: secrets.auth.status not OK (exit $rc)"
ops/plugins/host/bin/host-drift-audit:9:fail() { echo "host.drift.audit ERROR: $*" >&2; exit 1; }
ops/plugins/secrets/bin/secrets-cli-status:25:  echo "WARN shim missing: $SHIM (spine-only mode)"
ops/plugins/rag/bin/rag-remote-dependency-probe:18:warn() { echo "WARN: $*"; }
ops/plugins/rag/bin/rag-remote-dependency-probe:52:ERRORS=0
ops/plugins/rag/bin/rag-remote-dependency-probe:75:    ERRORS=$((ERRORS + 1))
ops/plugins/rag/bin/rag-remote-dependency-probe:109:if [[ "$ERRORS" -gt 0 ]]; then
ops/plugins/rag/bin/rag-remote-dependency-probe:110:  fail "$ERRORS dependency probe(s) failed (unreachable)"
ops/plugins/host/bin/host-macbook-drift-check:32:BOOT_ERRORS=0
ops/plugins/host/bin/host-macbook-drift-check:38:    BOOT_ERRORS=$((BOOT_ERRORS + 1))
ops/plugins/host/bin/host-macbook-drift-check:41:if [[ "$BOOT_ERRORS" -gt 0 ]]; then
ops/plugins/host/bin/host-macbook-drift-check:42:  echo "FAIL: $BOOT_ERRORS bootstrap CLI tool(s) missing"
ops/plugins/secrets/bin/secrets-set-interactive:17:[[ -f "$AGENT" ]] || { echo "STOP (2): missing infisical-agent.sh at $AGENT"; exit 2; }
ops/plugins/secrets/bin/secrets-set-interactive:41:  echo "STOP: project '$PROJECT' is deprecated or unknown."
ops/plugins/rag/bin/rag-reindex-remote-start:166:    echo "  session_state:      STOPPED"
ops/plugins/secrets/bin/secrets-namespace-status:31:  echo "STOP: incomplete secrets binding"
ops/plugins/secrets/bin/secrets-namespace-status:35:  echo "STOP: incomplete secrets namespace policy"
ops/plugins/secrets/bin/secrets-auth-status:35:  echo "STATUS: STOP (no auth present)"
ops/plugins/rag/bin/rag-embedding-probe:22:warn() { echo "WARN: $*"; }
ops/plugins/verify/tests/test-drift-gates-certify-domain.sh:72:require_match "$bad_out" '^ERROR: unknown domain:' "unknown domain error emitted"
ops/plugins/infra/bin/infra-relocation-impact:12:fail() { echo "ERROR: $*" >&2; exit 1; }
ops/plugins/cloudflare/bin/cloudflare-tunnel-status:13:  echo "STOP: CLOUDFLARE_ACCOUNT_ID missing" >&2
ops/plugins/cloudflare/bin/cloudflare-tunnel-status:17:  echo "STOP: CLOUDFLARE_API_TOKEN missing" >&2
ops/plugins/cloudflare/bin/cloudflare-tunnel-status:28:    print("STOP: tunnels API returned success=false")
surfaces/verify/tests/d90-test.sh:37:    pass "D90 correctly fails when session is STOPPED with quality issues"
ops/plugins/rag/bin/rag-reindex-remote-stop:88:    echo "  session:      STOPPED (nothing to stop)"
ops/plugins/rag/bin/rag-reindex-remote-stop:101:  echo "STOPPED: tmux session '$TMUX_SESSION' on $TARGET"
ops/plugins/host/bin/host-streamdeck-snapshot:13:  echo "STOP (2): no config.json at $RUNTIME_DIR"
ops/plugins/cloudflare/bin/cloudflare-inventory-sync:21:  echo "STOP: CLOUDFLARE_API_TOKEN missing" >&2
ops/plugins/cloudflare/bin/cloudflare-inventory-sync:25:  echo "STOP: CLOUDFLARE_ACCOUNT_ID missing" >&2
ops/plugins/observability/bin/stability-control-reconcile:46:      echo "STOP: unknown argument: $1" >&2
ops/plugins/observability/bin/stability-control-reconcile:54:    echo "STOP: missing dependency: $1" >&2
ops/plugins/observability/bin/stability-control-reconcile:61:    echo "STOP: missing file: $1" >&2
ops/plugins/observability/bin/stability-control-reconcile:79:  echo "STOP: no critical domains configured in contract" >&2
ops/plugins/observability/bin/stability-control-reconcile:113:    echo "STOP: unknown domain '$TARGET_DOMAIN'" >&2
ops/plugins/observability/bin/stability-control-reconcile:134:    detail="$(grep -E 'FAIL|STOP|disconnected|unreachable' "$tmp" | head -n1 || true)"
ops/plugins/maker/bin/maker-qr-generate:7:_need() { command -v "$1" >/dev/null 2>&1 || { echo "STOP: missing dependency: $1" >&2; exit 2; }; }
ops/plugins/maker/bin/maker-qr-generate:57:    echo "STOP: unknown format '$FORMAT' (use: png, svg, terminal)" >&2
ops/plugins/cloudflare/bin/cloudflare-status:22:  echo "STOP: CLOUDFLARE_API_TOKEN not present in injected environment." >&2
ops/plugins/cloudflare/bin/cloudflare-status:31:    echo "STOP: Cloudflare zones request failed (network/auth)." >&2
ops/plugins/cloudflare/bin/cloudflare-status:41:    print("STOP: Cloudflare API returned success=false for zones.", file=sys.stderr)
ops/plugins/cloudflare/bin/cloudflare-status:66:            print(f"- {name}: STOP (dns_records success=false)")
ops/plugins/cloudflare/bin/cloudflare-status:71:        print(f"- {name}: STOP (dns_records request failed)")
ops/plugins/observability/bin/gitea-status:15:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/observability/bin/immich-ingest-watch:11:    echo "STOP: missing dependency: $1" >&2
ops/plugins/observability/bin/immich-ingest-watch:18:    echo "STOP: missing file: $1" >&2
ops/plugins/observability/bin/immich-ingest-watch:51:  echo "STOP: policy.guided_only must be true in $CONTRACT for this sprint." >&2
ops/plugins/observability/bin/immich-ingest-watch:55:  echo "STOP: auto mutation/restart is disallowed by stabilization policy." >&2
ops/plugins/observability/bin/immich-ingest-watch:64:  echo "STOP: target '$target_id' has no host in $SSH_BINDING" >&2
ops/plugins/aof/bin/bootstrap-spine.sh:33:    *) echo "ERROR: unknown arg: $1" >&2; usage; exit 2 ;;
ops/plugins/aof/bin/bootstrap-spine.sh:37:[[ -n "$ENV_NAME" ]] || { echo "ERROR: --environment-name is required" >&2; exit 2; }
ops/plugins/aof/bin/bootstrap-spine.sh:38:[[ "$ENV_NAME" =~ ^[a-z0-9-]+$ ]] || { echo "ERROR: invalid environment name '$ENV_NAME' (kebab-case only)" >&2; exit 2; }
ops/plugins/aof/bin/bootstrap-spine.sh:41:  *) echo "ERROR: invalid --profile '$PROFILE' (expected minimal|product|production)" >&2; exit 2 ;;
ops/plugins/aof/bin/bootstrap-spine.sh:45:[[ -f "$PROFILE_FILE" ]] || { echo "ERROR: profile not found: $PROFILE_FILE" >&2; exit 1; }
ops/plugins/aof/bin/bootstrap-spine.sh:67:  echo "ERROR: $ENV_FILE exists (use --force to overwrite)" >&2
ops/plugins/aof/bin/bootstrap-spine.sh:72:  echo "ERROR: $IDENTITY_FILE exists (use --force to overwrite)" >&2
ops/plugins/infra/bin/infra-relocation-rollback:12:fail() { echo "ERROR: $*" >&2; exit 1; }
ops/plugins/observability/bin/switch-health-status:22:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/verify/bin/drift-gates-certify:39:      [[ $# -ge 2 ]] || { echo "ERROR: --domain requires a value" >&2; exit 2; }
ops/plugins/verify/bin/drift-gates-certify:60:      echo "ERROR: unknown arg: $1" >&2
ops/plugins/verify/bin/drift-gates-certify:68:    echo "ERROR: missing gate domain binding: $DOMAIN_BINDING" >&2
ops/plugins/verify/bin/drift-gates-certify:74:    echo "ERROR: yq is required for --list-domains/--domain modes." >&2
ops/plugins/verify/bin/drift-gates-certify:89:    echo "ERROR: unknown domain: $DOMAIN" >&2
ops/plugins/verify/bin/drift-gates-certify:98:  echo "ERROR: missing: $DG" >&2
ops/plugins/infra/bin/infra-relocation-preflight:16:fail() { echo "ERROR: $*" >&2; exit 1; }
ops/plugins/infra/bin/infra-relocation-preflight:21:note_warn() { echo "  [WARN] $*"; }
ops/plugins/infra/bin/infra-relocation-preflight:120:    ssh_opts=(-o BatchMode=yes -o ConnectTimeout=6 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR)
surfaces/verify/verify.sh:8:  echo "ERROR: verify surface missing: $V" >&2
ops/plugins/infra/bin/infra-hypervisor-hostname-set:36:fail() { echo "ERROR: $*" >&2; exit 1; }
ops/plugins/infra/bin/infra-hypervisor-hostname-set:74:ssh_opts=(-o BatchMode=yes -o ConnectTimeout=8 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR)
ops/plugins/observability/bin/idrac-health-status:22:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/observability/bin/idrac-health-status:93:    echo "status: PARSE_ERROR"
ops/plugins/maker/bin/maker-tools-status:6:_need() { command -v "$1" >/dev/null 2>&1 || { echo "STOP: missing dependency: $1" >&2; exit 2; }; }
ops/plugins/maker/bin/maker-tools-status:11:  echo "STOP: missing binding file: $BINDING_FILE" >&2
ops/plugins/maker/bin/maker-tools-status:40:  echo "STOP: no tools defined in $BINDING_FILE"
ops/plugins/maker/bin/maker-tools-status:81:  echo "WARN: $MISSING enabled tool(s) not installed"
ops/plugins/network/bin/network-cutover-preflight:25:ERRORS=0
ops/plugins/network/bin/network-cutover-preflight:36:  ERRORS=$((ERRORS + 1))
ops/plugins/network/bin/network-cutover-preflight:45:  ERRORS=$((ERRORS + 1))
ops/plugins/network/bin/network-cutover-preflight:78:  ERRORS=$((ERRORS + MISSING_PACKS))
ops/plugins/network/bin/network-cutover-preflight:83:SECTION_ERRORS=0
ops/plugins/network/bin/network-cutover-preflight:104:      if [[ "$SECTION_ERRORS" -eq 0 ]]; then
ops/plugins/network/bin/network-cutover-preflight:108:      SECTION_ERRORS=$((SECTION_ERRORS + 1))
ops/plugins/network/bin/network-cutover-preflight:113:if [[ "$SECTION_ERRORS" -eq 0 ]]; then
ops/plugins/network/bin/network-cutover-preflight:116:  ERRORS=$((ERRORS + SECTION_ERRORS))
ops/plugins/network/bin/network-cutover-preflight:121:if [[ "$ERRORS" -eq 0 ]]; then
ops/plugins/network/bin/network-cutover-preflight:125:  echo "result: NO-GO ($ERRORS error(s))"
ops/plugins/observability/bin/finance-stack-status:21:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/infra/bin/infra-vm-bootstrap:32:fail() { echo "ERROR: $*" >&2; exit 1; }
ops/plugins/infra/bin/infra-vm-bootstrap:153:    echo "ERROR: command requires root and sudo is unavailable: $*"
ops/plugins/infra/bin/infra-vm-bootstrap:159:  echo "ERROR: apt-get not found on target"
ops/plugins/infra/bin/infra-vm-bootstrap:190:  systemctl is-active --quiet qemu-guest-agent || { echo "ERROR: qemu-guest-agent not running"; exit 1; }
ops/plugins/infra/bin/infra-vm-bootstrap:194:  systemctl is-active --quiet docker || { echo "ERROR: docker service not running"; exit 1; }
ops/plugins/infra/bin/infra-vm-bootstrap:203:    echo "ERROR: neither cron nor crond service is running"; exit 1
ops/plugins/infra/bin/infra-vm-bootstrap:208:  command -v tailscale >/dev/null 2>&1 || { echo "ERROR: tailscale binary missing"; exit 1; }
ops/plugins/infra/bin/infra-vm-bootstrap:216:      [[ -n "$TS_IP" ]] || { echo "ERROR: tailscale up with authkey failed"; exit 1; }
ops/plugins/infra/bin/infra-vm-bootstrap:218:      echo "ERROR: tailscale not connected and no auth key provided (run tailscale up manually)"
ops/plugins/infra/bin/infra-vm-bootstrap:235:        echo "WARNING: could not fetch $ts_auth_secret_key from Infisical"
ops/plugins/infra/bin/infra-vm-bootstrap:240:ssh_opts=(-o BatchMode=yes -o ConnectTimeout=8 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR)
ops/plugins/audit/bin/audit-export:180:        return "ERROR"
ops/plugins/network/bin/network-lan-device-status:21:  echo "STOP (2): missing binding file: $BINDING_FILE"
ops/plugins/network/bin/network-lan-device-status:26:  echo "STOP (2): yq is required but not found"
ops/plugins/network/bin/network-lan-device-status:84:    -o "LogLevel=ERROR"
ops/plugins/network/bin/network-lan-device-status:107:  echo "STOP (2): target '$TARGET_FILTER' not found or not lan_only"
ops/plugins/maker/bin/maker-label-print:6:_need() { command -v "$1" >/dev/null 2>&1 || { echo "STOP: missing dependency: $1" >&2; exit 2; }; }
ops/plugins/maker/bin/maker-label-print:11:  echo "STOP: missing binding file: $BINDING_FILE" >&2
ops/plugins/maker/bin/maker-label-print:19:  echo "STOP: brother-ql is not enabled in $BINDING_FILE"
ops/plugins/maker/bin/maker-label-print:27:  echo "STOP: brother_ql command not found"
ops/plugins/maker/bin/maker-label-print:42:echo "STOP: label printing not yet implemented (hardware not connected)"
ops/plugins/observability/bin/uptime-kuma-monitors-sync:21:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/observability/bin/uptime-kuma-monitors-sync:66:    print("STOP (2): missing python3 dependency: websockets", file=sys.stderr)
ops/plugins/observability/bin/uptime-kuma-monitors-sync:124:                print(f"  ERROR: unexpected handshake: {msg[:100]}")
ops/plugins/observability/bin/uptime-kuma-monitors-sync:250:        print(f"ERROR: {e}", file=sys.stderr)
ops/plugins/observability/bin/nas-health-status:18:SSH_OPTS="-o ConnectTimeout=5 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o LogLevel=ERROR"
ops/plugins/observability/bin/nas-health-status:20:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/verify/bin/schema-conventions-audit:97:    for existing in "${WARNINGS[@]}"; do
ops/plugins/verify/bin/schema-conventions-audit:100:    WARNINGS+=("$entry")
ops/plugins/verify/bin/schema-conventions-audit:148:declare -a WARNINGS=()
ops/plugins/verify/bin/schema-conventions-audit:250:    echo "- Warnings: ${#WARNINGS[@]}"
ops/plugins/verify/bin/schema-conventions-audit:262:    if [[ "${#WARNINGS[@]}" -eq 0 ]]; then
ops/plugins/verify/bin/schema-conventions-audit:265:      for row in "${WARNINGS[@]}"; do
ops/plugins/verify/bin/schema-conventions-audit:278:    --argjson warnings "$(printf '%s\n' "${WARNINGS[@]}" | jq -R -s 'split("\n") | map(select(length > 0))')" \
ops/plugins/verify/bin/schema-conventions-audit:285:  echo "warnings: ${#WARNINGS[@]}"
ops/plugins/verify/bin/schema-conventions-audit:293:  if [[ "${#WARNINGS[@]}" -gt 0 ]]; then
ops/plugins/verify/bin/schema-conventions-audit:296:    printf '  - %s\n' "${WARNINGS[@]}"
ops/plugins/proposals/bin/proposals-supersede:29:[[ -n "$REASON" ]] || { echo "ERROR: --reason is required"; usage; }
ops/plugins/proposals/bin/proposals-supersede:40:[[ -n "$CP_DIR" && -d "$CP_DIR" ]] || { echo "ERROR: Proposal not found: $CP_NAME"; exit 1; }
ops/plugins/proposals/bin/proposals-supersede:43:[[ -f "$MANIFEST" ]] || { echo "ERROR: No manifest.yaml in $CP_DIR"; exit 1; }
ops/plugins/proposals/bin/proposals-supersede:47:  echo "ERROR: Cannot supersede an already-applied proposal"
ops/plugins/network/bin/network-home-unifi-clients-snapshot:10:[[ -x "$AGENT" ]] || { echo "STOP (2): missing unifi-home-agent.sh"; exit 2; }
ops/plugins/audit/bin/audit-triage:29:  echo "ERROR: outbox directory not found: $OUTBOX" >&2
ops/plugins/audit/bin/audit-triage:34:  echo "ERROR: operational gaps file not found: $GAPS_FILE" >&2
ops/plugins/audit/bin/audit-triage:39:  echo "WARN: loop-scopes directory not found: $SCOPES_DIR (loop cross-ref disabled)" >&2
ops/plugins/proposals/bin/proposals-status:164:  echo "WARN: $sla_breaches SLA breaches, $malformed malformed"
ops/plugins/network/bin/network-lan-host-identify:15:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/network/bin/network-lan-host-identify:53:  -o "LogLevel=ERROR"
ops/plugins/infra/bin/infra-relocation-state-transition:32:fail() { echo "ERROR: $*" >&2; exit 1; }
ops/plugins/network/bin/network-shop-audit-status:14:#  2 STOP (preconditions)
ops/plugins/network/bin/network-shop-audit-status:25:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/ha/bin/ha-addons-snapshot:10:SSH_OPTS="-o ConnectTimeout=10 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o LogLevel=ERROR"
ops/plugins/ha/bin/ha-addons-snapshot:19:  echo "STOP (2): cannot reach $SSH_TARGET via SSH"
ops/plugins/ha/bin/ha-addons-snapshot:29:  echo "STOP (2): Supervisor API returned empty response"
ops/plugins/ha/bin/ha-addons-snapshot:40:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/observability/bin/prometheus-targets-status:15:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/observability/bin/prometheus-targets-status:46:    print('status: ERROR')
ops/plugins/observability/bin/prometheus-targets-status:79:  echo "status: PARSE_ERROR"
ops/plugins/observability/bin/observability-stack-status:16:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/audit/bin/agent-session-closeout:29:  echo "ERROR: ledger not found: $LEDGER" >&2
ops/plugins/audit/bin/agent-session-closeout:34:  echo "ERROR: loop-scopes directory not found: $SCOPES_DIR" >&2
ops/plugins/audit/bin/agent-session-closeout:158:STOP_WORDS = {
ops/plugins/audit/bin/agent-session-closeout:167:    return [w for w in words if len(w) > 2 and w not in STOP_WORDS]
ops/plugins/observability/bin/stability-control-snapshot:37:      echo "STOP: unknown argument: $1" >&2
ops/plugins/observability/bin/stability-control-snapshot:45:    echo "STOP: missing dependency: $1" >&2
ops/plugins/observability/bin/stability-control-snapshot:52:    echo "STOP: missing file: $1" >&2
ops/plugins/observability/bin/stability-control-snapshot:112:  if printf '%s\n' "$detail" | grep -qiE 'status:[[:space:]]*FAIL|STATUS:[[:space:]]*FAIL|summary:[[:space:]]*INCIDENT|DEGRADED|UNREACHABLE|disconnected|not connected|precondition failed|ERROR'; then
ops/plugins/observability/bin/stability-control-snapshot:183:  PROBE_DETAIL="$(grep -E 'status:[[:space:]]*FAIL|STATUS:[[:space:]]*FAIL|summary:[[:space:]]*INCIDENT|DEGRADED|UNREACHABLE|disconnected|not connected|precondition failed|ERROR|timeout' "$tmp_file" | head -n1 || true)"
ops/plugins/observability/bin/stability-control-snapshot:261:    -o "LogLevel=ERROR"
ops/plugins/observability/bin/stability-control-snapshot:386:  if [[ -n "$LOAD_PER_CORE_WARN" ]]; then
ops/plugins/observability/bin/stability-control-snapshot:387:    if awk -v v="$load_per_core" -v t="$LOAD_PER_CORE_WARN" 'BEGIN { exit !(v >= t) }'; then
ops/plugins/observability/bin/stability-control-snapshot:390:        detail_notes="load_per_core_high($load_per_core>=$LOAD_PER_CORE_WARN)"
ops/plugins/observability/bin/stability-control-snapshot:392:        detail_notes="$detail_notes; load_per_core_high($load_per_core>=$LOAD_PER_CORE_WARN)"
ops/plugins/observability/bin/stability-control-snapshot:438:LOAD_PER_CORE_WARN="$(yq -r '.predictive_thresholds.vm_load_per_core_warn // ""' "$CONTRACT")"
ops/plugins/observability/bin/stability-control-snapshot:439:API_LAT_WARN_MS="$(yq -r '.predictive_thresholds.api_latency_warn_ms // 2000' "$CONTRACT")"
ops/plugins/observability/bin/stability-control-snapshot:441:HEARTBEAT_WARN_MINUTES="$(yq -r '.predictive_thresholds.heartbeat_stale_warn_minutes // 10' "$CONTRACT")"
ops/plugins/observability/bin/stability-control-snapshot:443:WARN_TREATMENT_CRITICAL="$(yq -r '.warn_treatment.critical_domains // "incident"' "$CONTRACT")"
ops/plugins/observability/bin/stability-control-snapshot:444:WARN_TREATMENT_NON_CRITICAL="$(yq -r '.warn_treatment.non_critical_domains // "warn"' "$CONTRACT")"
ops/plugins/observability/bin/stability-control-snapshot:448:  echo "STOP: no critical domains configured in contract" >&2
ops/plugins/observability/bin/stability-control-snapshot:478:    _domain_lat_warn="$API_LAT_WARN_MS"
ops/plugins/observability/bin/stability-control-snapshot:521:      elif [[ "$hb_age" -ge "$HEARTBEAT_WARN_MINUTES" ]]; then
ops/plugins/observability/bin/stability-control-snapshot:525:        probe_detail="$probe_detail; heartbeat_age_minutes=$hb_age (>= $HEARTBEAT_WARN_MINUTES)"
ops/plugins/observability/bin/stability-control-snapshot:534:    if printf '%s\n' "$PROBE_DETAIL" | grep -qiE 'WARN'; then
ops/plugins/observability/bin/stability-control-snapshot:572:    if [[ "$WARN_TREATMENT_CRITICAL" == "incident" && "$domain_mode" != "maintenance" ]]; then
ops/plugins/observability/bin/stability-control-snapshot:575:        domain_note="warn_promoted_to_incident(policy=critical_domains:$WARN_TREATMENT_CRITICAL)"
ops/plugins/observability/bin/stability-control-snapshot:577:        domain_note="$domain_note; warn_promoted_to_incident(policy=critical_domains:$WARN_TREATMENT_CRITICAL)"
ops/plugins/observability/bin/stability-control-snapshot:654:    --arg warn_treatment_critical "$WARN_TREATMENT_CRITICAL" \
ops/plugins/observability/bin/stability-control-snapshot:655:    --arg warn_treatment_non_critical "$WARN_TREATMENT_NON_CRITICAL" \
ops/plugins/observability/bin/stability-control-snapshot:665:      --arg vm_load_per_core_warn "$LOAD_PER_CORE_WARN" \
ops/plugins/observability/bin/stability-control-snapshot:666:      --argjson api_latency_warn_ms "$API_LAT_WARN_MS" \
ops/plugins/observability/bin/stability-control-snapshot:668:      --argjson heartbeat_stale_warn_minutes "$HEARTBEAT_WARN_MINUTES" \
ops/plugins/observability/bin/stability-control-snapshot:712:  if [[ -n "$LOAD_PER_CORE_WARN" ]]; then
ops/plugins/observability/bin/stability-control-snapshot:713:    echo "predictive_thresholds.vm_load_per_core_warn: $LOAD_PER_CORE_WARN"
ops/plugins/observability/bin/stability-control-snapshot:715:  echo "predictive_thresholds.api_latency_warn_ms: $API_LAT_WARN_MS"
ops/plugins/observability/bin/stability-control-snapshot:717:  echo "predictive_thresholds.heartbeat_stale_warn_minutes: $HEARTBEAT_WARN_MINUTES"
ops/plugins/observability/bin/stability-control-snapshot:720:  echo "warn_treatment.critical_domains: $WARN_TREATMENT_CRITICAL"
ops/plugins/observability/bin/stability-control-snapshot:721:  echo "warn_treatment.non_critical_domains: $WARN_TREATMENT_NON_CRITICAL"
ops/plugins/observability/bin/stability-control-snapshot:744:    echo "summary: WARN ($warn_count critical domains warning)"
ops/plugins/network/bin/network-pve-post-cutover-harden:35:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/network/bin/network-pve-post-cutover-harden:75:  -o "LogLevel=ERROR"
ops/plugins/verify/bin/surface-audit-full:23:  elif [[ "$status" == "WARN" ]]; then
ops/plugins/verify/bin/surface-audit-full:127:    record "8. VM Authority Consistency" "WARN" "vm authority files exist but vm.governance.audit returned non-zero"
ops/plugins/verify/bin/surface-audit-full:143:    record "9. Predictive Control Readiness" "WARN" "snapshot output valid; runtime currently degraded (rc=$snapshot_rc)"
ops/plugins/verify/bin/surface-audit-full:167:  echo "- Summary: FAIL=$fail_count WARN=$warn_count"
ops/plugins/observability/bin/immich-status:15:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/network/bin/network-md1400-bind-test:22:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/network/bin/network-md1400-bind-test:64:  -o "LogLevel=ERROR"
ops/plugins/n8n/bin/n8n-workflows:56:  echo "STOP: N8N_API_KEY is not set." >&2
ops/plugins/n8n/bin/n8n-workflows:73:    echo "WARN: no workflows found or API error" >&2
ops/plugins/n8n/bin/n8n-workflows:95:  echo "STOP: missing or not executable: $WORKFLOWS_CLI" >&2
ops/plugins/network/bin/network-nvr-reip-canonical:20:#   2  STOP (missing deps)
ops/plugins/network/bin/network-nvr-reip-canonical:35:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/network/bin/network-nvr-reip-canonical:78:  -o "LogLevel=ERROR"
ops/plugins/ssh/bin/ssh-target-status:5:# STOP=2 on preconditions (missing binding, missing yq).
ops/plugins/ssh/bin/ssh-target-status:24:# STOP=2 for preconditions
ops/plugins/ssh/bin/ssh-target-status:26:  echo "STOP (2): missing binding file: $BINDING_FILE"
ops/plugins/ssh/bin/ssh-target-status:31:  echo "STOP (2): yq is required but not found"
ops/plugins/ssh/bin/ssh-target-status:50:        echo "STOP (2): --id requires a target id"
ops/plugins/ssh/bin/ssh-target-status:54:        echo "STOP (2): too many target selectors (use one target id)"
ops/plugins/ssh/bin/ssh-target-status:67:        echo "STOP (2): too many target selectors (use one target id)"
ops/plugins/ssh/bin/ssh-target-status:86:  echo "STOP (2): no ssh.targets configured in $BINDING_FILE"
ops/plugins/ssh/bin/ssh-target-status:141:    -o "LogLevel=ERROR"
ops/plugins/ssh/bin/ssh-target-status:188:      echo "- id: $id host: $host user: $user -> WARN (${dur_ms}ms) reason=${reason} optional=true"
ops/plugins/ssh/bin/ssh-target-status:200:  echo "STOP (2): target '$TARGET_FILTER' not found in binding"
ops/plugins/proposals/bin/proposals-list:32:    echo "WARN: $proposal_name — missing manifest.yaml, skipping"
ops/plugins/proposals/bin/proposals-list:60:    echo "WARN: $proposal_name — missing agent or created field"
surfaces/verify/d96-evidence-retention-policy-lock.sh:9:ERRORS=0
surfaces/verify/d96-evidence-retention-policy-lock.sh:10:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
surfaces/verify/d96-evidence-retention-policy-lock.sh:18:  echo "D96 FAIL: $ERRORS check(s) failed"
surfaces/verify/d96-evidence-retention-policy-lock.sh:108:if [[ "$ERRORS" -gt 0 ]]; then
surfaces/verify/d96-evidence-retention-policy-lock.sh:109:  echo "D96 FAIL: $ERRORS check(s) failed"
ops/plugins/infra/bin/infra-proxmox-maintenance:25:  echo "STOP (2): missing binding file: $SSH_BINDING"
ops/plugins/infra/bin/infra-proxmox-maintenance:29:  echo "STOP (2): yq is required but not found"
ops/plugins/infra/bin/infra-proxmox-maintenance:34:  echo "STOP (2): python3 required but not found"
ops/plugins/infra/bin/infra-proxmox-maintenance:62:      echo "STOP (2): unknown arg: $1"
ops/plugins/infra/bin/infra-proxmox-maintenance:68:[[ -n "$MODE" ]] || { echo "STOP (2): --mode required (precheck|shutdown|startup)"; exit 2; }
ops/plugins/infra/bin/infra-proxmox-maintenance:78:  echo "STOP (2): unknown host-id or missing host binding: $HOST_ID"
ops/plugins/infra/bin/infra-proxmox-maintenance:87:  -o "LogLevel=ERROR"
ops/plugins/infra/bin/infra-proxmox-maintenance:123:  ssh_pve 'lspci -nnk -s 82:00.0 2>/dev/null || echo "WARN: pm8072 not present at 82:00.0 (verify slot/bdf)"'
ops/plugins/infra/bin/infra-proxmox-maintenance:150:    echo "STOP (2): OOB guard failed; refusing to proceed with shutdown"
ops/plugins/infra/bin/infra-proxmox-maintenance:276:    echo "STOP (2): unknown mode: $MODE"
ops/plugins/docs/tests/docs-jd-status-test.sh:47:  echo "WARN: docs.jd.status returned non-zero (may have warnings)"
ops/plugins/network/bin/network-unifi-clients-snapshot:10:#   2  STOP (missing deps)
ops/plugins/network/bin/network-unifi-clients-snapshot:18:[[ -f "$AGENT" ]] || { echo "STOP (2): missing unifi-agent.sh at $AGENT"; exit 2; }
ops/plugins/network/bin/network-unifi-clients-snapshot:19:[[ -x "$AGENT" ]] || { echo "STOP (2): unifi-agent.sh not executable"; exit 2; }
ops/plugins/infra/bin/infra-relocation-service-transition:38:fail() { echo "ERROR: $*" >&2; exit 1; }
ops/plugins/n8n/bin/n8n-infra-health:11:    echo "STOP: missing dependency: $1" >&2
ops/plugins/n8n/bin/n8n-infra-health:18:    echo "STOP: missing file: $1" >&2
ops/plugins/tenant/tests/tenant-profile-validate-test.sh:41:  if echo "$out" | grep -q "ERROR"; then
ops/plugins/infra/bin/infra-vm-provision:38:fail() { echo "ERROR: $*" >&2; exit 1; }
ops/plugins/infra/bin/infra-vm-provision:184:qm status "$TEMPLATE" >/dev/null 2>&1 || { echo "ERROR: template vmid $TEMPLATE not found"; exit 1; }
ops/plugins/infra/bin/infra-vm-provision:186:  echo "ERROR: vmid $VMID already exists"
ops/plugins/infra/bin/infra-vm-provision:221:ssh_opts=(-o BatchMode=yes -o ConnectTimeout=8 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR)
ops/plugins/proposals/bin/proposals-apply:45:  echo "ERROR: Proposal not found: $PROPOSAL_DIR" >&2
ops/plugins/proposals/bin/proposals-apply:53:  echo "ERROR: Manifest not found: $manifest" >&2
ops/plugins/proposals/bin/proposals-apply:58:  echo "ERROR: Proposal already applied: $PROPOSAL_NAME" >&2
ops/plugins/proposals/bin/proposals-apply:67:  echo "ERROR: Proposal is superseded: $PROPOSAL_NAME" >&2
ops/plugins/proposals/bin/proposals-apply:74:  echo "ERROR: Proposal is read-only (no file changes): $PROPOSAL_NAME" >&2
ops/plugins/proposals/bin/proposals-apply:81:  echo "ERROR: Working tree is not clean. Commit/stash before applying proposals." >&2
ops/plugins/proposals/bin/proposals-apply:131:  echo "ERROR: No actionable changes in manifest (changes may be empty or use nonstandard format): $manifest" >&2
ops/plugins/proposals/bin/proposals-apply:157:      echo "ERROR: Invalid action '$action' in $manifest" >&2
ops/plugins/proposals/bin/proposals-apply:164:    echo "ERROR: Unsafe/invalid path '$path' in $manifest" >&2
ops/plugins/proposals/bin/proposals-apply:179:        echo "ERROR: Missing proposal file for $action: $src" >&2
ops/plugins/proposals/bin/proposals-apply:184:        echo "ERROR: Proposal file must not be a symlink: $src" >&2
ops/plugins/proposals/bin/proposals-apply:202:        echo "WARN: Delete requested but file missing: $path"
ops/plugins/proposals/bin/proposals-apply:215:  echo "ERROR: One or more changes could not be applied. Aborting." >&2
ops/plugins/proposals/bin/proposals-apply:220:  echo "ERROR: No staged changes after applying proposal. Refusing to commit." >&2
ops/plugins/ha/bin/ha-scenes-snapshot:25:command -v python3 >/dev/null 2>&1 || { echo "STOP (2): python3 not found"; exit 2; }
ops/plugins/ha/bin/ha-scenes-snapshot:26:command -v curl >/dev/null 2>&1 || { echo "STOP (2): curl not found"; exit 2; }
ops/plugins/ha/bin/ha-scenes-snapshot:33:  echo "STOP (2): infisical-agent.sh not found"
ops/plugins/ha/bin/ha-scenes-snapshot:40:  echo "STOP (2): could not retrieve HA_API_TOKEN from Infisical"
ops/plugins/ha/bin/ha-scenes-snapshot:53:  echo "STOP (2): /api/states returned HTTP $HTTP_CODE"
ops/plugins/ha/bin/ha-scenes-snapshot:70:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/vm/bin/vm-profile-audit:19:_stop() { echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/verify/bin/drift-gates-failure-stats:22:  echo "ERROR: missing receipts root: $RECEIPTS_ROOT" >&2
ops/plugins/verify/bin/drift-gates-failure-stats:45:pat = re.compile(r"^(D\d+)\s+.*?\b(FAIL|PASS|WARN)\b", re.IGNORECASE)
ops/plugins/verify/bin/drift-gates-failure-stats:72:    elif status == "WARN":
ops/plugins/verify/bin/drift-gates-failure-stats:95:print("top WARN gates (count):")
ops/plugins/docs/bin/docs-orphan-detect:13:  echo "STOP: docs/README.md not found at $README" >&2
ops/plugins/network/bin/network-home-dhcp-audit:19:command -v yq >/dev/null 2>&1 || { echo "STOP (2): missing dependency: yq"; exit 2; }
ops/plugins/network/bin/network-home-dhcp-audit:20:command -v jq >/dev/null 2>&1 || { echo "STOP (2): missing dependency: jq"; exit 2; }
ops/plugins/network/bin/network-home-dhcp-audit:21:[[ -f "$REGISTRY" ]] || { echo "STOP (2): home.device.registry.yaml not found"; exit 2; }
ops/plugins/network/bin/network-home-dhcp-audit:29:  echo "STOP (2): UNIFI_HOME_API_KEY not available (check Infisical home-assistant/prod)"
ops/plugins/network/bin/network-home-dhcp-audit:38:SSH_OPTS="-o ConnectTimeout=10 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o LogLevel=ERROR"
ops/plugins/network/bin/network-home-dhcp-audit:63:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/network/bin/network-home-dhcp-audit:100:        print(f"  WARN: {name} ({mac}) — not found in UniFi client list")
ops/plugins/network/bin/network-home-dhcp-audit:145:print(f"RESULT: {ok_count} OK, {errors} FAIL, {warnings} WARN")
ops/plugins/n8n/bin/n8n-snapshot-cron:51:  log "WARN: zero workflows exported — snapshot directory will be pruned"
ops/plugins/github/bin/github-labels-status:8:command -v gh >/dev/null 2>&1 || { echo "STOP: gh not installed"; exit 2; }
ops/plugins/github/bin/github-labels-status:9:command -v yq >/dev/null 2>&1 || { echo "STOP: yq not installed"; exit 2; }
ops/plugins/github/bin/github-labels-status:10:command -v git >/dev/null 2>&1 || { echo "STOP: git not installed"; exit 2; }
ops/plugins/github/bin/github-labels-status:15:  echo "STOP: not inside a git repository"
ops/plugins/github/bin/github-labels-status:21:  echo "STOP: .github/labels.yml not found"
ops/plugins/github/bin/github-labels-status:28:  echo "STOP: no git remote 'origin' found"
ops/plugins/github/bin/github-labels-status:37:  echo "STOP: could not parse repo from origin: $ORIGIN_URL"
ops/plugins/network/bin/network-home-wifi-create:10:[[ -x "$AGENT" ]] || { echo "STOP (2): missing unifi-home-agent.sh"; exit 2; }
ops/plugins/ha/bin/ha-backup-create:9:command -v yq >/dev/null 2>&1 || { echo "STOP (2): missing dependency: yq"; exit 2; }
ops/plugins/ha/bin/ha-backup-create:23:SSH_OPTS="-o ConnectTimeout=15 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o LogLevel=ERROR"
ops/plugins/tenant/bin/tenant-profile-validate:28:command -v yq >/dev/null 2>&1 || { echo "ERROR: yq is required but not found"; exit 1; }
ops/plugins/tenant/bin/tenant-profile-validate:31:[[ -f "$PROFILE" ]] || { echo "ERROR: Profile not found: $PROFILE"; exit 1; }
ops/plugins/tenant/bin/tenant-profile-validate:32:[[ -f "$SCHEMA" ]] || { echo "ERROR: Schema not found: $SCHEMA"; exit 1; }
ops/plugins/tenant/bin/tenant-profile-validate:34:ERRORS=0
ops/plugins/tenant/bin/tenant-profile-validate:35:WARNINGS=0
ops/plugins/tenant/bin/tenant-profile-validate:36:err() { echo "  FAIL: $*" >&2; ERRORS=$((ERRORS + 1)); }
ops/plugins/tenant/bin/tenant-profile-validate:37:warn() { echo "  WARN: $*" >&2; WARNINGS=$((WARNINGS + 1)); }
ops/plugins/tenant/bin/tenant-profile-validate:216:if [[ "$ERRORS" -gt 0 ]]; then
ops/plugins/tenant/bin/tenant-profile-validate:217:  echo "VALIDATION FAILED: $ERRORS error(s), $WARNINGS warning(s)"
ops/plugins/tenant/bin/tenant-profile-validate:220:  echo "VALIDATION PASSED: 0 errors, $WARNINGS warning(s)"
ops/plugins/infra/bin/infra-proxmox-node-path-migrate:43:fail() { echo "ERROR: $*" >&2; exit 1; }
ops/plugins/infra/bin/infra-proxmox-node-path-migrate:73:ssh_opts=(-o BatchMode=yes -o ConnectTimeout=8 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR)
ops/plugins/github/bin/github-status:5:# Exit codes: 0 OK, 2 STOP (missing deps / not in git repo), 1 FAIL
ops/plugins/github/bin/github-status:7:command -v git >/dev/null 2>&1 || { echo "STOP: git not found"; exit 2; }
ops/plugins/github/bin/github-status:8:git rev-parse --is-inside-work-tree >/dev/null 2>&1 || { echo "STOP: not a git repo"; exit 2; }
ops/plugins/docs/bin/docs-brain-freshness:12:  echo "STOP: docs/brain/ not found at $BRAIN" >&2
ops/plugins/docs/bin/docs-brain-freshness:22:WARN_COUNT=0
ops/plugins/docs/bin/docs-brain-freshness:53:    echo "WARN     ${age_days}d  $rel  (last: $date_str)"
ops/plugins/docs/bin/docs-brain-freshness:54:    WARN_COUNT=$((WARN_COUNT+1))
ops/plugins/docs/bin/docs-brain-freshness:63:echo "warn: $WARN_COUNT"
ops/plugins/home/bin/home-vm-status:15:    echo "ERROR: Cannot reach proxmox-home via SSH"
ops/plugins/home/bin/home-vm-status:22:    "qm list" 2>/dev/null || echo "ERROR: qm list failed"
ops/plugins/home/bin/home-vm-status:28:    "pct list" 2>/dev/null || echo "ERROR: pct list failed"
ops/plugins/vm/bin/vm-governance-audit:23:_stop() { echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/tenant/bin/tenant-provision-dry-run:27:command -v yq >/dev/null 2>&1 || { echo "ERROR: yq is required but not found"; exit 1; }
ops/plugins/tenant/bin/tenant-provision-dry-run:28:[[ -f "$PROFILE" ]] || { echo "ERROR: Profile not found: $PROFILE"; exit 1; }
ops/plugins/tenant/bin/tenant-provision-dry-run:50:  WARN_POLICY="$(yq -r ".presets.$POLICY_PRESET.knobs.warn_policy // \"advisory\"" "$PRESETS")"
ops/plugins/tenant/bin/tenant-provision-dry-run:56:  WARN_POLICY="advisory"
ops/plugins/tenant/bin/tenant-provision-dry-run:99:echo "  Warn policy:      $WARN_POLICY"
ops/plugins/infra/bin/infra-hypervisor-identity-status:40:fail() { echo "ERROR: $*" >&2; exit 1; }
ops/plugins/infra/bin/infra-hypervisor-identity-status:126:    ssh_opts=(-o BatchMode=yes -o ConnectTimeout=6 -o ControlMaster=no -o ControlPersist=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR)
ops/plugins/infra/bin/infra-hypervisor-identity-status:169:            echo "  WARN: hostname mismatch allowed (expected '$expected_hn', got '$remote_hn')"
ops/plugins/infra/bin/infra-hypervisor-identity-status:177:            echo "  WARN: duplicate hostname '$remote_hn' allowed by naming policy exception"
ops/plugins/network/bin/network-oob-guard-status:24:  echo "STOP (2): missing binding file: $SSH_BINDING"
ops/plugins/network/bin/network-oob-guard-status:28:  echo "STOP (2): yq is required but not found"
ops/plugins/network/bin/network-oob-guard-status:33:  echo "STOP (2): tailscale CLI required but not found"
ops/plugins/network/bin/network-oob-guard-status:37:  echo "STOP (2): python3 required for JSON parsing but not found"
ops/plugins/network/bin/network-oob-guard-status:48:      [[ -n "$SUBNET" ]] || { echo "STOP (2): --subnet requires a value"; exit 2; }
ops/plugins/network/bin/network-oob-guard-status:60:      echo "STOP (2): unknown arg: $1"
ops/plugins/github/bin/github-actions-status:8:command -v gh >/dev/null 2>&1 || { echo "STOP: gh not installed"; exit 2; }
ops/plugins/github/bin/github-actions-status:11:gh auth status >/dev/null 2>&1 || { echo "STOP: gh not authenticated"; exit 2; }
ops/plugins/github/bin/github-actions-status:16:  echo "STOP: could not determine repo from gh repo view"
ops/plugins/docs/bin/infra-extraction-status:9:_need() { command -v "$1" >/dev/null 2>&1 || { echo "STOP: missing dependency: $1" >&2; exit 2; }; }
ops/plugins/network/bin/network-ap-facts-capture:17:#   2 STOP (preconditions)
ops/plugins/network/bin/network-ap-facts-capture:30:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/network/bin/network-ap-facts-capture:65:PROXY_CMD="ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o LogLevel=ERROR -W %h:%p ${PROBE_USER}@${PROBE_HOST}"
ops/plugins/network/bin/network-ap-facts-capture:75:    -o "LogLevel=ERROR" \
ops/plugins/home/bin/home-health-alert:22:  echo "WARN: HA_API_TOKEN unavailable -- running probe-only (no alerting)"
ops/plugins/home/bin/home-health-alert:74:    echo "WARN: HA notification failed (HTTP $HTTP_CODE)"
ops/plugins/tenant/bin/tenant-storage-audit:16:ERRORS=0
ops/plugins/tenant/bin/tenant-storage-audit:42:echo "Summary: $CHECKS checked, $ERRORS contract errors"
ops/plugins/tenant/bin/tenant-storage-audit:44:if [[ "$ERRORS" -gt 0 ]]; then
ops/plugins/services/bin/services-health-status:5:# STOP=2 on preconditions (missing binding, yq, curl).
ops/plugins/services/bin/services-health-status:20:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/services/bin/services-health-status:148:  echo "STOP (2): no endpoints matched filter(s): id='${FILTER_ID:-*}' host='${FILTER_HOST:-*}'"
ops/plugins/infra/bin/infra-post-power-recovery:27:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/infra/bin/infra-post-power-recovery:87:      -o "NumberOfPasswordPrompts=0" -o "LogLevel=ERROR" \
ops/plugins/infra/bin/infra-post-power-recovery:106:      -o "NumberOfPasswordPrompts=0" -o "LogLevel=ERROR" \
ops/plugins/infra/bin/infra-post-power-recovery:176:      -o "NumberOfPasswordPrompts=0" -o "LogLevel=ERROR" \
ops/plugins/ha/bin/ha-zwave-devices-snapshot:20:  echo "STOP (2): infisical-agent.sh not found"
ops/plugins/ha/bin/ha-zwave-devices-snapshot:26:  echo "STOP (2): could not retrieve HA_API_TOKEN"
ops/plugins/ha/bin/ha-zwave-devices-snapshot:34:  echo "STOP (2): HA unreachable"
ops/plugins/ha/bin/ha-zwave-devices-snapshot:59:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/network/bin/network-md1400-pm8072-stage:24:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/network/bin/network-md1400-pm8072-stage:66:  -o "LogLevel=ERROR"
ops/plugins/n8n/bin/n8n-snapshot-status:66:  echo "WARN: latest snapshot is older than 26 hours (stale)"
ops/plugins/home/bin/home-backup-status:17:    echo "ERROR: Cannot reach proxmox-home via SSH"
ops/plugins/infra/bin/infra-vm-ready-status:31:fail() { echo "ERROR: $*" >&2; exit 1; }
ops/plugins/infra/bin/infra-vm-ready-status:113:ssh_opts=(-o BatchMode=yes -o ConnectTimeout=8 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR)
ops/plugins/ha/bin/ha-z2m-devices-snapshot:23:  echo "STOP (2): cannot reach $SSH_TARGET via SSH"
ops/plugins/ha/bin/ha-z2m-devices-snapshot:37:  echo "WARN: Could not discover Z2M slug, using default: $Z2M_CONTAINER"
ops/plugins/ha/bin/ha-z2m-devices-snapshot:55:  echo "STOP (2): Z2M database.db is empty or inaccessible"
ops/plugins/ha/bin/ha-z2m-devices-snapshot:65:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/docs/bin/docs-jd-status:54:WARN=0
ops/plugins/docs/bin/docs-jd-status:159:  echo "WARN: Unmapped docs (may need JD ID or exclusion):"
ops/plugins/docs/bin/docs-jd-status:161:  WARN=$((WARN+1))
ops/plugins/docs/bin/docs-jd-status:171:echo "WARN: $WARN"
ops/plugins/docs/bin/docs-jd-status:178:elif [[ $WARN -gt 0 ]]; then
ops/plugins/docs/bin/docs-jd-status:179:  echo "status: WARN"
ops/plugins/ha/bin/ha-hacs-snapshot:13:SSH_OPTS="-o ConnectTimeout=10 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o LogLevel=ERROR"
ops/plugins/ha/bin/ha-hacs-snapshot:28:command -v python3 >/dev/null 2>&1 || { echo "STOP (2): python3 not found"; exit 2; }
ops/plugins/ha/bin/ha-hacs-snapshot:29:command -v curl >/dev/null 2>&1 || { echo "STOP (2): curl not found"; exit 2; }
ops/plugins/ha/bin/ha-hacs-snapshot:61:    echo "STOP (2): could not retrieve HA_API_TOKEN from Infisical"
ops/plugins/ha/bin/ha-hacs-snapshot:69:    echo "WARN: HACS update entity not found (HTTP $HTTP_CODE) — writing empty binding"
ops/plugins/ha/bin/ha-hacs-snapshot:87:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/docs/bin/docs-status:10:_need() { command -v "$1" >/dev/null 2>&1 || { echo "STOP: missing dependency: $1" >&2; exit 2; }; }
ops/plugins/docs/bin/docs-status:14:  echo "STOP: docs/infrastructure not found at $DOCS_DIR" >&2
ops/plugins/network/bin/network-shop-audit-canonical:13:#  2  STOP (missing deps, pve unreachable)
ops/plugins/network/bin/network-shop-audit-canonical:26:stop(){ echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/network/bin/network-shop-audit-canonical:60:  echo "STOP: local parity audit returned STOP"
ops/plugins/network/bin/network-shop-audit-canonical:153:  -o "LogLevel=ERROR"
ops/plugins/network/bin/network-shop-audit-canonical:158:  echo "STOP (2): cannot reach pve at ${PVE_USER}@${PVE_HOST}"
ops/plugins/network/bin/network-shop-audit-canonical:219:  echo "WARN: probe script had errors (partial results may follow)"
ops/plugins/docs/bin/docs-lint:12:_need() { command -v "$1" >/dev/null 2>&1 || { echo "STOP: missing dependency: $1" >&2; exit 2; }; }
ops/plugins/docs/bin/docs-lint:16:  echo "STOP: docs/ not found at $DOCS" >&2
ops/plugins/docs/bin/docs-lint:24:ERRORS=0
ops/plugins/docs/bin/docs-lint:25:WARNINGS=0
ops/plugins/docs/bin/docs-lint:27:err()  { echo "ERROR: $*"; ERRORS=$((ERRORS+1)); }
ops/plugins/docs/bin/docs-lint:28:warn() { echo "WARN:  $*"; WARNINGS=$((WARNINGS+1)); }
ops/plugins/docs/bin/docs-lint:284:echo "errors: $ERRORS"
ops/plugins/docs/bin/docs-lint:285:echo "warnings: $WARNINGS"
ops/plugins/docs/bin/docs-lint:287:if [[ "$ERRORS" -gt 0 ]]; then
ops/plugins/ha/bin/ha-dashboard-backup:14:SSH_OPTS="-o ConnectTimeout=10 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o LogLevel=ERROR"
ops/plugins/ha/bin/ha-dashboard-backup:28:command -v python3 >/dev/null 2>&1 || { echo "STOP (2): python3 not found"; exit 2; }
ops/plugins/ha/bin/ha-dashboard-backup:31:  echo "STOP (2): ha.dashboards.yaml binding not found. Run ha.dashboard.snapshot first."
ops/plugins/ha/bin/ha-dashboard-backup:37:  echo "STOP (2): SSH to HA failed"
ops/plugins/ha/bin/ha-dashboard-backup:74:    echo "    WARN: failed to read $filename"
ops/plugins/ha/bin/ha-config-extract:31:  echo "status: STOP"
ops/plugins/network/bin/network-home-dhcp-reservation-create:10:[[ -x "$AGENT" ]] || { echo "STOP (2): missing unifi-home-agent.sh"; exit 2; }
ops/plugins/infra/bin/infra-timezone-set:37:fail() { echo "ERROR: $*" >&2; exit 1; }
ops/plugins/infra/bin/infra-timezone-set:57:ssh_opts=(-o BatchMode=yes -o ConnectTimeout=6 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR)
ops/plugins/docs/bin/docs-index-verify:11:_need() { command -v "$1" >/dev/null 2>&1 || { echo "STOP: missing dependency: $1" >&2; exit 2; }; }
ops/plugins/docs/bin/docs-index-verify:15:  echo "STOP: docs/governance/ not found at $GOV" >&2
ops/plugins/docs/bin/docs-index-verify:20:  echo "STOP: _index.yaml not found at $INDEX" >&2
ops/plugins/docs/bin/docs-index-verify:28:ERRORS=0
ops/plugins/docs/bin/docs-index-verify:30:err()  { echo "ERROR: $*"; ERRORS=$((ERRORS+1)); }
ops/plugins/docs/bin/docs-index-verify:92:echo "errors: $ERRORS"
ops/plugins/docs/bin/docs-index-verify:94:if [[ "$ERRORS" -gt 0 ]]; then
ops/plugins/backup/bin/backup-vzdump-status:10:_need() { command -v "$1" >/dev/null 2>&1 || { echo "STOP: missing dependency: $1" >&2; exit 2; }; }
ops/plugins/backup/bin/backup-vzdump-status:16:  echo "STOP: missing ssh targets binding: $SSH_TARGETS_FILE" >&2
ops/plugins/backup/bin/backup-vzdump-status:34:  echo "STOP: unknown ssh target id: $TARGET_ID" >&2
ops/plugins/backup/bin/backup-vzdump-status:44:  -o "LogLevel=ERROR"
ops/plugins/docs/bin/docs-frontmatter-lint:11:  echo "STOP: docs/governance/ not found at $GOV" >&2
ops/plugins/docs/bin/docs-frontmatter-lint:19:ERRORS=0
ops/plugins/docs/bin/docs-frontmatter-lint:20:WARNINGS=0
ops/plugins/docs/bin/docs-frontmatter-lint:23:err()  { echo "ERROR: $*"; ERRORS=$((ERRORS+1)); }
ops/plugins/docs/bin/docs-frontmatter-lint:24:warn() { echo "WARN:  $*"; WARNINGS=$((WARNINGS+1)); }
ops/plugins/docs/bin/docs-frontmatter-lint:92:      WARNINGS=$((WARNINGS+1))
ops/plugins/docs/bin/docs-frontmatter-lint:137:echo "errors: $ERRORS"
ops/plugins/docs/bin/docs-frontmatter-lint:138:echo "warnings: $WARNINGS"
ops/plugins/docs/bin/docs-frontmatter-lint:140:if [[ "$ERRORS" -gt 0 ]]; then
ops/plugins/ha/bin/ha-scripts-snapshot:25:command -v python3 >/dev/null 2>&1 || { echo "STOP (2): python3 not found"; exit 2; }
ops/plugins/ha/bin/ha-scripts-snapshot:26:command -v curl >/dev/null 2>&1 || { echo "STOP (2): curl not found"; exit 2; }
ops/plugins/ha/bin/ha-scripts-snapshot:33:  echo "STOP (2): infisical-agent.sh not found"
ops/plugins/ha/bin/ha-scripts-snapshot:40:  echo "STOP (2): could not retrieve HA_API_TOKEN from Infisical"
ops/plugins/ha/bin/ha-scripts-snapshot:53:  echo "STOP (2): /api/states returned HTTP $HTTP_CODE"
ops/plugins/ha/bin/ha-scripts-snapshot:70:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/ha/bin/ha-status:96:  STOPPED=$(yq e '[.addons[] | select(.state == "stopped")] | length' "$ADDONS_FILE" 2>/dev/null || echo "0")
ops/plugins/ha/bin/ha-status:97:  ERRORS=$(yq e '[.addons[] | select(.state == "error")] | length' "$ADDONS_FILE" 2>/dev/null || echo "0")
ops/plugins/ha/bin/ha-status:101:  printf "│  %-20s %s\n" "Stopped:" "$STOPPED"
ops/plugins/ha/bin/ha-status:103:  if (( ERRORS > 0 )); then
ops/plugins/ha/bin/ha-status:104:    printf "│  %-20s \033[31m%s\033[0m\n" "Errors:" "$ERRORS"
ops/plugins/docs/bin/docs-impact-status:9:    echo "STOP: missing dependency: $1" >&2
ops/plugins/docs/bin/docs-impact-status:16:    echo "STOP: missing file: $1" >&2
ops/plugins/docs/bin/docs-impact-status:27:  echo "STOP: docs impact contract is missing defaults.workbench_root" >&2
ops/plugins/ha/bin/ha-service-call:80:command -v curl >/dev/null 2>&1 || { echo "status: STOP"; echo "reason: missing_dep_curl"; exit 2; }
ops/plugins/ha/bin/ha-service-call:81:command -v jq >/dev/null 2>&1  || { echo "status: STOP"; echo "reason: missing_dep_jq"; exit 2; }
ops/plugins/ha/bin/ha-service-call:84:[[ -x "$INFISICAL_AGENT" ]] || { echo "status: STOP"; echo "reason: infisical_agent_not_found"; exit 2; }
ops/plugins/ha/bin/ha-service-call:93:  echo "status: STOP"
ops/plugins/docs/bin/docs-impact-note:9:    echo "STOP: missing dependency: $1" >&2
ops/plugins/docs/bin/docs-impact-note:16:    echo "STOP: missing file: $1" >&2
ops/plugins/docs/bin/docs-impact-note:52:  echo "STOP: unknown domain '$DOMAIN' in $CONTRACT" >&2
ops/plugins/docs/bin/docs-impact-note:61:  echo "STOP: runbook missing for domain '$DOMAIN': $RUNBOOK_ABS" >&2
ops/plugins/docs/bin/docs-sprawl-detect:12:  echo "STOP: docs/ not found at $DOCS" >&2
ops/plugins/docs/bin/docs-sprawl-detect:106:  echo "WARN: governance/ exceeds 100 docs ($count_governance) — consider consolidation"
ops/plugins/backup/bin/backup-vzdump-vmid-set:13:_need() { command -v "$1" >/dev/null 2>&1 || { echo "STOP: missing dependency: $1" >&2; exit 2; }; }
ops/plugins/backup/bin/backup-vzdump-vmid-set:20:  echo "STOP: missing ssh targets binding: $SSH_TARGETS_FILE" >&2
ops/plugins/backup/bin/backup-vzdump-vmid-set:39:    *) echo "STOP: unknown arg: $1" >&2; exit 2;;
ops/plugins/backup/bin/backup-vzdump-vmid-set:44:  echo "STOP: --vmids is required" >&2
ops/plugins/backup/bin/backup-vzdump-vmid-set:60:  echo "STOP: unknown ssh target id: $TARGET_ID" >&2
ops/plugins/backup/bin/backup-vzdump-vmid-set:70:  -o "LogLevel=ERROR"
ops/plugins/backup/bin/backup-vzdump-vmid-set:86:  echo "STOP: missing $CFG" >&2
ops/plugins/backup/bin/backup-vzdump-vmid-set:92:  # If job not provided, select the only vzdump job, else STOP.
ops/plugins/backup/bin/backup-vzdump-vmid-set:95:    echo "STOP: no vzdump jobs found in $CFG" >&2
ops/plugins/backup/bin/backup-vzdump-vmid-set:99:    echo "STOP: multiple vzdump jobs found; pass --job <id>" >&2
ops/plugins/backup/bin/backup-vzdump-vmid-set:148:    raise SystemExit(f"STOP: vzdump job '{job}' not found")
ops/plugins/ha/bin/ha-addon-restart:11:SSH_OPTS="-o ConnectTimeout=10 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o LogLevel=ERROR"
ops/plugins/ha/bin/ha-addon-restart:43:    echo "WARN: slug '$SLUG' not found in ha.addons.yaml — proceeding anyway"
ops/plugins/ha/bin/ha-addon-restart:54:  echo "status: STOP"
ops/plugins/ha/bin/ha-addon-restart:78:    echo "  status: WARN"
ops/plugins/ha/bin/ha-ssot-apply:20:command -v python3 >/dev/null 2>&1 || { echo "status: STOP"; echo "reason: missing_dep_python3"; exit 2; }
ops/plugins/ha/bin/ha-ssot-apply:23:  echo "status: STOP"
ops/plugins/ha/bin/ha-ssot-apply:29:  echo "status: STOP"
ops/plugins/ha/bin/ha-ssot-apply:47:    echo "status: STOP"
ops/plugins/ha/bin/ha-ssot-apply:62:# Check for STOP
ops/plugins/ha/bin/ha-ssot-apply:63:if echo "$PROPOSE_OUTPUT" | grep -q "^status: STOP"; then
ops/plugins/ha/bin/ha-ssot-apply:66:  echo "apply: aborted (propose reported STOP)"
ops/plugins/ha/bin/ha-ssot-apply:74:  echo "status: STOP"
ops/plugins/ha/bin/ha-ssot-apply:106:    print("status: STOP")
ops/plugins/ha/bin/ha-sync-agent:89:    log "ERROR: Snapshot failed: $snapshot_cap"
ops/plugins/ha/bin/ha-sync-agent:229:      log "ERROR: WebSocket auth failed"
ops/plugins/docs/bin/docs-freshness-audit:13:  echo "STOP: docs/governance/ not found at $GOV" >&2
ops/plugins/docs/bin/docs-freshness-audit:23:WARN_COUNT=0
ops/plugins/docs/bin/docs-freshness-audit:68:      echo "WARN     ${age_days}d  $rel  (last: $date_str)"
ops/plugins/docs/bin/docs-freshness-audit:69:      WARN_COUNT=$((WARN_COUNT+1))
ops/plugins/docs/bin/docs-freshness-audit:91:echo "warn: $WARN_COUNT"
ops/plugins/ha/bin/ha-automation-create:57:command -v curl >/dev/null 2>&1 || { echo "status: STOP"; echo "reason: missing_dep_curl"; exit 2; }
ops/plugins/ha/bin/ha-automation-create:58:command -v jq >/dev/null 2>&1  || { echo "status: STOP"; echo "reason: missing_dep_jq"; exit 2; }
ops/plugins/ha/bin/ha-automation-create:59:[[ -x "$INFISICAL_AGENT" ]] || { echo "status: STOP"; echo "reason: infisical_agent_not_found"; exit 2; }
ops/plugins/ha/bin/ha-automation-create:64:  echo "status: STOP"
ops/plugins/backup/bin/backup-vzdump-prune:12:need() { command -v "$1" >/dev/null 2>&1 || { echo "STOP: missing dependency: $1" >&2; exit 2; }; }
ops/plugins/backup/bin/backup-vzdump-prune:18:  echo "STOP: missing ssh targets binding: $SSH_TARGETS_FILE" >&2
ops/plugins/backup/bin/backup-vzdump-prune:52:    *) echo "STOP: unknown arg: $1" >&2; usage >&2; exit 2 ;;
ops/plugins/backup/bin/backup-vzdump-prune:57:  echo "STOP: --keep-last must be an integer (got: $KEEP_LAST)" >&2
ops/plugins/backup/bin/backup-vzdump-prune:63:  *) echo "STOP: --type must be qemu|lxc (got: $TYPE)" >&2; exit 2 ;;
ops/plugins/backup/bin/backup-vzdump-prune:78:  echo "STOP: unknown ssh target id: $TARGET_ID" >&2
ops/plugins/backup/bin/backup-vzdump-prune:88:  -o "LogLevel=ERROR"
ops/plugins/mcp/bin/mcp-runtime-status:85:    echo "[$surface] WARN: no surface path configured"
ops/plugins/mcp/bin/mcp-runtime-status:103:    echo "  WARN: no MCP servers discovered"
ops/plugins/ha/bin/ha-helpers-snapshot:26:command -v python3 >/dev/null 2>&1 || { echo "STOP (2): python3 not found"; exit 2; }
ops/plugins/ha/bin/ha-helpers-snapshot:27:command -v curl >/dev/null 2>&1 || { echo "STOP (2): curl not found"; exit 2; }
ops/plugins/ha/bin/ha-helpers-snapshot:34:  echo "STOP (2): infisical-agent.sh not found"
ops/plugins/ha/bin/ha-helpers-snapshot:41:  echo "STOP (2): could not retrieve HA_API_TOKEN from Infisical"
ops/plugins/ha/bin/ha-helpers-snapshot:54:  echo "STOP (2): /api/states returned HTTP $HTTP_CODE"
ops/plugins/ha/bin/ha-helpers-snapshot:71:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/ha/bin/ha-hacs-updates-check:19:command -v python3 >/dev/null 2>&1 || { echo "STOP (2): python3 not found"; exit 2; }
ops/plugins/ha/bin/ha-hacs-updates-check:22:  echo "STOP (2): ha.hacs.yaml binding not found. Run ha.hacs.snapshot first."
ops/plugins/ha/bin/ha-hacs-updates-check:36:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/ha/bin/ha-hacs-updates-check:45:    print("WARN: HACS binding is in fallback mode — no repo data available.")
ops/plugins/ha/bin/ha-integrations-snapshot:25:command -v python3 >/dev/null 2>&1 || { echo "STOP (2): python3 not found"; exit 2; }
ops/plugins/ha/bin/ha-integrations-snapshot:26:command -v curl >/dev/null 2>&1 || { echo "STOP (2): curl not found"; exit 2; }
ops/plugins/ha/bin/ha-integrations-snapshot:33:  echo "STOP (2): infisical-agent.sh not found"
ops/plugins/ha/bin/ha-integrations-snapshot:40:  echo "STOP (2): could not retrieve HA_API_TOKEN from Infisical"
ops/plugins/ha/bin/ha-integrations-snapshot:53:  echo "STOP (2): /api/config/config_entries/entry returned HTTP $HTTP_CODE"
ops/plugins/ha/bin/ha-integrations-snapshot:70:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/mailroom-bridge/bin/mailroom-bridge-start:23:  echo "ERROR: serve script not executable: $SERVE"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-start:27:  echo "ERROR: missing binding: $BINDING"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-start:64:  echo "STOP (2): auth.require_token=true but no token is available (${token_env} or ${token_file})"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-start:116:  echo "STOP (2): launchctl required (macOS launchd)"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-start:199:  echo "ERROR: service did not start (no PID from launchctl)"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-start:221:echo "ERROR: /health probe failed: ${health_url}"
ops/plugins/ha/bin/ha-ssot-baseline-build:26:command -v python3 >/dev/null 2>&1 || { echo "STOP (2): python3 not found"; exit 2; }
ops/plugins/ha/bin/ha-ssot-baseline-build:27:command -v curl >/dev/null 2>&1 || { echo "STOP (2): curl not found"; exit 2; }
ops/plugins/ha/bin/ha-ssot-baseline-build:34:  echo "STOP (2): infisical-agent.sh not found"
ops/plugins/ha/bin/ha-ssot-baseline-build:41:  echo "STOP (2): could not retrieve HA_API_TOKEN from Infisical"
ops/plugins/ha/bin/ha-ssot-baseline-build:53:SSH_OPTS="-o ConnectTimeout=10 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o LogLevel=ERROR"
ops/plugins/ha/bin/ha-ssot-baseline-build:71:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/ha/bin/ha-health-status:18:command -v curl >/dev/null 2>&1 || { echo "status: STOP"; echo "reason: missing_dep_curl"; exit 2; }
ops/plugins/ha/bin/ha-health-status:19:command -v jq >/dev/null 2>&1  || { echo "status: STOP"; echo "reason: missing_dep_jq"; exit 2; }
ops/plugins/ha/bin/ha-health-status:22:  echo "status: STOP"
ops/plugins/ha/bin/ha-health-status:35:  echo "status: STOP"
ops/plugins/mcp/bin/mcp-inventory-status:11:  echo "STOP: missing or non-executable MCP inventory check: $CHECK" >&2
ops/plugins/ha/bin/ha-ssot-propose:20:command -v yq >/dev/null 2>&1 || { echo "status: STOP"; echo "reason: missing_dep_yq"; exit 2; }
ops/plugins/ha/bin/ha-ssot-propose:21:command -v jq >/dev/null 2>&1 || { echo "status: STOP"; echo "reason: missing_dep_jq"; exit 2; }
ops/plugins/ha/bin/ha-ssot-propose:22:command -v curl >/dev/null 2>&1 || { echo "status: STOP"; echo "reason: missing_dep_curl"; exit 2; }
ops/plugins/ha/bin/ha-ssot-propose:23:command -v python3 >/dev/null 2>&1 || { echo "status: STOP"; echo "reason: missing_dep_python3"; exit 2; }
ops/plugins/ha/bin/ha-ssot-propose:25:[[ -x "$INFISICAL_AGENT" ]] || { echo "status: STOP"; echo "reason: infisical_agent_not_found"; exit 2; }
ops/plugins/ha/bin/ha-ssot-propose:28:  echo "status: STOP"
ops/plugins/ha/bin/ha-ssot-propose:41:  echo "status: STOP"
ops/plugins/ha/bin/ha-ssot-propose:63:  echo "status: STOP"
ops/plugins/ha/bin/ha-ssot-propose:72:  echo "status: STOP"
ops/plugins/ha/bin/ha-ssot-propose:79:  echo "status: STOP"
ops/plugins/ha/bin/ha-ssot-propose:101:  echo "status: STOP"
ops/plugins/ha/bin/ha-ssot-propose:115:  echo "status: STOP"
ops/plugins/ha/bin/ha-ssot-propose:200:    print("status: STOP")
ops/plugins/backup/bin/backup-status:7:_need() { command -v "$1" >/dev/null 2>&1 || { echo "STOP: missing dependency: $1" >&2; exit 2; }; }
ops/plugins/backup/bin/backup-status:14:  echo "STOP: missing binding file: $BINDING_FILE" >&2
ops/plugins/backup/bin/backup-status:18:  echo "STOP: missing ssh targets binding: $SSH_TARGETS_FILE" >&2
ops/plugins/backup/bin/backup-status:44:  echo "STOP: no enabled file_glob targets in $BINDING_FILE"
ops/plugins/backup/bin/backup-status:103:      "$NAME" "$HOST" "${BASE_PATH:0:28}" "-" "-" "-" "ERROR" "unknown_host_binding"
ops/plugins/backup/bin/backup-status:115:      "$NAME" "$HOST" "${BASE_PATH:0:28}" "-" "-" "-" "ERROR" "host_unassigned"
ops/plugins/backup/bin/backup-status:145:      -o "LogLevel=ERROR"
ops/plugins/backup/bin/backup-status:163:      "$NAME" "$HOST" "${BASE_PATH:0:28}" "-" "-" "-" "ERROR" "$REASON"
ops/plugins/backup/bin/backup-status:187:      "$NAME" "$HOST" "${BASE_PATH:0:28}" "${NEWEST:0:24}" "-" "-" "ERROR" "parse_error"
ops/plugins/ha/bin/ha-z2m-health:23:command -v curl >/dev/null 2>&1 || { echo "status: STOP"; echo "reason: missing_dep_curl"; exit 2; }
ops/plugins/ha/bin/ha-z2m-health:24:command -v jq >/dev/null 2>&1  || { echo "status: STOP"; echo "reason: missing_dep_jq"; exit 2; }
ops/plugins/ha/bin/ha-z2m-health:25:command -v yq >/dev/null 2>&1  || { echo "status: STOP"; echo "reason: missing_dep_yq"; exit 2; }
ops/plugins/ha/bin/ha-z2m-health:28:  echo "status: STOP"
ops/plugins/ha/bin/ha-z2m-health:34:  echo "status: STOP"
ops/plugins/ha/bin/ha-z2m-health:41:  echo "status: STOP"
ops/plugins/ha/bin/ha-z2m-health:54:  echo "status: STOP"
ops/plugins/ha/bin/ha-z2m-health:114:  echo "status: STOP"
ops/plugins/ha/bin/ha-z2m-health:183:    echo "status: STOP"
ops/plugins/ha/bin/ha-z2m-health:211:WARNINGS=0
ops/plugins/ha/bin/ha-z2m-health:272:      STATUS="WARN"
ops/plugins/ha/bin/ha-z2m-health:274:      WARNINGS=$((WARNINGS + 1))
ops/plugins/ha/bin/ha-z2m-health:284:      STATUS="WARN"
ops/plugins/ha/bin/ha-z2m-health:286:      WARNINGS=$((WARNINGS + 1))
ops/plugins/ha/bin/ha-z2m-health:305:echo "  Summary: ${DEVICE_COUNT} devices, ${CRITICAL} critical, ${WARNINGS} warning"
ops/plugins/ha/bin/ha-device-rename:45:command -v python3 >/dev/null 2>&1 || { echo "status: STOP"; echo "reason: missing_dep_python3"; exit 2; }
ops/plugins/ha/bin/ha-device-rename:46:python3 -c "import websockets" 2>/dev/null || { echo "status: STOP"; echo "reason: missing_dep_websockets (pip3 install websockets)"; exit 2; }
ops/plugins/ha/bin/ha-device-rename:47:[[ -x "$INFISICAL_AGENT" ]] || { echo "status: STOP"; echo "reason: infisical_agent_not_found"; exit 2; }
ops/plugins/ha/bin/ha-device-rename:52:  echo "status: STOP"
ops/plugins/ha/bin/ha-entity-state-baseline:27:command -v python3 >/dev/null 2>&1 || { echo "STOP (2): python3 not found"; exit 2; }
ops/plugins/ha/bin/ha-entity-state-baseline:28:command -v curl >/dev/null 2>&1 || { echo "STOP (2): curl not found"; exit 2; }
ops/plugins/ha/bin/ha-entity-state-baseline:35:  echo "STOP (2): infisical-agent.sh not found"
ops/plugins/ha/bin/ha-entity-state-baseline:42:  echo "STOP (2): could not retrieve HA_API_TOKEN from Infisical"
ops/plugins/ha/bin/ha-entity-state-baseline:55:  echo "STOP (2): /api/states returned HTTP $HTTP_CODE"
ops/plugins/ha/bin/ha-entity-state-baseline:73:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/ha/bin/ha-dashboard-snapshot:15:SSH_OPTS="-o ConnectTimeout=10 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o BatchMode=yes -o LogLevel=ERROR"
ops/plugins/ha/bin/ha-dashboard-snapshot:27:command -v python3 >/dev/null 2>&1 || { echo "STOP (2): python3 not found"; exit 2; }
ops/plugins/ha/bin/ha-dashboard-snapshot:151:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/ha/bin/ha-entity-status:33:command -v curl >/dev/null 2>&1 || { echo "status: STOP"; echo "reason: missing_dep_curl"; exit 2; }
ops/plugins/ha/bin/ha-entity-status:34:command -v jq >/dev/null 2>&1  || { echo "status: STOP"; echo "reason: missing_dep_jq"; exit 2; }
ops/plugins/ha/bin/ha-entity-status:37:  echo "status: STOP"
ops/plugins/ha/bin/ha-entity-status:50:  echo "status: STOP"
ops/plugins/ha/bin/ha-device-map-build:27:command -v python3 >/dev/null 2>&1 || { echo "STOP (2): python3 not found"; exit 2; }
ops/plugins/ha/bin/ha-device-map-build:28:command -v curl >/dev/null 2>&1 || { echo "STOP (2): curl not found"; exit 2; }
ops/plugins/ha/bin/ha-device-map-build:31:  echo "STOP (2): home.device.registry.yaml not found"
ops/plugins/ha/bin/ha-device-map-build:36:  echo "STOP (2): z2m.devices.yaml not found"
ops/plugins/ha/bin/ha-device-map-build:45:  echo "STOP (2): infisical-agent.sh not found"
ops/plugins/ha/bin/ha-device-map-build:52:  echo "STOP (2): could not retrieve HA_API_TOKEN from Infisical"
ops/plugins/ha/bin/ha-device-map-build:66:  echo "STOP (2): HA API returned HTTP $HTTP_CODE"
ops/plugins/ha/bin/ha-device-map-build:80:  echo "STOP (2): template endpoint returned empty device list"
ops/plugins/ha/bin/ha-device-map-build:106:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/backup/bin/backup-vzdump-run:13:_need() { command -v "$1" >/dev/null 2>&1 || { echo "STOP: missing dependency: $1" >&2; exit 2; }; }
ops/plugins/backup/bin/backup-vzdump-run:18:  echo "STOP: missing ssh targets binding: $SSH_TARGETS_FILE" >&2
ops/plugins/backup/bin/backup-vzdump-run:43:    *) echo "STOP: unknown arg: $1" >&2; exit 2;;
ops/plugins/backup/bin/backup-vzdump-run:59:  echo "STOP: unknown ssh target id: $TARGET_ID" >&2
ops/plugins/backup/bin/backup-vzdump-run:68:  -o "LogLevel=ERROR"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-expose-enable:15:  echo "STOP (2): tailscale CLI required (install + connect first)"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-expose-enable:19:  echo "STOP (2): curl required"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-expose-enable:25:  echo "STOP: bridge is not healthy at http://${host}:${port}/health"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-expose-enable:38:    echo "STOP: tailscale serve already configured for this node; refusing to overwrite."
ops/plugins/mailroom-bridge/bin/mailroom-bridge-expose-enable:63:    echo "WARN: tailnet health check failed: ${scheme}://${dns_name}/health"
ops/plugins/ha/bin/ha-automations-snapshot:25:command -v python3 >/dev/null 2>&1 || { echo "STOP (2): python3 not found"; exit 2; }
ops/plugins/ha/bin/ha-automations-snapshot:26:command -v curl >/dev/null 2>&1 || { echo "STOP (2): curl not found"; exit 2; }
ops/plugins/ha/bin/ha-automations-snapshot:33:  echo "STOP (2): infisical-agent.sh not found"
ops/plugins/ha/bin/ha-automations-snapshot:40:  echo "STOP (2): could not retrieve HA_API_TOKEN from Infisical"
ops/plugins/ha/bin/ha-automations-snapshot:53:  echo "STOP (2): /api/states returned HTTP $HTTP_CODE"
ops/plugins/ha/bin/ha-automations-snapshot:70:    print("STOP (2): PyYAML not installed", file=sys.stderr)
ops/plugins/version/bin/version-compat-verify:16:ERRORS=0
ops/plugins/version/bin/version-compat-verify:32:      ERRORS=$((ERRORS + 1))
ops/plugins/version/bin/version-compat-verify:50:      ERRORS=$((ERRORS + 1))
ops/plugins/version/bin/version-compat-verify:56:echo "Summary: $COMPONENTS components, $ERRORS errors"
ops/plugins/version/bin/version-compat-verify:58:if [[ "$ERRORS" -gt 0 ]]; then
ops/plugins/backup/bin/backup-calendar-generate:14:#   2 STOP (missing bindings/deps)
ops/plugins/backup/bin/backup-calendar-generate:30:stop() { echo "STOP (2): $*" >&2; exit 2; }
ops/plugins/slo/bin/slo-evidence-daily:25:WARN_COUNT=0
ops/plugins/slo/bin/slo-evidence-daily:31:  WARN_COUNT=$(jq -r '.warn_count // 0' /tmp/slo-snap-$$ 2>/dev/null || echo "0")
ops/plugins/slo/bin/slo-evidence-daily:67:| Warnings | ${WARN_COUNT} | 0 | $([[ "$WARN_COUNT" -eq 0 ]] && echo "✅" || echo "⚠️") |
ops/plugins/slo/bin/slo-evidence-daily:97:echo "  Incidents: $INCIDENT_COUNT | Warnings: $WARN_COUNT"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-expose-disable:15:  echo "STOP (2): tailscale CLI required"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-expose-disable:26:  echo "STOP: tailscale serve is configured, but does not reference 127.0.0.1:${port}."
ops/plugins/mailroom-bridge/bin/mailroom-bridge-expose-disable:35:  echo "STOP: serve config contains other handlers besides 127.0.0.1:${port}; refusing to reset."
ops/plugins/mailroom-bridge/bin/mailroom-bridge-serve:32:    print(f"STOP: {msg}", file=sys.stderr)
ops/plugins/mailroom-bridge/bin/mailroom-bridge-serve:432:            return self._json(HTTPStatus.INTERNAL_SERVER_ERROR, {"error": "ops binary not found"})
ops/plugins/mailroom-bridge/bin/mailroom-bridge-serve:460:                HTTPStatus.INTERNAL_SERVER_ERROR,
ops/plugins/mailroom-bridge/bin/mailroom-bridge-serve:624:            return self._json(HTTPStatus.INTERNAL_SERVER_ERROR, {"error": "ops binary not found"})
ops/plugins/mailroom-bridge/bin/mailroom-bridge-serve:688:            HTTPStatus.OK if p.returncode == 0 else HTTPStatus.INTERNAL_SERVER_ERROR,
ops/plugins/mailroom-bridge/bin/mailroom-bridge-serve:739:            return self._json(HTTPStatus.INTERNAL_SERVER_ERROR, {"error": "enqueue script missing"})
ops/plugins/mailroom-bridge/bin/mailroom-bridge-serve:753:                HTTPStatus.INTERNAL_SERVER_ERROR,
ops/plugins/mailroom-bridge/bin/mailroom-bridge-stop:14:  echo "STOP (2): launchctl required (macOS launchd)"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-stop:26:  echo "STOP: plist not found at $PLIST"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-stop:30:echo "STOPPING: ${LABEL}"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-stop:35:echo "STOPPED"
ops/plugins/authority/bin/project-governance-bootstrap:44:stop() { echo "STOP: $*" >&2; exit 2; }
ops/plugins/authority/bin/project-governance-bootstrap:246:      echo "- WARN remote branch '${ORIGIN_REMOTE}/${CURRENT_BRANCH}' not found; push with:"
ops/plugins/authority/bin/project-governance-bootstrap:253:  echo "- WARN unable to detect current branch; skipping upstream alignment"
ops/plugins/mailroom-bridge/bin/mailroom-bridge-expose-status:64:  echo "STOP (2): tailscale CLI required for exposure status"
ops/plugins/orchestration/bin/orchestration-terminal-entry:184:    echo "WARN: forcing legacy worktree reuse for branch '$target_branch' at '$occupied_wt'" >&2
ops/plugins/loops/bin/gaps-status:15:command -v yq >/dev/null 2>&1 || { echo "ERROR: yq required"; exit 1; }
ops/plugins/loops/bin/gaps-status:33:  echo "WARNING: $null_status gap(s) have null/missing status — invisible to reconciliation"
ops/plugins/orchestration/bin/_orchestration-common:17:  echo "ERROR: $*" >&2
ops/plugins/loops/lib/gap-claims.sh:20:command -v yq >/dev/null 2>&1 || { echo "ERROR: yq required" >&2; exit 1; }
ops/plugins/loops/lib/gap-claims.sh:88:      echo "STOP: $gap_id already claimed by PID $existing_pid" >&2
ops/plugins/loops/lib/gap-claims.sh:92:    echo "WARN: Recovering stale claim on $gap_id (PID $existing_pid dead)" >&2
ops/plugins/loops/lib/gap-claims.sh:114:    echo "WARN: No claim exists for $gap_id" >&2
ops/plugins/loops/lib/gap-claims.sh:123:      echo "STOP: Cannot unclaim $gap_id — owned by PID $owner_pid (still running)" >&2
ops/plugins/loops/lib/gap-claims.sh:152:    echo "WARN: Stale claim on $gap_id (PID $owner_pid dead) — proceeding" >&2
ops/plugins/loops/lib/gap-claims.sh:157:  echo "STOP: $gap_id claimed by PID $owner_pid — cannot mutate without ownership" >&2
ops/plugins/loops/bin/gaps-file:124:    echo "WARN: auto-claim failed for $GAP_ID (may already be claimed)" >&2
ops/plugins/authority/bin/project-governance-status:26:stop() { echo "STOP: $*" >&2; exit 2; }
ops/plugins/authority/bin/project-governance-status:101:warn() { printf "WARN  %s\n" "$*"; warn_count=$((warn_count + 1)); }
ops/plugins/loops/bin/gaps-unclaim:41:  echo "WARN: positional GAP_ID is deprecated; use --id <GAP_ID>" >&2
ops/plugins/loops/bin/gaps-close:53:  echo "WARN: positional GAP_ID is deprecated; use --id <GAP_ID>" >&2
ops/plugins/loops/bin/gaps-claim:44:  echo "WARN: positional GAP_ID is deprecated; use --id <GAP_ID>" >&2
