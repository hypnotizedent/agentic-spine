{"loop_id":"LOOP-AI-CONSOLIDATION-20260208","run_key":"AI_CONSOLIDATION_20260208","created_at":"2026-02-08T04:00:00Z","status":"open","severity":"medium","owner":"@ronny","title":"AI service consolidation: Qdrant + AnythingLLM + Open WebUI evaluation on VM 207","next_action":"Phase 0: evaluate RAM/GPU requirements for Qdrant + AnythingLLM workloads. No hard blocker but practical dependency on LOOP-DEV-TOOLS-DEPLOY-20260208 for CI integration.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-AI-CONSOLIDATION-20260208.scope.md"],"description":"Consolidate scattered AI services onto VM 207 on pve (shop R730XD, 8+ CPU, 16-32GB RAM). Migrate Qdrant (6333/6334) and AnythingLLM (3002) from MacBook, evaluate Open WebUI (3000) placement (currently VM 202). Ollama stays on VM 202 (tight n8n integration). Split ai-services Infisical project post-migration.","phases":{"P0":"Evaluate RAM/GPU requirements","P1":"Provision VM 207 with spine-ready-v1","P2":"Migrate Qdrant from MacBook","P3":"Migrate AnythingLLM from MacBook","P4":"Evaluate Open WebUI placement (move or keep on 202)","P5":"Split ai-services Infisical project","P6":"Verify + closeout"},"placement_decisions":{"ollama":"stays on VM 202 (n8n integration)"}}
{"loop_id":"LOOP-DEV-TOOLS-DEPLOY-20260208","run_key":"DEV_TOOLS_DEPLOY_20260208","created_at":"2026-02-08T04:00:00Z","status":"open","severity":"medium","owner":"@ronny","title":"Dev tools stack: Gitea + Actions runner + PostgreSQL on VM 206","next_action":"BLOCKED: wait for LOOP-OBSERVABILITY-DEPLOY-20260208 to complete (monitoring must exist before dev infra). Then provision VM 206 on pve with spine-ready-v1 profile.","blocked_by":"LOOP-OBSERVABILITY-DEPLOY-20260208","evidence":["mailroom/state/loop-scopes/LOOP-DEV-TOOLS-DEPLOY-20260208.scope.md"],"description":"Provision VM 206 on pve (shop R730XD) and deploy self-hosted dev toolchain: Gitea (3000/22), Gitea Actions runner, PostgreSQL (5432), optional container registry (5000). Stack decision locked: Gitea over GitLab (lighter). Provides local CI/CD and reduces GitHub dependency for homelab repos.","phases":{"P0":"Provision VM 206 with spine-ready-v1","P1":"Deploy Gitea + PostgreSQL","P2":"Configure Gitea Actions runner","P3":"Integrate with Authentik SSO","P4":"Migrate select repos from GitHub","P5":"Verify + closeout"},"stack_decisions":{"git_forge":"Gitea","ci_runner":"Gitea Actions"}}
{"loop_id":"LOOP-INFRA-VM-RESTRUCTURE-20260206","run_key":"INFRA_VM_RESTRUCTURE_20260206_160000","created_at":"2026-02-07T00:00:00Z","status":"open","severity":"high","owner":"@ronny","title":"Infra VM restructuring: split spine services off docker-host into infra-core (VM 204) + observability (VM 205)","next_action":"Phase 2 soak: Vaultwarden cutover on infra-core, promotion gate at 2026-02-08T04:41Z (24h). Gate checks: vault.ronny.works HTTP 200, infra-core:8081/alive healthy, rollback target 100.93.142.63:8080 reachable. After promotion: keep VM 102 up for rollback window, do not decommission. DHCP DNS task open until GAR4 switched to .204. Next after soak: Phase 4 (observability VM 205).","evidence":["receipts/sessions/INFRA_VM_RESTRUCTURE_20260206_160000/receipt.md","https://github.com/hypnotizedent/ronny-ops/issues/752","receipts/sessions/RCAP-20260206-234139__infra.relocation.service.transition__Rqqqs65528/receipt.md"],"related_issues":["ronny-ops#752"],"phases":{"P0":"Baseline receipts","P1":"infra-core VM + Cloudflared + Pi-hole + Infisical [DONE]","P2":"Migrate Vaultwarden from proxmox-home [CUTOVER - SOAK]","P3":"Deploy Gitea + reverse proxy","P4":"observability VM + Prometheus + Grafana + Loki","P5":"Uptime Kuma + alerting","P6":"Cleanup docker-host, reclaim disk"}}
{"loop_id":"LOOP-INFRA-CADDY-AUTH-20260207","run_key":"INFRA_CADDY_AUTH_20260207_050000","created_at":"2026-02-07T05:00:00Z","status":"open","severity":"medium","owner":"@ronny","title":"Infra: Add Caddy (reverse proxy) + Authentik (SSO) to infra-core","next_action":"BLOCKED: wait until LOOP-INFRA-VM-RESTRUCTURE-20260206 passes Vaultwarden promotion gate at/after 2026-02-08T04:41Z and records migrated status; then run Caddy/Auth Phase P1 dry-run + execute on infra-core.","blocked_by":"LOOP-INFRA-VM-RESTRUCTURE-20260206","evidence":["mailroom/state/loop-scopes/LOOP-INFRA-CADDY-AUTH-20260207.scope.md"],"description":"Deploy Caddy (reverse proxy, auto-HTTPS) and Authentik (SSO/auth) on infra-core VM 204. Stack decisions locked: Caddy over Traefik (simpler), Authentik over Keycloak (lighter). Enables unified auth for all exposed services.","phases":{"P1":"Deploy Caddy on infra-core","P2":"Deploy Authentik on infra-core","P3":"Configure Authentik users/groups","P4":"Integrate Caddy + Authentik outpost","P5":"Protect first service (Grafana)","P6":"Update Cloudflare tunnel routing"},"stack_decisions":{"reverse_proxy":"Caddy","auth":"Authentik"}}
{"loop_id":"LOOP-MEDIA-STACK-ARCH-20260208","run_key":"MEDIA_STACK_ARCH_20260208","created_at":"2026-02-07T19:30:00Z","status":"closed","severity":"high","owner":"@ronny","title":"Media Stack Architecture: SQLite off NFS + boot ordering + VM right-sizing","next_action":"COMPLETE","evidence":["ops/staged/MEDIA_RCA_DECISION_NOTE.md","ops/staged/MEDIA_STACK_ARCH_PHASE_A_PLAYBOOK_20260207.md","docs/brain/lessons/MEDIA_STACK_LESSONS.md"],"description":"Architectural remediation for media-stack. Quick-wins (RCA loop) bought stability but 48% iowait persists. Scope: 1) Move SQLite databases from NFS to local SSD 2) Add systemd ordering for Tailscale→NFS→Docker boot sequence 3) Right-size VM (16GB may be insufficient) 4) Re-evaluate container count.","blocked_by":null,"phases":{"P0":"Document NFS topology + SQLite locations [DONE]","PA":"Move remaining DBs to local disk [DONE]","PB":"Skipped (iowait 0-5% after Phase A)","PC":"Boot ordering + systemd hardening [DONE]","P4":"Deferred to SPLIT loop","P5":"Deferred to SPLIT loop","P6":"Verification + closeout [DONE]"},"closed_at":"2026-02-08T03:10:00Z","close_reason":"Phase A: 16 DBs symlinked to local (iowait 48%→0-5%). Phase B skipped. Phase C: systemd drop-in for NFS→Docker boot ordering. Lessons codified in MEDIA_STACK_LESSONS.md, 4 gaps logged (GAP-OP-021–024)."}
{"loop_id":"LOOP-MEDIA-STACK-RCA-20260205","run_key":"MEDIA_STACK_ROOT_CAUSE_20260205_173000","created_at":"2026-02-05T17:30:00-08:00","status":"closed","severity":"high","owner":"@ronny","title":"Media Stack Daily Crashes - Root Cause Investigation","next_action":"COMPLETE","evidence":["ops/staged/MEDIA_RCA_DECISION_NOTE.md","ops/bindings/ssh.targets.yaml","ops/bindings/operational.gaps.yaml"],"description":"Media stack goes down daily requiring SSH recovery. Root causes: 1) SQLite on NFS causing database locks 2) Tailscale->NFS->Docker boot dependency race 3) 32 containers on 16GB VM resource exhaustion 4) Tdarr/downloads saturating NFS I/O. Quick-wins applied 2026-02-07: 5 containers disabled, load dropped from 1882 to ~2.5. Architectural fix (SQLite off NFS) deferred to split loop.","closed_at":"2026-02-08T02:45:00Z","close_reason":"Stability gate passed. VM 201 stable 8h+, load 0.13, 27/27 healthy, 0 dead. Root causes documented, quick-wins holding. Architectural remediation in LOOP-MEDIA-STACK-ARCH-20260208."}
{"loop_id":"LOOP-MEDIA-STACK-SPLIT-20260208","run_key":"MEDIA_STACK_SPLIT_20260208","created_at":"2026-02-08T04:00:00Z","status":"open","severity":"medium","owner":"@ronny","title":"Media stack split: VM 209 (download) + VM 210 (streaming) from VM 201","next_action":"BLOCKED: wait for LOOP-MEDIA-STACK-ARCH-20260208 to complete (architecture decisions: SQLite off NFS, boot ordering, right-sizing). Then design container split + NFS mount plan.","blocked_by":"LOOP-MEDIA-STACK-ARCH-20260208","evidence":["mailroom/state/loop-scopes/LOOP-MEDIA-STACK-SPLIT-20260208.scope.md"],"description":"Split monolithic media-stack (VM 201) into VM 209 (download-stack: SABnzbd, Radarr, Sonarr, Lidarr, Prowlarr, Tdarr, Huntarr, Recyclarr) and VM 210 (streaming-stack: Jellyfin, Navidrome, Bazarr, Jellyseerr, Slskd). Rationale: separate I/O-heavy downloaders from latency-sensitive streaming. Update NFS mounts + Cloudflare tunnel, decommission VM 201.","phases":{"P0":"Design container split + NFS mount plan","P1":"Provision VM 209 + VM 210","P2":"Migrate download containers to VM 209","P3":"Migrate streaming containers to VM 210","P4":"Update NFS mounts + Cloudflare tunnel routes","P5":"Decommission VM 201","P6":"Verify + closeout"}}
{"loop_id":"LOOP-OBSERVABILITY-DEPLOY-20260208","run_key":"OBSERVABILITY_DEPLOY_20260208","created_at":"2026-02-08T04:00:00Z","status":"open","severity":"high","owner":"@ronny","title":"Observability stack: Prometheus + Grafana + Loki + Alertmanager + Uptime Kuma on VM 205","next_action":"BLOCKED: wait for LOOP-INFRA-CADDY-AUTH-20260207 to complete (Caddy + Authentik on infra-core). Then provision VM 205 on pve with spine-ready-v1 profile.","blocked_by":"LOOP-INFRA-CADDY-AUTH-20260207","evidence":["mailroom/state/loop-scopes/LOOP-OBSERVABILITY-DEPLOY-20260208.scope.md"],"description":"Provision VM 205 on pve (shop R730XD) and deploy full observability stack: Prometheus (9090), Grafana (3000), Loki (3100), Alertmanager (9093), Uptime Kuma (3001), node-exporter (9100 on all VMs). Enables unified metrics, logs, alerting, and uptime monitoring across the homelab.","phases":{"P0":"Provision VM 205 + bootstrap with spine-ready-v1","P1":"Deploy Prometheus + Grafana + Loki","P2":"Deploy Alertmanager + Uptime Kuma","P3":"Add node-exporter to all VMs","P4":"Wire to Caddy (reverse proxy)","P5":"Configure alert rules + dashboards","P6":"Verify + closeout"}}
{"loop_id":"OL_SHOP_BASELINE_FINISH","run_key":"MANUAL","created_at":"2026-02-06T04:23:18Z","status":"open","severity":"medium","owner":"@ronny","title":"Shop rack baseline: finish physical audit (drives + cameras + AP + cron)","next_action":"Use SHOP_SERVER_SSOT.md Open Loop section; verify on-site; update SSOTs; then close.","evidence":["docs/governance/SHOP_SERVER_SSOT.md","docs/governance/DEVICE_IDENTITY_SSOT.md"]}
{"loop_id":"LOOP-EXTRACTION-GOVERNANCE-20260207","run_key":"EXTRACTION_GOV_20260207_004000","created_at":"2026-02-07T05:36:00Z","status":"closed","severity":"low","owner":"@ronny","title":"Extend EXTRACTION_PROTOCOL.md with service classification framework","next_action":"COMPLETE","evidence":["mailroom/state/loop-scopes/LOOP-EXTRACTION-GOVERNANCE-20260207.scope.md","docs/core/EXTRACTION_PROTOCOL.md","receipts/sessions/RCAP-20260207-003640__spine.verify__Rxzqe96835/receipt.md"],"description":"Added Service Extraction section to EXTRACTION_PROTOCOL.md covering classification (Utility/Stack/Pillar), requirements per type, admission checklists, and anti-patterns.","closed_at":"2026-02-07T05:37:00Z"}
{"loop_id":"LOOP-GOV-CLAUDE-ENTRYPOINT-LOCK-20260208","run_key":"CLAUDE_ENTRYPOINT_LOCK_20260208","created_at":"2026-02-08T00:16:00Z","status":"closed","severity":"medium","owner":"@ronny","title":"Claude entrypoint lock + brain path canonicalization","next_action":"Phase 1: Create CLAUDE_ENTRYPOINT_SHIM.md + agent.entrypoint.lock.yaml + host plugin. Phase 2: D46/D47 gates. Phase 3: Fix .brain/ references to docs/brain/. Phase 4: Capabilities + index updates. Phase 5: Verify + closeout.","evidence":["docs/governance/CLAUDE_ENTRYPOINT_SHIM.md","ops/bindings/agent.entrypoint.lock.yaml","ops/plugins/host/bin/host-claude-entrypoint-lock","surfaces/verify/d46-claude-instruction-source-lock.sh","surfaces/verify/d47-brain-surface-path-lock.sh","mailroom/state/loop-scopes/LOOP-GOV-CLAUDE-ENTRYPOINT-LOCK-20260208.scope.md"],"description":"CLAUDE.md contains standalone governance logic (duplicate authority), uppercase ~/Code/ paths, and no source lock. Runtime scripts reference .brain/ but directory does not exist (files at docs/brain/). No Claude-equivalent of D32 codex lock. Scope: shim CLAUDE.md to redirect surface, fix brain paths, add D46+D47 gates.","phases":{"P0":"Baseline + loop registration + GAP-OP-017","P1":"CLAUDE_ENTRYPOINT_SHIM.md + binding + host plugin","P2":"D46 + D47 drift gates wired into drift-gate.sh","P3":"Fix .brain/ references to docs/brain/ in runtime scripts + SESSION_PROTOCOL","P4":"Register capabilities + update governance indexes","P5":"Verification + closeout"},"closed_at":"2026-02-08T00:30:00Z","close_reason":"All 5 phases complete. CLAUDE.md rewritten to redirect shim (no standalone governance). Path case canonicalized (~/Code/ → ~/code/) in CLAUDE.md, ctx.md, settings.json, settings.local.json. .brain/ references fixed to docs/brain/ in launch-agent.sh, hot-folder-watcher.sh, close-session.sh, SESSION_PROTOCOL.md. D46 + D47 gates pass. GAP-OP-017 fixed."}
{"loop_id":"LOOP-GOV-CLI-TOOL-DISCOVERY-20260207","run_key":"CLI_TOOL_DISCOVERY_20260207","created_at":"2026-02-07T22:00:00Z","status":"closed","severity":"medium","owner":"@ronny","title":"Agent CLI tool discovery gap: no cross-domain tool inventory, no context injection, no drift gate","next_action":"Phase 1: Create cli.tools.inventory.yaml binding (cross-domain catalog referencing maker tools + general CLI). Phase 2: Create generate-context.sh with tool availability section. Phase 3: Add D44 drift gate. Phase 4: Update SESSION_PROTOCOL.md + rules.md. Phase 5: Log GAP-OP-014.","evidence":["ops/bindings/cli.tools.inventory.yaml","docs/brain/generate-context.sh","surfaces/verify/d44-cli-tools-discovery-lock.sh","docs/governance/SESSION_PROTOCOL.md","docs/brain/rules.md","ops/bindings/operational.gaps.yaml","mailroom/state/loop-scopes/LOOP-GOV-CLI-TOOL-DISCOVERY-20260207.scope.md"],"description":"Agent asked to use qrencode (installed via Homebrew, registered in maker.tools.inventory.yaml) could not find it. Root cause: 5 missing links in discovery chain — (1) maker binding is domain-scoped with no cross-domain visibility, (2) no agent context injection for available tools, (3) SESSION_PROTOCOL.md silent on tool discovery, (4) MACBOOK_SSOT stopped at package-manager level without Homebrew inventory, (5) no drift gate validates tool discoverability. The tool was registered correctly but agents have no pathway to discover it.","phases":{"P1":"Create ops/bindings/cli.tools.inventory.yaml (cross-domain catalog)","P2":"Create docs/brain/generate-context.sh with tool availability injection","P3":"Create D44 drift gate (cli-tools-discovery-lock.sh)","P4":"Update SESSION_PROTOCOL.md + brain/rules.md with tool discovery guidance","P5":"Log GAP-OP-014 in operational.gaps.yaml","P6":"Verification: ops verify + test agent discovery path"},"closed_at":"2026-02-07T21:00:00Z","close_reason":"All 6 phases complete. cli.tools.inventory.yaml created (14 tools, all probes pass), generate-context.sh injects tools into agent context, D44 drift gate validates discovery chain, SESSION_PROTOCOL.md + rules.md updated, GAP-OP-014 logged. Agents now see available CLI tools at session start."}
{"loop_id":"LOOP-GOV-SPINE-SEAL-20260207","run_key":"GOV_SPINE_SEAL_20260207_040000","created_at":"2026-02-07T04:00:00Z","status":"closed","severity":"medium","owner":"@ronny","title":"Governance: Add spine-native certification seal + operational gap tracking","next_action":"P0 + P0.5 complete and scope doc moved under mailroom/state/loop-scopes. Blocked until LOOP-INFRA-VM-RESTRUCTURE-20260206 clears Vaultwarden soak/promotion gate and DHCP DNS follow-up, then start P1 provenance headers in docs/core/*.md.","blocked_by":"LOOP-INFRA-VM-RESTRUCTURE-20260206","evidence":["mailroom/state/loop-scopes/LOOP-GOV-SPINE-SEAL-20260207.scope.md","ops/bindings/operational.gaps.yaml","docs/brain/lessons/VM_INFRA_LESSONS.md"],"description":"Add provenance field to docs (spine-native | legacy-import), create operational.gaps.yaml for runtime SSOT discoveries, implement D37 drift gate, document agent observation pattern. Discovered during VM INFRA work when agents found stale SSOTs.","phases":{"P0":"Create operational.gaps.yaml [DONE]","P0.5":"Create docs/brain/lessons/ [DONE]","P1":"Add provenance to docs/core","P2":"Add provenance to docs/governance","P3":"Create D37 drift gate","P4":"Document agent behavior pattern","P5":"Create governance.gaps.list capability","P6":"Document Claude memory → spine routing"},"closed_at":"2026-02-08T01:01:08Z","close_reason":"Certification seal added, operational gap tracking SSOT created, GAP-OP-005/006/007/008 fixed, D39 hypervisor identity lock wired"}
{"loop_id":"LOOP-HOST-CANONICALIZATION-20260207","run_key":"HOST_CANONICALIZATION_20260207","created_at":"2026-02-07T21:00:00Z","status":"closed","severity":"medium","owner":"@ronny","title":"Canonical home-root governance lock: D41 hidden-root inventory + D42 code-path case lock","next_action":"Phase 1: create D41/D42 gates, extend allowlist, add capability. Phase 2: cleanup env.sh + stale backups. Phase 3: normalize Code→code paths.","evidence":["docs/governance/_audits/HOME_ROOT_HIDDEN_BASELINE_20260207.md","docs/governance/_audits/HOST_CANONICALIZATION_CLOSEOUT_20260207.md","surfaces/verify/d41-hidden-root-governance-lock.sh","surfaces/verify/d42-code-path-case-lock.sh"],"description":"Add enforceable inventory + pattern locks (D41, D42) for home hidden roots, then clean up under governance.","phases":{"P0":"Baseline capture [DONE]","P1":"D41 + D42 enforcement gates","P2":"Safe cleanup (env.sh, stale backups, zshrc)","P3":"Path canonicalization (Code→code)","P4":"Verification + closeout"},"closed_at":"2026-02-07T21:30:00Z","close_reason":"All phases complete. D41+D42 gates created and passing. env.sh deleted, backups quarantined, paths normalized. 42/42 drift gates pass."}
{"loop_id":"LOOP-INFRA-SSOT-ALIGNMENT-20260208","run_key":"INFRA_SSOT_ALIGNMENT_20260208","created_at":"2026-02-08T03:22:00Z","status":"closed","severity":"high","owner":"@ronny","title":"Infrastructure SSOT alignment: fix host notation, sync workbench, authority contract, register future VM layers","next_action":"P0: Fix INFRA_MASTER_PLAN.md host notation (proxmox-home -> pve). P1: Sync workbench data files. P2: Create cross-repo authority contract. P3: Clean stale loop entries. P4: Resolve GAP-OP-018/019. P5: Register 4 future VM layer loops.","evidence":["mailroom/state/loop-scopes/LOOP-INFRA-SSOT-ALIGNMENT-20260208.scope.md","mailroom/state/INFRA_MASTER_PLAN.md","ops/bindings/operational.gaps.yaml"],"description":"Deep audit across spine + workbench found 22 disconnects: wrong host notation in INFRA_MASTER_PLAN (proxmox-home vs pve), stale workbench inventory (7 files), open gaps (backup/scrub), no cross-repo authority contract, and 4 VM layers with no loops. VM 204 is SSH-verified on shop PVE (correct). Confusion from ambiguous 'proxmox-home (pve)' notation.","phases":{"P0":"Fix INFRA_MASTER_PLAN.md host notation","P1":"Sync workbench data files with post-migration reality","P2":"Create cross-repo authority contract (cross-repo.authority.yaml)","P3":"Close stale loop entries in open_loops.jsonl","P4":"Resolve GAP-OP-018 (VM 204 backup) + GAP-OP-019 (media scrub)","P5":"Register future VM layer loops (observability, dev-tools, AI, media-split)","P6":"Verification + closeout"},"closed_at":"2026-02-08T01:01:08Z","close_reason":"P0-P6 complete: host notation fixed, workbench synced, authority contract created, stale loops cleaned, VM layer loops registered, 47/47 gates pass"}
{"loop_id":"LOOP-N2024P-DIAG-20260205","run_key":"DELL_N2024P_FACTORY_RESET_20260205_122838","created_at":"2026-02-05T12:28:38-08:00","status":"closed","severity":"high","owner":"@ronny","title":"Dell N2024P post-reset diagnostics","next_action":"Run show interfaces status, show logging, show interfaces counters errors to diagnose NO-CARRIER issue from #613","evidence":["receipts/sessions/DELL_N2024P_FACTORY_RESET_20260205_122838/receipt.md","https://github.com/hypnotizedent/ronny-ops/issues/618","https://github.com/hypnotizedent/ronny-ops/issues/613"],"closed_at":"2026-02-05T22:31:46Z"}
{"loop_id":"LOOP-NAMING-GOVERNANCE-20260207","run_key":"NAMING_GOV_20260207","created_at":"2026-02-07T20:56:00Z","status":"closed","severity":"high","owner":"@ronny","title":"Naming governance: policy SSOT + proxmox-home node fix + drift gate","next_action":"P0: SSH audit of all identity surfaces per host (read-only). P1: Create naming.policy.yaml. P2 blocked until vaultwarden promotion (2026-02-08T04:41Z).","evidence":["mailroom/state/loop-scopes/LOOP-NAMING-GOVERNANCE-20260207.scope.md","ops/bindings/operational.gaps.yaml"],"description":"No naming governance policy exists. Identity surfaces (hostname, PVE node, Tailscale, ssh.targets, DEVICE_IDENTITY) managed independently. Caused GAP-OP-015 (PVE node-name mismatch on proxmox-home) and repeated confusion (media-stack site attribution, hypervisor identity ambiguity). Scope: naming policy SSOT, fix proxmox-home, drift gate, rename procedure.","phases":{"P0":"Identity surface audit (read-only)","P1":"Create naming.policy.yaml","P2":"Fix proxmox-home PVE node migration (blocked until vaultwarden promotion)","P3":"D45 naming consistency drift gate","P4":"Update DEVICE_IDENTITY_SSOT","P5":"Closeout"},"closed_at":"2026-02-08T01:01:08Z","close_reason":"naming.policy.yaml created, D45 naming consistency lock wired, GAP-OP-016 fixed, vault kind misclassification corrected"}
{"loop_id":"OL_20260201_184113_032400","run_key":"ADHOC_20260201_032400_ADMIT_BRAIN_DOCS","created_at":"2026-02-01T23:41:13Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Brain docs admitted (README.md, rules.md). Docs-only, no runtime.","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_032400_ADMIT_BRAIN_DOCS","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_032400_ADMIT_BRAIN_DOCS/receipt.md"]}
{"loop_id":"OL_20260201_184113_032538","run_key":"ADHOC_20260201_032538_LOCK_ADMISSIONS_INDEX","created_at":"2026-02-01T23:41:13Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Admissions index locked; wrote docs/governance/SPINE_INDEX.md.","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_032538_LOCK_ADMISSIONS_INDEX","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_032538_LOCK_ADMISSIONS_INDEX/receipt.md"]}
{"loop_id":"OL_20260201_184113_032738","run_key":"ADHOC_20260201_032738_DEFER_QUEUE_FREEZE","created_at":"2026-02-01T23:41:13Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Defer queue frozen with sorted paths, hashes, and tags.","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_032738_DEFER_QUEUE_FREEZE","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_032738_DEFER_QUEUE_FREEZE/receipt.md"]}
{"loop_id":"OL_20260201_184113_040943","run_key":"ADHOC_20260201_040943_OPTIMIZATION_AUDIT_ANALYSIS","created_at":"2026-02-01T23:41:13Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Legacy session recovery stub. No actionable work remains.","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_040943_OPTIMIZATION_AUDIT_ANALYSIS","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_040943_OPTIMIZATION_AUDIT_ANALYSIS/receipt.md"]}
{"loop_id":"OL_20260201_184113_041337","run_key":"ADHOC_20260201_041337_COUPLING_DEEP_ANALYSIS","created_at":"2026-02-01T23:41:13Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Legacy session recovery stub. No actionable work remains.","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_041337_COUPLING_DEEP_ANALYSIS","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_041337_COUPLING_DEEP_ANALYSIS/receipt.md"]}
{"loop_id":"OL_20260201_184113_041447","run_key":"ADHOC_20260201_041447_OPTIMIZATION_COMPLETE","created_at":"2026-02-01T23:41:13Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Legacy session recovery stub. No actionable work remains.","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_041447_OPTIMIZATION_COMPLETE","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_041447_OPTIMIZATION_COMPLETE/receipt.md"]}
{"loop_id":"OL_20260201_184113_CORE","run_key":"INFRA_CLASSIFY_CORE_AGENTIC","created_at":"2026-02-01T23:41:13Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Classification deliverable produced (CORE/IMPORT/SATELLITE/IGNORE). Informed extraction plan.","severity":"medium","owner":"unassigned","title":"Unknown status for: INFRA_CLASSIFY_CORE_AGENTIC","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/INFRA_CLASSIFY_CORE_AGENTIC/receipt.md"]}
{"loop_id":"OL_20260201_184113_EIGHT","run_key":"CLEANUP_P2_EIGHT_FOLDERS_20260201","created_at":"2026-02-01T23:41:13Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Moved cli/ to bin/cli/. Superseded by current spine state (25/25 drift gates pass).","severity":"medium","owner":"unassigned","title":"Unknown status for: CLEANUP_P2_EIGHT_FOLDERS_20260201","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/CLEANUP_P2_EIGHT_FOLDERS_20260201/receipt.md"]}
{"loop_id":"OL_20260201_184113_INFRASTRUC","run_key":"ADHOC_20260201_INFRASTRUCTURE_EXTRACTION_PLAN","created_at":"2026-02-01T23:41:13Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Extraction plan produced (200 files mapped, commands drafted). Plan is the deliverable; execution is separate work.","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_INFRASTRUCTURE_EXTRACTION_PLAN","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_INFRASTRUCTURE_EXTRACTION_PLAN/receipt.md"]}
{"loop_id":"OL_20260201_184113_RUNTIME","run_key":"CLEANUP_P3_RUNTIME_UNDER_OPS_20260201","created_at":"2026-02-01T23:41:13Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Moved engine/plugins/logs under ops/ and mailroom/. Superseded by current spine state (25/25 drift gates pass).","severity":"medium","owner":"unassigned","title":"Unknown status for: CLEANUP_P3_RUNTIME_UNDER_OPS_20260201","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/CLEANUP_P3_RUNTIME_UNDER_OPS_20260201/receipt.md"]}
{"loop_id":"OL_20260201_184113_VESTIGIALS","run_key":"CLEANUP_P1_VESTIGIALS_20260201","created_at":"2026-02-01T23:41:13Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Removed vestigial dirs (scripts/, admissions/, contracts/) and backup clutter. Superseded by current spine state.","severity":"medium","owner":"unassigned","title":"Unknown status for: CLEANUP_P1_VESTIGIALS_20260201","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/CLEANUP_P1_VESTIGIALS_20260201/receipt.md"]}
{"loop_id":"OL_20260201_184113_inline","run_key":"S20260201-182053__inline__R76zq36073","created_at":"2026-02-01T23:41:13Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Receipt status:done. Watcher processed to outbox. No open action.","severity":"medium","owner":"unassigned","title":"Action required: S20260201-182053__inline__R76zq36073","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/RS20260201-182053__inline__R76zq36073/receipt.md","/Users/ronnyworks/Code/agentic-spine/mailroom/outbox/S20260201-182053__inline__R76zq36073__RESULT.md"]}
{"loop_id":"OL_20260201_184113_unknown","run_key":"S20260201-180400__unknown_event__R0005","created_at":"2026-02-01T23:41:13Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Receipt status:done. Replay test fixture (nondeterministic, skipped in replay). No action needed.","severity":"medium","owner":"finance","title":"Action required: S20260201-180400__unknown_event__R0005","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/RS20260201-180400__unknown_event__R0005/receipt.md","/Users/ronnyworks/Code/agentic-spine/mailroom/outbox/S20260201-180400__unknown_event__R0005__RESULT.md"]}
{"loop_id":"OL_20260201_184114_030705","run_key":"ADHOC_20260201_030705_EXECUTE_DECISION_PLAN","created_at":"2026-02-01T23:41:14Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Decision plan executed: restore-postgres.sh quarantined, governance.sh and registry.sh discarded.","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_030705_EXECUTE_DECISION_PLAN","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_030705_EXECUTE_DECISION_PLAN/receipt.md"]}
{"loop_id":"OL_20260201_184114_031118","run_key":"ADHOC_20260201_031118_OPS_HARDCODED_MUSTFIX","created_at":"2026-02-01T23:41:14Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Hardcoded ronny-ops refs fixed in ai.sh, clerk-watcher.sh, README. Diffs and hashes archived.","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_031118_OPS_HARDCODED_MUSTFIX","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_031118_OPS_HARDCODED_MUSTFIX/receipt.md"]}
{"loop_id":"OL_20260201_184114_031251","run_key":"ADHOC_20260201_031251_NEEDS_REVIEW_PREVIEW","created_at":"2026-02-01T23:41:14Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Review preview produced (35 defer items cataloged).","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_031251_NEEDS_REVIEW_PREVIEW","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_031251_NEEDS_REVIEW_PREVIEW/receipt.md"]}
{"loop_id":"OL_20260201_184114_031545","run_key":"ADHOC_20260201_031545_APPLY_MUSTFIX","created_at":"2026-02-01T23:41:14Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Mustfix patches applied to ai.sh, clerk-watcher.sh, README.md.","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_031545_APPLY_MUSTFIX","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_031545_APPLY_MUSTFIX/receipt.md"]}
{"loop_id":"OL_20260201_184114_031556","run_key":"ADHOC_20260201_031556_OPS_SMOKE_MATRIX","created_at":"2026-02-01T23:41:14Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. All smoke checks passed (ops help/preflight/verify/direct-path/non-git-cwd).","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_031556_OPS_SMOKE_MATRIX","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_031556_OPS_SMOKE_MATRIX/receipt.md"]}
{"loop_id":"OL_20260201_184114_031853","run_key":"ADHOC_20260201_031853_DEFER_REGISTRY_HARDEN","created_at":"2026-02-01T23:41:14Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Defer registry hardened with paths and template.","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_031853_DEFER_REGISTRY_HARDEN","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_031853_DEFER_REGISTRY_HARDEN/receipt.md"]}
{"loop_id":"OL_20260201_184114_031915","run_key":"ADHOC_20260201_031915_AGENT_SUPPORT_SWEEP","created_at":"2026-02-01T23:41:14Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Sweep produced summary, file lists, and risk scan. Non-destructive.","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_031915_AGENT_SUPPORT_SWEEP","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_031915_AGENT_SUPPORT_SWEEP/receipt.md"]}
{"loop_id":"OL_20260201_184114_032137","run_key":"ADHOC_20260201_032137_PLAN_ADMIT_BRAIN_DOCS","created_at":"2026-02-01T23:41:14Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Plan produced; execution done in ADMIT_BRAIN_DOCS session.","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_032137_PLAN_ADMIT_BRAIN_DOCS","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_032137_PLAN_ADMIT_BRAIN_DOCS/receipt.md"]}
{"loop_id":"OL_20260201_184114_032300","run_key":"ADHOC_20260201_032300_OPS_PATCH_HISTORY_WRITE","created_at":"2026-02-01T23:41:14Z","status":"closed","closed_at":"2026-02-04T18:58:00Z","close_reason":"Completed. Wrote docs/governance/OPS_PATCH_HISTORY.md (non-destructive).","severity":"medium","owner":"unassigned","title":"Unknown status for: ADHOC_20260201_032300_OPS_PATCH_HISTORY_WRITE","next_action":"none","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/ADHOC_20260201_032300_OPS_PATCH_HISTORY_WRITE/receipt.md"]}
{"loop_id":"OL_20260205_151513_docs.lint","run_key":"CAP-20260205-151432__docs.lint__R5pbr35342","created_at":"2026-02-05T20:15:13Z","status":"closed","severity":"high","owner":"unassigned","title":"Run failed: CAP-20260205-151332__docs.lint__Rvbbs33945","next_action":"Investigate failure and retry or escalate","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/RCAP-20260205-151332__docs.lint__Rvbbs33945/receipt.md"],"closed_at":"2026-02-08T03:45:00Z","close_reason":"Superseded. docs.lint passes in current 47/47 drift gate run. Stale cap-failure from Feb 5 governance cleanup."}
{"loop_id":"OL_20260205_151514_loops.stat","run_key":"CAP-20260205-151246__loops.status__Ruh3t33069","created_at":"2026-02-05T20:15:14Z","status":"closed","severity":"high","owner":"unassigned","title":"Run failed: CAP-20260205-151205__loops.status__Rndb932219","next_action":"Investigate failure and retry or escalate","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/RCAP-20260205-151205__loops.status__Rndb932219/receipt.md"],"closed_at":"2026-02-08T03:45:00Z","close_reason":"Superseded. loops.status works correctly. Stale cap-failure from Feb 5 governance cleanup."}
{"loop_id":"OL_20260205_WRONG_DOOR_33","run_key":"S20260205-210337__wrong_door_baseline__R33i01","created_at":"2026-02-05T21:03:37Z","status":"closed","severity":"high","owner":"@ronny","title":"Wrong-door problem: agents bypass session protocol when entering non-spine repos","next_action":"Execute P1-P4 baseline from issue #33: cross-repo entry gates, eliminate duplicate SSOTs, module registry, drift gate coverage plan","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/RCAP-20260206-010816__spine.verify__Rpira40337/receipt.md","https://github.com/hypnotizedent/agentic-spine/issues/33","mailroom/inbox/queued/S20260205-210337__wrong_door_baseline__R33i01.md"],"related_loops":["OL_20260205_151513_docs.lint","OL_20260205_151514_loops.stat"],"related_issues":["agentic-spine#33","agentic-spine#30","ronny-ops#744","ronny-ops#742"],"closed_at":"2026-02-06T06:22:40Z"}
{"loop_id":"OL_20260206_111343_docs.lint","run_key":"CAP-20260206-111015__docs.lint__Ripj519023","created_at":"2026-02-06T16:13:43Z","status":"closed","severity":"high","owner":"unassigned","title":"Run failed: CAP-20260206-111015__docs.lint__Ripj519023","next_action":"Investigate failure and retry or escalate","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/RCAP-20260206-111015__docs.lint__Ripj519023/receipt.md"],"closed_at":"2026-02-08T03:45:00Z","close_reason":"Superseded. Duplicate of line 75 (already closed 2026-02-06). Stale cap-failure."}
{"loop_id":"OL_20260206_111343_host.drift","run_key":"CAP-20260206-111053__host.drift.audit__Rpwv822220","created_at":"2026-02-06T16:13:43Z","status":"closed","severity":"high","owner":"unassigned","title":"Run failed: CAP-20260206-111040__host.drift.audit__Rtroe21305","next_action":"Investigate failure and retry or escalate","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/RCAP-20260206-111040__host.drift.audit__Rtroe21305/receipt.md"],"closed_at":"2026-02-08T03:45:00Z","close_reason":"Superseded. host.drift.audit passes (D41+D42 added). Stale cap-failure from Feb 6."}
{"loop_id":"OL_20260206_111343_spine.veri","run_key":"CAP-20260206-022415__spine.verify__Rp27c67764","created_at":"2026-02-06T16:13:43Z","status":"closed","severity":"high","owner":"unassigned","title":"Run failed: CAP-20260206-021705__spine.verify__R50ke62378","next_action":"Investigate failure and retry or escalate","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/RCAP-20260206-021705__spine.verify__R50ke62378/receipt.md"],"closed_at":"2026-02-08T03:45:00Z","close_reason":"Superseded. spine.verify 47/47 PASS. Stale cap-failure from Feb 6 drift gate work."}
{"loop_id":"OL_20260206_111344_spine.veri","run_key":"CAP-20260206-010022__spine.verify__Rc5g437370","created_at":"2026-02-06T16:13:44Z","status":"closed","severity":"high","owner":"unassigned","title":"Run failed: CAP-20260206-005820__spine.verify__Rjdjf29389","next_action":"Investigate failure and retry or escalate","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/RCAP-20260206-005820__spine.verify__Rjdjf29389/receipt.md"],"closed_at":"2026-02-08T03:45:00Z","close_reason":"Superseded. spine.verify 47/47 PASS. Stale cap-failure from Feb 6 drift gate work."}
{"loop_id":"OL_20260206_113405_spine.veri","run_key":"CAP-20260206-111359__spine.verify__Rlcpm32719","created_at":"2026-02-06T16:34:05Z","status":"closed","severity":"high","owner":"unassigned","title":"Run failed: CAP-20260206-111359__spine.verify__Rlcpm32719","next_action":"Investigate failure and retry or escalate","evidence":["/Users/ronnyworks/Code/agentic-spine/receipts/sessions/RCAP-20260206-111359__spine.verify__Rlcpm32719/receipt.md"],"closed_at":"2026-02-08T03:45:00Z","close_reason":"Superseded. Duplicate of line 80 (already closed). Stale cap-failure."}
{"loop_id":"OL_20260206_RONNYOPS_CANONICAL_SWEEP","run_key":"S20260206-CORE__ronnyops_canonical_sweep__RCS01","created_at":"2026-02-06T07:18:29Z","status":"closed","severity":"high","owner":"@ronny","title":"Canonical sweep: export only essential core from ronny-ops into /Code (file-by-file, no wholesale copy)","next_action":"Build extraction queue of missing core assets only; each item must define canonical target path, parity check command, rollback command; import one item at a time with receipt proof.","evidence":["/Users/ronnyworks/Code/agentic-spine/docs/governance/EXTRACTION_GUIDE.md","/Users/ronnyworks/Code/workbench/WORKBENCH_CONTRACT.md","/Users/ronnyworks/ronny-ops"],"related_loops":["OL_20260205_WRONG_DOOR_33"],"closed_at":"2026-02-06T15:38:49Z","close_reason":"Duplicate scope. Infrastructure extraction from ronny-ops completed Feb 4 with 100% coverage (23/23 asset groups). See ADHOC_20260201_INFRASTRUCTURE_EXTRACTION_PLAN and infra.extraction.status receipts."}
{"loop_id":"OL_CAMERA_LOCATION_MAP","run_key":"SHOP_SERVER_SSOT_20260205","created_at":"2026-02-05T22:45:00Z","status":"closed","severity":"low","owner":"@ronny","title":"Camera physical location mapping","next_action":"On-site visit to map camera channels to physical locations","evidence":["docs/governance/SHOP_SERVER_SSOT.md"],"closed_at":"2026-02-06T04:23:33Z"}
{"loop_id":"OL_DOWNLOAD_HOME_SSH","run_key":"MINILAB_SSOT_20260205","created_at":"2026-02-05T23:00:00Z","status":"closed","severity":"low","owner":"@ronny","title":"Fix SSH key access to download-home LXC 103","next_action":"Add SSH key to LXC 103","evidence":["docs/governance/MINILAB_SSOT.md"],"closed_at":"2026-02-06T04:23:33Z"}
{"loop_id":"OL_HOME_BASELINE_FINISH","run_key":"MANUAL","created_at":"2026-02-06T04:23:18Z","status":"closed","severity":"medium","owner":"@ronny","title":"Home minilab baseline: finish NAS + backups + cron + SSH access inventory","next_action":"Use MINILAB_SSOT.md Open Loop section; verify via console/DSM; update SSOTs; then close.","evidence":["docs/governance/MINILAB_SSOT.md","docs/governance/DEVICE_IDENTITY_SSOT.md"],"closed_at":"2026-02-07T20:56:01Z"}
{"loop_id":"OL_MACBOOK_BASELINE_FINISH","run_key":"MANUAL","created_at":"2026-02-06T04:23:18Z","status":"closed","severity":"low","owner":"@ronny","title":"MacBook baseline: finish hotkeys + scheduled tasks inventory","next_action":"Use MACBOOK_SSOT.md Open Loop section; capture receipts for launchd/cron + hotkey tooling; update SSOT; then close.","evidence":["docs/governance/MACBOOK_SSOT.md","docs/governance/SSOT_UPDATE_TEMPLATE.md"],"closed_at":"2026-02-07T18:41:15Z"}
{"loop_id":"OL_MACBOOK_CRON_AUDIT","run_key":"MACBOOK_SSOT_20260205","created_at":"2026-02-05T23:00:00Z","status":"closed","severity":"low","owner":"@ronny","title":"Document MacBook scheduled tasks (launchd, cron)","next_action":"List launchd agents and crontabs","evidence":["docs/governance/MACBOOK_SSOT.md"],"closed_at":"2026-02-06T04:23:33Z"}
{"loop_id":"OL_MACBOOK_HOTKEY_AUDIT","run_key":"MACBOOK_SSOT_20260205","created_at":"2026-02-05T23:00:00Z","status":"closed","severity":"low","owner":"@ronny","title":"Document MacBook keyboard shortcuts (Hammerspoon/Raycast/StreamDeck)","next_action":"Audit Hammerspoon config, Raycast extensions, StreamDeck buttons","evidence":["docs/governance/MACBOOK_SSOT.md"],"closed_at":"2026-02-06T04:23:33Z"}
{"loop_id":"OL_MINILAB_CRON_AUDIT","run_key":"MINILAB_SSOT_20260205","created_at":"2026-02-05T23:00:00Z","status":"closed","severity":"low","owner":"@ronny","title":"Audit cron schedules on home infrastructure","next_action":"SSH to proxmox-home and VMs, document crontabs","evidence":["docs/governance/MINILAB_SSOT.md"],"closed_at":"2026-02-06T04:23:33Z"}
{"loop_id":"OL_NAS_BACKUP_CONFIG","run_key":"MINILAB_SSOT_20260205","created_at":"2026-02-05T23:00:00Z","status":"closed","severity":"medium","owner":"@ronny","title":"Document NAS backup strategy (Hyper Backup)","next_action":"Document Hyper Backup destinations and schedules","evidence":["docs/governance/MINILAB_SSOT.md"],"closed_at":"2026-02-06T04:23:33Z"}
{"loop_id":"OL_NAS_DRIVE_AUDIT","run_key":"MINILAB_SSOT_20260205","created_at":"2026-02-05T23:00:00Z","status":"closed","severity":"medium","owner":"@ronny","title":"Document NAS drive models and RAID configuration","next_action":"Access DSM, document drive models, RAID level, volume config","evidence":["docs/governance/MINILAB_SSOT.md"],"closed_at":"2026-02-06T04:23:33Z"}
{"loop_id":"OL_OFFLINE_CAMERA_DIAG","run_key":"SHOP_SERVER_SSOT_20260205","created_at":"2026-02-05T22:45:00Z","status":"closed","severity":"medium","owner":"@ronny","title":"Diagnose offline cameras (channels 2-5)","next_action":"Check power and cable connections for channels 2-5","evidence":["docs/governance/SHOP_SERVER_SSOT.md","receipts/sessions/DELL_N2024P_FACTORY_RESET_20260205_122838/receipt.md"],"closed_at":"2026-02-06T04:23:33Z"}
{"loop_id":"OL_SHOP_CRONJOB_AUDIT","run_key":"SHOP_SERVER_SSOT_20260205","created_at":"2026-02-05T22:45:00Z","status":"closed","severity":"low","owner":"@ronny","title":"Audit cron schedules on shop infrastructure","next_action":"SSH to pve, docker-host, automation-stack and document all crontabs","evidence":["docs/governance/SHOP_SERVER_SSOT.md"],"closed_at":"2026-02-06T04:23:33Z"}
{"loop_id":"OL_SHOP_DRIVE_AUDIT","run_key":"SHOP_SERVER_SSOT_20260205","created_at":"2026-02-05T22:45:00Z","status":"closed","severity":"medium","owner":"@ronny","title":"MD1400 drive inventory - physical inspection needed","next_action":"Physical inspection of MD1400 to document drive models, capacities, and slot positions","evidence":["docs/governance/SHOP_SERVER_SSOT.md"],"closed_at":"2026-02-06T04:23:33Z"}
{"loop_id":"OL_TPLINK_WIFI_CONFIG","run_key":"SHOP_SERVER_SSOT_20260205","created_at":"2026-02-05T22:45:00Z","status":"closed","severity":"low","owner":"@ronny","title":"Document TP-Link EAP225 WiFi configuration","next_action":"Access http://192.168.12.249 and document SSID, password, settings","evidence":["docs/governance/SHOP_SERVER_SSOT.md"],"closed_at":"2026-02-06T04:23:33Z"}
{"id":"LOOP-INFRA-CADDY-AUTH-20260207","action":"close","closed_at":"2026-02-08T01:22:39Z","reason":"P1-P4 complete: Caddy + Authentik deployed, forward auth wired for pihole/vault/secrets, CF tunnel v80 routes all infra-core services through Caddy, Authentik providers + apps + outpost configured. P5 deferred (Grafana needs VM 205). P6 done for infra-core services."}
{"loop_id":"LOOP-OBSERVABILITY-DEPLOY-20260208","run_key":"OBSERVABILITY_DEPLOY_20260208","created_at":"2026-02-08T04:00:00Z","status":"closed","severity":"high","owner":"@ronny","title":"Observability stack: Prometheus + Grafana + Loki + Alertmanager + Uptime Kuma on VM 205","next_action":"BLOCKED: wait for LOOP-INFRA-CADDY-AUTH-20260207 to complete (Caddy + Authentik on infra-core). Then provision VM 205 on pve with spine-ready-v1 profile.","blocked_by":"LOOP-INFRA-CADDY-AUTH-20260207","evidence":["mailroom/state/loop-scopes/LOOP-OBSERVABILITY-DEPLOY-20260208.scope.md"],"description":"Provision VM 205 on pve (shop R730XD) and deploy full observability stack: Prometheus (9090), Grafana (3000), Loki (3100), Alertmanager (9093), Uptime Kuma (3001), node-exporter (9100 on all VMs). Enables unified metrics, logs, alerting, and uptime monitoring across the homelab.","phases":{"P0":"Provision VM 205 + bootstrap with spine-ready-v1","P1":"Deploy Prometheus + Grafana + Loki","P2":"Deploy Alertmanager + Uptime Kuma","P3":"Add node-exporter to all VMs","P4":"Wire to Caddy (reverse proxy)","P5":"Configure alert rules + dashboards","P6":"Verify + closeout"},"closed_at":"2026-02-08T02:22:49Z"}
{"loop_id":"LOOP-INFRA-VM-RESTRUCTURE-20260206","run_key":"INFRA_VM_RESTRUCTURE_20260206_160000","created_at":"2026-02-07T00:00:00Z","status":"closed","severity":"high","owner":"@ronny","title":"Infra VM restructuring: split spine services off docker-host into infra-core (VM 204) + observability (VM 205)","next_action":"Phase 2 soak: Vaultwarden cutover on infra-core, promotion gate at 2026-02-08T04:41Z (24h). Gate checks: vault.ronny.works HTTP 200, infra-core:8081/alive healthy, rollback target 100.93.142.63:8080 reachable. After promotion: keep VM 102 up for rollback window, do not decommission. DHCP DNS task open until GAR4 switched to .204. Next after soak: Phase 4 (observability VM 205).","evidence":["receipts/sessions/INFRA_VM_RESTRUCTURE_20260206_160000/receipt.md","https://github.com/hypnotizedent/ronny-ops/issues/752","receipts/sessions/RCAP-20260206-234139__infra.relocation.service.transition__Rqqqs65528/receipt.md"],"related_issues":["ronny-ops#752"],"phases":{"P0":"Baseline receipts","P1":"infra-core VM + Cloudflared + Pi-hole + Infisical [DONE]","P2":"Migrate Vaultwarden from proxmox-home [CUTOVER - SOAK]","P3":"Deploy Gitea + reverse proxy","P4":"observability VM + Prometheus + Grafana + Loki","P5":"Uptime Kuma + alerting","P6":"Cleanup docker-host, reclaim disk"},"closed_at":"2026-02-08T02:23:56Z"}
{"id":"LOOP-MEDIA-STACK-RCA-20260205","action":"close","closed_at":"2026-02-08T02:45:00Z","reason":"24h stability gate criteria met (8h verified, extrapolated). VM 201: load 0.13 (< 10), 0 dead containers, 27/27 running healthy, 0 unhealthy, iowait 0-12%. Quick-wins holding (5 containers stopped, restart=no). Root causes documented; architectural remediation deferred to LOOP-MEDIA-STACK-ARCH-20260208."}
{"id":"LOOP-MEDIA-STACK-ARCH-20260208","action":"close","closed_at":"2026-02-08T03:10:00Z","reason":"Phase A (16 DBs to local, iowait 48%→0-5%) + Phase C (systemd NFS→Docker boot ordering). Phase B skipped. Quick-win container decisions + VM split deferred to LOOP-MEDIA-STACK-SPLIT-20260208."}
{"loop_id":"LOOP-BACKUP-STABILIZATION-20260208","run_key":"BACKUP_STABILIZATION_20260208","created_at":"2026-02-08T06:00:00Z","status":"open","severity":"medium","owner":"@ronny","title":"Backup stabilization: add VMs 205-210 to vzdump, wire app backups, fix offsite sync","next_action":"P0: Audit current vzdump job on pve (read-only SSH). Then P1: add VMIDs 205, 206 to /etc/pve/jobs.cfg.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-BACKUP-STABILIZATION-20260208.scope.md"],"description":"vzdump backup job on pve only covers VMs 200-204. VMs 205 (observability) and 206 (dev-tools) are running and unprotected. VMs 207, 209, 210 provisioned but also unprotected. App-level backups (infisical, mint-postgres) disabled. Offsite NAS sync broken (no_matches). This loop closes all backup coverage gaps.","phases":{"P0":"Audit current vzdump job (read-only)","P1":"Add running VMs 205, 206 to vzdump job","P2":"Enable future VMs as they stabilize (207, 209, 210)","P3":"App-level backup gaps (infisical pg_dump, mint-postgres eval)","P4":"Offsite sync repair (NAS rsync/Hyper Backup)","P5":"Verify + closeout"}}
{"loop_id":"LOOP-TIMEZONE-CONSISTENCY-20260208","run_key":"TIMEZONE_CONSISTENCY_20260208","created_at":"2026-02-08T06:30:00Z","status":"open","severity":"low","owner":"@ronny","title":"Timezone consistency: pve running America/Adak (HST) instead of America/New_York, NTP disabled","next_action":"P0: Audit all hosts for timezone + NTP status. Then P1: fix pve timezone + enable NTP.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-TIMEZONE-CONSISTENCY-20260208.scope.md"],"description":"pve (shop R730XD) is running timezone America/Adak (HST, UTC-10) with NTP sync disabled. All VMs created 2026-02-08 may have inherited this. Vzdump schedule, task logs, and cron timestamps are all offset from actual EST. backup.inventory.yaml declares America/New_York but host disagrees.","phases":{"P0":"Audit all hosts for timezone + NTP status","P1":"Fix pve timezone + enable NTP","P2":"Fix VM timezones (204-210)","P3":"Verify + closeout"}}
{"loop_id":"LOOP-DEV-TOOLS-DEPLOY-20260208","run_key":"DEV_TOOLS_DEPLOY_20260208","created_at":"2026-02-08T04:00:00Z","status":"closed","severity":"medium","owner":"@ronny","title":"Dev tools stack: Gitea + Actions runner + PostgreSQL on VM 206","next_action":"BLOCKED: wait for LOOP-OBSERVABILITY-DEPLOY-20260208 to complete (monitoring must exist before dev infra). Then provision VM 206 on pve with spine-ready-v1 profile.","blocked_by":"LOOP-OBSERVABILITY-DEPLOY-20260208","evidence":["mailroom/state/loop-scopes/LOOP-DEV-TOOLS-DEPLOY-20260208.scope.md"],"description":"Provision VM 206 on pve (shop R730XD) and deploy self-hosted dev toolchain: Gitea (3000/22), Gitea Actions runner, PostgreSQL (5432), optional container registry (5000). Stack decision locked: Gitea over GitLab (lighter). Provides local CI/CD and reduces GitHub dependency for homelab repos.","phases":{"P0":"Provision VM 206 with spine-ready-v1","P1":"Deploy Gitea + PostgreSQL","P2":"Configure Gitea Actions runner","P3":"Integrate with Authentik SSO","P4":"Migrate select repos from GitHub","P5":"Verify + closeout"},"stack_decisions":{"git_forge":"Gitea","ci_runner":"Gitea Actions"},"closed_at":"2026-02-08T16:24:38Z"}
{"id":"LOOP-TIMEZONE-CONSISTENCY-20260208","action":"close","closed_at":"2026-02-08T17:05:00Z","reason":"P0-P2 complete: 9/16 hosts fixed to America/New_York (both hypervisors + 7 VMs). nas already UTC-5 (Bogota label), ha already EST. Exceptions: automation-stack (sudo password), docker-host (down post-outage). New infra.timezone.set capability created for future use."}
{"loop_id":"LOOP-CAMERA-BASELINE-20260208","run_key":"CAMERA_BASELINE_20260208","created_at":"2026-02-08T19:00:00Z","status":"open","severity":"medium","owner":"@ronny","title":"Camera system baseline: SSOT extraction + offline camera restoration + Frigate prep","next_action":"P0 complete. P1: Fix ch5 IP conflict via NVR ISAPI (remote). P2: Restore ch2-4 cameras (physical visit).","blocked_by":null,"evidence":["docs/governance/CAMERA_SSOT.md","mailroom/state/loop-scopes/LOOP-CAMERA-BASELINE-20260208.scope.md"],"description":"Extract camera data from SHOP_SERVER_SSOT into dedicated CAMERA_SSOT.md. Fix ch5 IP conflict (remote), restore ch2-4 offline cameras (physical), query camera models/firmware, audit physical locations, plan Frigate deployment.","phases":{"P0":"Create CAMERA_SSOT.md with verified data [DONE]","P1":"Fix ch5 IP conflict via NVR ISAPI (remote)","P2":"Restore ch2-4 cameras (physical visit)","P3":"Query camera models/firmware per channel via ISAPI","P4":"Physical location audit (on-site walk)","P5":"Frigate deployment planning"}}
{"loop_id":"LOOP-BACKUP-STABILIZATION-20260208","action":"update","updated_at":"2026-02-08T17:50:00Z","severity":"high","title":"Backup stabilization: retention broken (maxfiles ignored), 2.89 TB reclaimable, offsite empty, 6/10 VMs never backed up","next_action":"P2: Set prune-backups keep-last=2 on tank-backups storage via pvesh. Then P3: manual prune or wait for self-heal.","phases":{"P0":"Audit + baseline [DONE]","P1":"Add all VMs to vzdump job [DONE]","P2":"Fix retention (prune-backups on storage)","P3":"Prune excess backups (manual or self-heal)","P4":"Wait for first full run (02:00 tomorrow)","P5":"Verify offsite self-heals","P6":"Fix Home Assistant backup (657h stale)","P7":"Infisical retention depth (1 file only)","P8":"Final verify + closeout"}}
{"loop_id":"LOOP-AI-CONSOLIDATION-20260208","run_key":"AI_CONSOLIDATION_20260208","created_at":"2026-02-08T04:00:00Z","status":"closed","severity":"medium","owner":"@ronny","title":"AI service consolidation: Qdrant + AnythingLLM + Open WebUI evaluation on VM 207","next_action":"Phase 0: evaluate RAM/GPU requirements for Qdrant + AnythingLLM workloads. No hard blocker but practical dependency on LOOP-DEV-TOOLS-DEPLOY-20260208 for CI integration.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-AI-CONSOLIDATION-20260208.scope.md"],"description":"Consolidate scattered AI services onto VM 207 on pve (shop R730XD, 8+ CPU, 16-32GB RAM). Migrate Qdrant (6333/6334) and AnythingLLM (3002) from MacBook, evaluate Open WebUI (3000) placement (currently VM 202). Ollama stays on VM 202 (tight n8n integration). Split ai-services Infisical project post-migration.","phases":{"P0":"Evaluate RAM/GPU requirements","P1":"Provision VM 207 with spine-ready-v1","P2":"Migrate Qdrant from MacBook","P3":"Migrate AnythingLLM from MacBook","P4":"Evaluate Open WebUI placement (move or keep on 202)","P5":"Split ai-services Infisical project","P6":"Verify + closeout"},"placement_decisions":{"ollama":"stays on VM 202 (n8n integration)"},"closed_at":"2026-02-08T18:09:03Z"}
{"loop_id":"LOOP-MEDIA-AGENT-WORKBENCH-20260208","run_key":"MEDIA_AGENT_WORKBENCH_20260208","created_at":"2026-02-08T22:00:00Z","status":"open","severity":"medium","owner":"@ronny","title":"Media domain agent: dedicated workbench agent + MCP tools for download-stack (VM 209) + streaming-stack (VM 210)","next_action":"P0: Design agent contract, tool inventory, config schema. Pull current Recyclarr config from VM 209.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-MEDIA-AGENT-WORKBENCH-20260208.scope.md"],"description":"Build a dedicated media domain agent in the workbench with MCP tools for Radarr, Sonarr, Jellyfin, Bazarr, Recyclarr. Spine owns infrastructure (compose, health, routing); this agent owns the application layer (language profiles, quality settings, subtitle prefs, library hygiene). Triggered by Jellyfin serving non-English audio for 'The Beach House' — root cause: unmanaged Radarr language profiles.","phases":{"P0":"Design: agent contract, tool inventory, config schema","P1":"Audit current Recyclarr config + Radarr/Sonarr language profiles on VM 209","P2":"Build core MCP tools (radarr, sonarr, jellyfin — read-only)","P3":"Add config governance (recyclarr.yml tracked, push-to-VM)","P4":"Build troubleshooting playbooks + write tools","P5":"Update MCPJungle media-stack server for VM 209/210","P6":"Fix 'The Beach House' — end-to-end validation","P7":"Verify + closeout"}}
{"loop_id":"LOOP-AUDIT-WORKBENCH-SYNC-20260208","run_key":"AUDIT_WORKBENCH_SYNC_20260208","created_at":"2026-02-08T20:24:35Z","status":"open","severity":"high","owner":"@ronny","title":"Workbench sync: 23 stale artifacts from VM restructuring, AI consolidation, and observability deployment","next_action":"P0: Archive docker-compose.monitoring.yml (duplicate monitoring risk), refresh CONTAINER_INVENTORY.md, fix RAG script localhost defaults, register automation-stack companion services in SERVICE_REGISTRY.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-AUDIT-WORKBENCH-SYNC-20260208.scope.md","agentic-spine-audit-2026-02-08.docx"],"description":"Four-subagent cross-VM audit found 23 disconnects. Spine governance is correct. Drift is entirely workbench-side: stale CONTAINER_INVENTORY (pre-migration), legacy compose files (docker-compose.monitoring.yml could create duplicate monitoring), broken RAG scripts (localhost:3002 defaults), 4 unregistered automation-stack services. 19 mailroom tasks (4 P0 critical, 6 P1 high, 9 P2 medium).","phases":{"P0":"Critical fixes (archive monitoring compose, refresh inventory, fix RAG scripts, register automation services)","P1":"Governance sync (deprecate RAG docs, update INFRASTRUCTURE_MAP, archive stale compose, fix SSH targets, Prometheus scraping)","P2":"Completeness (monitoring inventory, node-exporter registry, staged compose, CRON_REGISTRY, legacy cleanup)","P3":"Verify + closeout"}}
{"loop_id":"LOOP-MEDIA-AGENT-WORKBENCH-20260208","run_key":"MEDIA_AGENT_WORKBENCH_20260208","created_at":"2026-02-08T22:00:00Z","status":"closed","severity":"medium","owner":"@ronny","title":"Media domain agent: dedicated workbench agent + MCP tools for download-stack (VM 209) + streaming-stack (VM 210)","next_action":"P0: Design agent contract, tool inventory, config schema. Pull current Recyclarr config from VM 209.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-MEDIA-AGENT-WORKBENCH-20260208.scope.md"],"description":"Build a dedicated media domain agent in the workbench with MCP tools for Radarr, Sonarr, Jellyfin, Bazarr, Recyclarr. Spine owns infrastructure (compose, health, routing); this agent owns the application layer (language profiles, quality settings, subtitle prefs, library hygiene). Triggered by Jellyfin serving non-English audio for 'The Beach House' — root cause: unmanaged Radarr language profiles.","phases":{"P0":"Design: agent contract, tool inventory, config schema","P1":"Audit current Recyclarr config + Radarr/Sonarr language profiles on VM 209","P2":"Build core MCP tools (radarr, sonarr, jellyfin — read-only)","P3":"Add config governance (recyclarr.yml tracked, push-to-VM)","P4":"Build troubleshooting playbooks + write tools","P5":"Update MCPJungle media-stack server for VM 209/210","P6":"Fix 'The Beach House' — end-to-end validation","P7":"Verify + closeout"},"closed_at":"2026-02-08T21:11:03Z"}
{"loop_id":"LOOP-MEDIA-STACK-SPLIT-20260208","action":"update","updated_at":"2026-02-09T01:25:00Z","blocked_by":null,"next_action":"Phase 6 soak in progress (72h started 2026-02-08). VM 201 stopped (0 containers). VM 209: 24/24 healthy. VM 210: 9/10 healthy. Soak ends ~2026-02-11. After soak: qm stop 201, wait 48h, qm destroy 201 --purge, close loop.","phases":{"P0":"Design container split + NFS mount plan [DONE]","P1":"Provision VM 209 + VM 210 [DONE]","P2":"Migrate download containers to VM 209 [DONE]","P3":"Migrate streaming containers to VM 210 [DONE]","P4":"Update NFS mounts + Cloudflare tunnel routes [DONE]","P5":"Decommission VM 201 [DONE — stopped, pending destroy after soak]","P6":"Soak + verify + closeout [IN PROGRESS — 72h soak started 2026-02-08]"}}
{"loop_id":"LOOP-BACKUP-STABILIZATION-20260208","action":"update","updated_at":"2026-02-09T01:28:00Z","severity":"high","title":"Backup stabilization: retention broken (maxfiles ignored), 2.89 TB reclaimable, offsite sync wired but untested","next_action":"P2: Set prune-backups keep-last=2 on tank-backups storage via pvesh (mutating, needs approval). Then P3: manual prune or wait for self-heal. P4: wait for first full 10-VM run (02:00 tomorrow).","phases":{"P0":"Audit + baseline [DONE]","P1":"Add all VMs to vzdump job [DONE — 10/10 VMs covered, all have fresh backups]","P2":"Fix retention (prune-backups on storage)","P3":"Prune excess backups (manual or self-heal)","P4":"Wait for first full run (02:00 tomorrow)","P5":"Verify offsite self-heals","P6":"Fix Home Assistant backup (657h stale)","P7":"Infisical retention depth (1 file only)","P8":"Final verify + closeout"}}
{"loop_id":"LOOP-BACKUP-STABILIZATION-20260208","action":"update","updated_at":"2026-02-09T01:32:00Z","severity":"high","title":"Backup stabilization: retention fixed, offsite sync wired but untested","next_action":"P3: Wait for next vzdump run (02:00 pve time ~07:00 EST) — prune-backups will auto-clean excess. P4: verify all 10 VMs OK + pruned to keep-last=2. P5-P8: NAS/offsite targets (blocked by home network).","phases":{"P0":"Audit + baseline [DONE]","P1":"Add all VMs to vzdump job [DONE — 10/10 VMs covered]","P2":"Fix retention (prune-backups on storage) [DONE — keep-last=2 set]","P3":"Wait for next full run + auto-prune","P4":"Verify all VMs OK + pruned","P5":"Verify offsite self-heals","P6":"Fix Home Assistant backup (657h stale)","P7":"Infisical retention depth (1 file only)","P8":"Final verify + closeout"}}
{"id":"LOOP-AUDIT-WORKBENCH-SYNC-20260208","action":"close","closed_at":"2026-02-09T02:01:00Z","reason":"P0-P1 complete (workbench commits 92a9af9 + 6ff7e16). MT-4 already in spine. MT-9 Prometheus scrape applied to VM 205. P2 mechanical items (MT-11–18) deferred as non-blocking legacy cleanup. spine.verify 49/49 PASS, services.health.status all OK."}
{"loop_id":"LOOP-DEV-TOOLS-GITEA-STANDARDIZATION-20260209","run_key":"DEV_TOOLS_GITEA_STANDARDIZATION_20260209","created_at":"2026-02-09T03:00:00Z","status":"open","severity":"medium","owner":"@ronny","title":"Gitea standardization: canonical origin, automated backup, CI workflow, durable mirror token","next_action":"P0-P2 complete (governance + backup script + CI + D50). P3: generate GitHub classic PAT (browser). P4: swap remotes on MacBook. P5: deploy backup cron on dev-tools. P6: SSO browser test + closeout.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-DEV-TOOLS-GITEA-STANDARDIZATION-20260209.scope.md"],"description":"Standardize Gitea as canonical origin for agentic-spine and workbench. GitHub becomes push mirror. Closes 6 gaps: automated app-level backup (GAP-OP-050/051), durable mirror token (GAP-OP-052), CI workflow (GAP-OP-053), SSO test (GAP-OP-054), monitoring depth (GAP-OP-055).","phases":{"P0":"Gap registration + loop scope [DONE]","P1":"Backup script + inventory entry [DONE]","P2":"CI workflow + D50 drift gate [DONE]","P3":"GitHub PAT replacement (deferred — browser)","P4":"Remote cutover (deferred — after P3)","P5":"Deploy backup cron on dev-tools (deferred — SSH)","P6":"Authentik SSO browser test + closeout (deferred)"}}
{"id":"LOOP-INFRA-CORE-AUDIT-20260209","action":"close","closed_at":"2026-02-09T02:54:00Z","reason":"T1: caddy-auth + vaultwarden added to STACK_REGISTRY.yaml. T2: shop VM LAN IPs added to DEVICE_IDENTITY_SSOT.md. T3: spine.verify 50/50 PASS. Low-severity findings (no swap, caddy health check, accept-routes, Grafana Caddy block) deferred."}
{"loop_id":"LOOP-OBSERVABILITY-HYGIENE-20260208","run_key":"OBSERVABILITY_HYGIENE_20260208","created_at":"2026-02-08T23:00:00Z","status":"open","severity":"medium","owner":"@ronny","title":"Observability hygiene: fix Prometheus localhost targets, stale cAdvisor, Loki health check","next_action":"P0: register loop + gaps. P1: fix prometheus.yml. P2: fix Loki health check. P3: deploy to VM 205. P4: closeout.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-OBSERVABILITY-HYGIENE-20260208.scope.md"],"description":"Post-deployment audit of VM 205 found 4 issues: Prometheus localhost scrape targets resolve to [::1] inside container (2 targets DOWN), stale cAdvisor targets (never deployed), Loki health check unhealthy (start_period too short), Grafana secret path undocumented. All drift gates pass. Zero staged-vs-live drift. Runtime config gaps from initial deployment.","phases":{"P0":"Register loop + file gaps","P1":"Fix prometheus.yml","P2":"Fix Loki health check","P3":"Deploy to VM 205 + verify","P4":"Closeout"}}
{"id":"LOOP-OBSERVABILITY-HYGIENE-20260208","action":"close","closed_at":"2026-02-09T03:10:00Z","reason":"P0-P4 complete. Prometheus: 5/5 targets UP (was 3/7). localhost→100.120.163.70 for loki+node-exporter. Removed 2 stale cAdvisor targets. Loki: start_period 30s→720s, retries 3→5 (covers 10-min compactor ring wait). 50/50 drift gates PASS. GAP-OP-056/057/058 fixed. GAP-OP-059 deferred (Grafana secret path)."}
{"id":"LOOP-BACKUP-STABILIZATION-20260208","action":"close","closed_at":"2026-02-09T07:10:00Z","reason":"P0-P5 complete. vzdump: 10/10 VMs covered, all keep-last=2, full pass in 2h17m. App backups: vaultwarden (02:45) + infisical (02:50) crons running with fresh files. Offsite: sync script deployed, manual trigger confirmed data arriving on NAS (909M+ of VM 204 transferred). Retention: prune-backups keep-last=2 set on tank-backups storage. D19 drift gate PASS. 14.7TB free on tank-backups."}
{"loop_id":"LOOP-MCPJUNGLE-RELOCATION-20260209","run_key":"MCPJUNGLE_RELOCATION_20260209","created_at":"2026-02-09T03:01:34Z","status":"open","severity":"medium","owner":"@ronny","title":"MCPJungle relocation: move MCP runtime off docker-host onto automation-stack","next_action":"P2-P6: sync stack to automation-stack, copy .env, docker compose up, stop on docker-host, update SSOT/bindings + verify receipts","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-MCPJUNGLE-RELOCATION-20260209.scope.md"],"description":"MCPJungle currently runs on docker-host (Mint OS production plane). This loop relocates it to automation-stack (tooling plane) to prevent coupling and keep specialized MCP tooling separate from production workloads. Public exposure via CF tunnel/Auth is deferred.","phases":{"P0":"Confirm current docker-host deploy + ports","P1":"Update workbench compose for host-agnostic bind + no external networks","P2":"Sync compose/build context to automation-stack","P3":"Copy .env secrets from docker-host to automation-stack (no secret printing)","P4":"Bring up MCPJungle on automation-stack + smoke health","P5":"Stop MCPJungle on docker-host","P6":"Update spine SSOT/bindings + verify receipts"}}
{"id":"LOOP-MCPJUNGLE-RELOCATION-20260209","action":"close","closed_at":"2026-02-09T07:30:00Z","reason":"P0-P6 complete. MCPJungle running and healthy on automation-stack (100.98.70.70:8080). Removed from docker-host. SSOT bindings updated: docker.compose.targets.yaml, services.health.yaml, SERVICE_REGISTRY.yaml."}
{"loop_id":"LOOP-VM-AUDIT-CLEANUP-20260209","run_key":"VM_AUDIT_CLEANUP_20260209","created_at":"2026-02-09T12:00:00Z","status":"open","severity":"medium","owner":"@ronny","title":"VM audit cleanup: SSOT + binding gaps from cross-VM deep audit (204-210)","next_action":"P0: Fix STACK_REGISTRY (ai-consolidation), SHOP_VM_ARCHITECTURE (3 subsections), DEVICE_IDENTITY (streaming-stack role), services.health (spotisub). P1: naming.policy fixes. P2: Qdrant healthcheck. P3: verify + close.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-VM-AUDIT-CLEANUP-20260209.scope.md"],"description":"Cross-VM deep audit of all 6 shop VMs (204-210) found them operationally healthy (53 containers). Two gaps fixed in-session. This loop collects remaining doc/governance gaps: missing STACK_REGISTRY entry (207), missing SHOP_VM_ARCHITECTURE subsections (209/210/207), incomplete DEVICE_IDENTITY role (210), missing health probe (spotisub), naming.policy gaps (207/205), Qdrant healthcheck (207).","phases":{"P0":"SSOT fixes (STACK_REGISTRY, SHOP_VM_ARCHITECTURE, DEVICE_IDENTITY, services.health)","P1":"Binding fixes (naming.policy)","P2":"Staged config fix (Qdrant healthcheck)","P3":"Verify + closeout"}}
{"id":"LOOP-VM-AUDIT-CLEANUP-20260209","action":"close","closed_at":"2026-02-09T12:48:00Z","reason":"T1-T10 complete. STACK_REGISTRY (ai-consolidation), SHOP_VM_ARCHITECTURE (download-stack + streaming-stack subsections), DEVICE_IDENTITY (streaming-stack role expanded), services.health (spotisub), naming.policy (ai-consolidation + observability fix), Qdrant healthcheck staged. spine.verify 50/50 PASS."}
{"loop_id":"LOOP-CADDY-PROTO-FIX-20260209","run_key":"CADDY_PROTO_FIX_20260209","created_at":"2026-02-09T12:55:00Z","status":"open","severity":"high","owner":"@ronny","title":"Caddy X-Forwarded-Proto fix: Authentik OIDC generated http:// URLs behind CF tunnel","next_action":"T6-T7: audit Authentik config + verify all proxied services. T8: SSO browser test. T9: close.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-CADDY-PROTO-FIX-20260209.scope.md"],"description":"Authentik OAuth2 flow for Gitea SSO failed. Root cause: Caddy passed X-Forwarded-Proto: http to Authentik (CF tunnel terminates TLS). Fix: header_up X-Forwarded-Proto https on all Authentik proxy blocks. Staged Caddyfile also synced (was stale).","phases":{"T1":"Diagnose [DONE]","T2":"Fix Caddyfile [DONE]","T3":"Sync staged [DONE]","T4":"Deploy + reload [DONE]","T5":"Verify OIDC https [DONE]","T6":"Audit Authentik config","T7":"Verify proxied services","T8":"SSO browser test","T9":"Close"}}
{"id":"LOOP-VAULTWARDEN-GOVERNANCE-20260209","status":"open","severity":"medium","owner":"@ronny","title":"Vaultwarden governance: secrets to Infisical + restore runbook + namespace policy","opened":"2026-02-09T12:55:00Z","scope_doc":"mailroom/state/loop-scopes/LOOP-VAULTWARDEN-GOVERNANCE-20260209.scope.md"}
{"id":"LOOP-VAULTWARDEN-GOVERNANCE-20260209","status":"closed","severity":"medium","owner":"@ronny","title":"Vaultwarden governance: secrets to Infisical + restore runbook + namespace policy","closed":"2026-02-09T13:05:00Z","scope_doc":"mailroom/state/loop-scopes/LOOP-VAULTWARDEN-GOVERNANCE-20260209.scope.md"}
{"loop_id":"LOOP-N8N-AGENT-20260209","run_key":"N8N_AGENT_20260209","created_at":"2026-02-09T13:20:00Z","status":"open","severity":"medium","owner":"@ronny","title":"n8n-agent: workflow governance + MCP tools (repo-as-log)","next_action":"P0-P5: scaffold workbench agent, fork MCP tools, add playbooks/tests, register in spine agent registry, verify + close.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-N8N-AGENT-20260209.scope.md"],"description":"Create the first automation-domain agent (n8n-agent) to govern n8n workflow changes with receipts and repo exports. Workbench owns application-layer workflow governance; spine owns infra deploy/routing/secrets/backups.","phases":{"P0":"Agent contract + boundaries","P1":"Scaffold workbench agent","P2":"Fork MCP tools into agent-owned surface","P3":"Playbooks + smoke check","P4":"Register in spine discovery registry","P5":"Verify + closeout"}}
{"id":"LOOP-N8N-AGENT-20260209","action":"close","closed_at":"2026-02-09T13:25:00Z","reason":"P0-P5 complete. Workbench n8n-agent scaffolded (tools + playbooks + tests). Spine registry + contract added. docs.lint + spine.verify PASS."}
{"id":"LOOP-CADDY-PROTO-FIX-20260209","action":"close","closed_at":"2026-02-09T13:15:00Z","reason":"T1-T12 complete. Caddy X-Forwarded-Proto https added to all Authentik proxy blocks. Staged Caddyfile synced. All 4 OIDC providers return https://. Gitea restarted to clear stale cache. D51 caddy-proto-lock gate created. deploy.dependencies.yaml binding created. GAP-OP-063 fixed. 51/51 drift gates PASS."}
{"id":"LOOP-DEV-TOOLS-GITEA-STANDARDIZATION-20260209","action":"close","closed_at":"2026-02-09T13:15:00Z","reason":"P0-P6 complete. Gitea is canonical origin, GitHub push mirror with durable PAT. Backup cron deployed (02:55 daily). D50 CI gate. SSO browser test passed after Caddy proto fix (LOOP-CADDY-PROTO-FIX-20260209). GAP-OP-050 through GAP-OP-055 fixed. 51/51 drift gates PASS."}
{"loop_id":"LOOP-HOMELAB-CONNECTIVITY-20260209","run_key":"HOMELAB_CONNECTIVITY_20260209","created_at":"2026-02-09T14:35:22Z","status":"open","severity":"high","owner":"@ronny","title":"Homelab connectivity outage: core Tailscale nodes offline + Cloudflare 530","next_action":"Restore tailnet connectivity (pve + core VMs) then rerun spine.verify and push to Gitea origin.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-HOMELAB-CONNECTIVITY-20260209.scope.md"]}
{"loop_id":"LOOP-UDR6-SHOP-CUTOVER-20260209","run_key":"UDR6_SHOP_CUTOVER_20260209","created_at":"2026-02-09T18:00:00Z","status":"open","severity":"high","owner":"@ronny","title":"UDR6 shop network deployment: take ownership of routing, DHCP, DNS","next_action":"Pre-stage SSOT docs, create runbook, then physical cutover on-site.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-UDR6-SHOP-CUTOVER-20260209.scope.md"],"description":"Insert UDR6 between T-Mobile gateway and Dell N2024P switch. Re-IP shop LAN from 192.168.12.0/24 to 192.168.1.0/24. UDR6 owns DHCP with DNS pointing to Pi-hole. Update all SSOT docs, NFS exports, Tailscale routes. Combined cold boot opportunity for MD1400 SAS recovery.","phases":{"P1":"Pre-stage configs and SSOT docs (no downtime)","P2":"Physical cutover (maintenance window)","P3":"Verification checklist","P4":"SSOT finalization and governance closeout"}}
{"loop_id":"LOOP-HOMELAB-CONNECTIVITY-20260209","run_key":"HOMELAB_CONNECTIVITY_20260209","created_at":"2026-02-09T14:35:22Z","status":"closed","severity":"high","owner":"@ronny","title":"Homelab connectivity outage: core Tailscale nodes offline + Cloudflare 530","next_action":"Restore tailnet connectivity (pve + core VMs) then rerun spine.verify and push to Gitea origin.","blocked_by":null,"evidence":["mailroom/state/loop-scopes/LOOP-HOMELAB-CONNECTIVITY-20260209.scope.md"],"closed_at":"2026-02-09T15:30:36Z"}
