#!/usr/bin/env bash
set -euo pipefail

ROOT="${SPINE_ROOT:-$HOME/code/agentic-spine}"
AUDITS_DIR="$ROOT/receipts/audits/governance"
DATE="${1:-$(date +%Y-%m-%d)}"
OUTPUT_FILE="$AUDITS_DIR/slo-evidence-${DATE}.md"

mkdir -p "$AUDITS_DIR"

echo "SLO Evidence Collection: $DATE"
echo "================================"
echo

echo "Collecting verify.core.run..."
VERIFY_RESULT="$("$ROOT/ops/plugins/verify/bin/verify-topology" core --json 2>/dev/null || true)"
if [[ -z "$VERIFY_RESULT" ]]; then
  VERIFY_RESULT='{"total":0,"pass":0,"fail":0,"failing_ids":[]}'
fi
VERIFY_TOTAL=$(echo "$VERIFY_RESULT" | jq -r '.total // 0' 2>/dev/null || echo "0")
VERIFY_PASS=$(echo "$VERIFY_RESULT" | jq -r '.pass // 0' 2>/dev/null || echo "0")
VERIFY_FAIL=$(echo "$VERIFY_RESULT" | jq -r '.fail // 0' 2>/dev/null || echo "0")
FAILING_IDS=$(echo "$VERIFY_RESULT" | jq -r '.failing_ids // [] | join(", ")' 2>/dev/null || echo "")
# Guard against malformed/multiline numeric parse values before arithmetic.
VERIFY_TOTAL="$(echo "$VERIFY_TOTAL" | tr -cd '0-9')"
VERIFY_PASS="$(echo "$VERIFY_PASS" | tr -cd '0-9')"
VERIFY_FAIL="$(echo "$VERIFY_FAIL" | tr -cd '0-9')"
[[ -n "$VERIFY_TOTAL" ]] || VERIFY_TOTAL=0
[[ -n "$VERIFY_PASS" ]] || VERIFY_PASS=0
[[ -n "$VERIFY_FAIL" ]] || VERIFY_FAIL=0

echo "Collecting stability.control.snapshot..."
SNAPSHOT_STATUS="unknown"
INCIDENT_COUNT=0
WARN_COUNT=0
DOMAIN_SUMMARY=""

if timeout 90 "$ROOT/ops/plugins/observability/bin/stability-control-snapshot" --json > /tmp/slo-snap-$$ 2>&1; then
  SNAPSHOT_STATUS=$(jq -r '.status // "unknown"' /tmp/slo-snap-$$ 2>/dev/null || echo "unknown")
  INCIDENT_COUNT=$(jq -r '.incident_count // 0' /tmp/slo-snap-$$ 2>/dev/null || echo "0")
  WARN_COUNT=$(jq -r '.warn_count // 0' /tmp/slo-snap-$$ 2>/dev/null || echo "0")
  DOMAIN_SUMMARY=$(jq -r '.domains[] | "| \(.id) | \(.status) |"' /tmp/slo-snap-$$ 2>/dev/null || echo "")
  rm -f /tmp/slo-snap-$$
else
  DOMAIN_SUMMARY="| (timeout) | (timeout) |"
fi

echo "Collecting automation.stack.latency.status..."
AUTOMATION_LATENCY_STATUS="unknown"
AUTOMATION_LATENCY_P95_MS="n/a"
AUTOMATION_LATENCY_P99_MS="n/a"
AUTOMATION_LATENCY_N8N_MS="n/a"
AUTOMATION_LATENCY_SAMPLES="n/a"
AUTOMATION_LATENCY_FAILED_SAMPLES="n/a"

if timeout 90 "$ROOT/ops/plugins/observability/bin/automation-stack-latency-status" --json > /tmp/slo-automation-latency-$$ 2>&1; then
  AUTOMATION_LATENCY_STATUS="$(jq -r '.status // "unknown"' /tmp/slo-automation-latency-$$ 2>/dev/null || echo "unknown")"
  AUTOMATION_LATENCY_P95_MS="$(jq -r '.data.p95_ms // "n/a"' /tmp/slo-automation-latency-$$ 2>/dev/null || echo "n/a")"
  AUTOMATION_LATENCY_P99_MS="$(jq -r '.data.p99_ms // "n/a"' /tmp/slo-automation-latency-$$ 2>/dev/null || echo "n/a")"
  AUTOMATION_LATENCY_N8N_MS="$(jq -r '.data.n8n_quick.duration_ms // "n/a"' /tmp/slo-automation-latency-$$ 2>/dev/null || echo "n/a")"
  AUTOMATION_LATENCY_SAMPLES="$(jq -r '.data.total_samples // "n/a"' /tmp/slo-automation-latency-$$ 2>/dev/null || echo "n/a")"
  AUTOMATION_LATENCY_FAILED_SAMPLES="$(jq -r '.data.failed_samples // "n/a"' /tmp/slo-automation-latency-$$ 2>/dev/null || echo "n/a")"
  rm -f /tmp/slo-automation-latency-$$
fi

OPEN_LOOPS=$("$ROOT/bin/ops" loops list --open 2>/dev/null | grep -c "LOOP-" || echo "0")
OPEN_GAPS=$("$ROOT/bin/ops" status 2>/dev/null | grep -E "gaps:.*open" | grep -oE '[0-9]+' | head -1 || echo "0")

VERIFY_PCT=0
[[ "$VERIFY_TOTAL" -gt 0 ]] && VERIFY_PCT=$((VERIFY_PASS * 100 / VERIFY_TOTAL))
SLO_PASS="FAIL"
[[ "$VERIFY_PCT" -ge 95 ]] && [[ "$VERIFY_FAIL" -eq 0 ]] && [[ "$INCIDENT_COUNT" -eq 0 ]] && [[ "$AUTOMATION_LATENCY_STATUS" != "incident" ]] && SLO_PASS="PASS"

AUTOMATION_LATENCY_ICON="✅"
if [[ "$AUTOMATION_LATENCY_STATUS" == "incident" ]]; then
  AUTOMATION_LATENCY_ICON="❌"
elif [[ "$AUTOMATION_LATENCY_STATUS" == "warn" ]]; then
  AUTOMATION_LATENCY_ICON="⚠️"
elif [[ "$AUTOMATION_LATENCY_STATUS" == "unknown" ]]; then
  AUTOMATION_LATENCY_ICON="ℹ️"
fi

AUTOMATION_LATENCY_SAMPLE_ICON="✅"
if [[ "$AUTOMATION_LATENCY_FAILED_SAMPLES" == "n/a" ]]; then
  AUTOMATION_LATENCY_SAMPLE_ICON="ℹ️"
fi

cat > "$OUTPUT_FILE" << EOF
---
date: ${DATE}
type: slo-evidence-daily
snapshot_status: ${SNAPSHOT_STATUS}
verify_pass: ${VERIFY_PCT}%
slo_pass: ${SLO_PASS}
---

# SLO Evidence Daily Report

**Date:** ${DATE}  
**Snapshot Status:** ${SNAPSHOT_STATUS}  
**SLO Result:** ${SLO_PASS}

## SLO Metrics

| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| Verify Gates | ${VERIFY_PASS}/${VERIFY_TOTAL} (${VERIFY_PCT}%) | 100% | $([[ "$VERIFY_FAIL" -eq 0 ]] && echo "✅" || echo "❌") |
| Incidents | ${INCIDENT_COUNT} | 0 | $([[ "$INCIDENT_COUNT" -eq 0 ]] && echo "✅" || echo "❌") |
| Warnings | ${WARN_COUNT} | 0 | $([[ "$WARN_COUNT" -eq 0 ]] && echo "✅" || echo "⚠️") |
| Automation Latency Budget | ${AUTOMATION_LATENCY_STATUS} (p95=${AUTOMATION_LATENCY_P95_MS}ms, p99=${AUTOMATION_LATENCY_P99_MS}ms, n8n=${AUTOMATION_LATENCY_N8N_MS}ms) | non-incident | ${AUTOMATION_LATENCY_ICON} |
| Automation Latency Samples | total=${AUTOMATION_LATENCY_SAMPLES}, failed=${AUTOMATION_LATENCY_FAILED_SAMPLES} | failed <= budget | ${AUTOMATION_LATENCY_SAMPLE_ICON} |
| Open Loops | ${OPEN_LOOPS} | - | ℹ️ |
| Open Gaps | ${OPEN_GAPS} | - | ℹ️ |

## Domain Status

| Domain | Status |
|--------|--------|
${DOMAIN_SUMMARY:-| (none) | (none) |}

## Failing Verify Gates

$(if [[ -n "$FAILING_IDS" && "$FAILING_IDS" != "" ]]; then echo "$FAILING_IDS"; else echo "None"; fi)

## Evidence Trail

- **Generated:** $(date -u +%Y-%m-%dT%H:%M:%SZ)
- **Run Command:** \`./bin/ops cap run slo.evidence.daily\`

---

*Day 1 of 7-day SLO monitoring period*
EOF

echo
echo "Evidence written to: $OUTPUT_FILE"
echo
echo "Summary:"
echo "  Snapshot Status: $SNAPSHOT_STATUS"
echo "  Verify Gates: ${VERIFY_PASS}/${VERIFY_TOTAL} pass (${VERIFY_PCT}%)"
echo "  Incidents: $INCIDENT_COUNT | Warnings: $WARN_COUNT"
echo "  Open Loops: $OPEN_LOOPS | Open Gaps: $OPEN_GAPS"
echo
echo "SLO Target: ≥95% verify pass rate, 0 incidents for 7 consecutive days"
echo "Day 1 Status: $SLO_PASS"
