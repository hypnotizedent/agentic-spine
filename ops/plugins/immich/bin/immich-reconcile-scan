#!/usr/bin/env bash
# immich-reconcile-scan - Scan Immich for duplicate assets (SHA + pHash + EXIF)
# TRIAGE: Read-only scan. Fetches /api/duplicates and paginates /api/search/metadata.
#
# Outputs:
#   mailroom/outbox/immich-reconcile/<run_ts>/phash_groups.json
#   mailroom/outbox/immich-reconcile/<run_ts>/sha_groups.json
#   mailroom/outbox/immich-reconcile/<run_ts>/scan_summary.json
#
# Usage:
#   immich-reconcile-scan [--output-dir DIR]

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
SPINE_ROOT="${SPINE_ROOT:-$(cd "$SCRIPT_DIR/../../../.." && pwd)}"

stop(){ echo "STOP (2): $*" >&2; exit 2; }

command -v curl  >/dev/null 2>&1 || stop "missing dependency: curl"
command -v python3 >/dev/null 2>&1 || stop "missing dependency: python3"

IMMICH_HOST="${IMMICH_HOST:-100.114.101.50:2283}"
IMMICH_URL="http://${IMMICH_HOST}"
INFISICAL_AGENT="${SPINE_ROOT}/ops/tools/infisical-agent.sh"
CONNECT_TIMEOUT=10
MAX_TIME=120

# --- Parse args ---
OUTPUT_DIR=""
while [[ $# -gt 0 ]]; do
  case "$1" in
    --output-dir) OUTPUT_DIR="$2"; shift 2 ;;
    *) stop "unknown arg: $1" ;;
  esac
done

# --- Resolve API key ---
IMMICH_KEY=""
if [[ -x "$INFISICAL_AGENT" ]]; then
  IMMICH_KEY=$("$INFISICAL_AGENT" get infrastructure prod IMMICH_API_KEY 2>/dev/null) || true
  if [[ -z "$IMMICH_KEY" ]]; then
    IMMICH_KEY=$("$INFISICAL_AGENT" get immich prod IMMICH_API_KEY 2>/dev/null) || true
  fi
fi
[[ -n "$IMMICH_KEY" ]] || stop "IMMICH_API_KEY not found in Infisical"

# --- Prepare output directory ---
RUN_TS="$(date -u '+%Y%m%dT%H%M%SZ')"
if [[ -z "$OUTPUT_DIR" ]]; then
  OUTPUT_DIR="${SPINE_ROOT}/mailroom/outbox/immich-reconcile/${RUN_TS}"
fi
mkdir -p "$OUTPUT_DIR"

echo "immich.reconcile.scan"
echo "host: ${IMMICH_HOST}"
echo "output: ${OUTPUT_DIR}"
echo

# --- Helper: curl with API key ---
api_get() {
  local path="$1"
  curl -sf --connect-timeout "$CONNECT_TIMEOUT" --max-time "$MAX_TIME" \
    -H "x-api-key: ${IMMICH_KEY}" "${IMMICH_URL}${path}"
}

api_post() {
  local path="$1"
  local body="$2"
  curl -sf --connect-timeout "$CONNECT_TIMEOUT" --max-time "$MAX_TIME" \
    -H "x-api-key: ${IMMICH_KEY}" \
    -H "Content-Type: application/json" \
    -d "$body" "${IMMICH_URL}${path}"
}

# --- Step 1: Health check ---
echo "Checking Immich health..."
ping_ok=$(api_get "/api/server/ping" | python3 -c "import sys,json; d=json.load(sys.stdin); print('yes' if d.get('res')=='pong' else 'no')" 2>/dev/null) || ping_ok="no"
if [[ "$ping_ok" != "yes" ]]; then
  stop "Immich API unreachable or unhealthy"
fi
echo "  health: OK"

# --- Step 2: Fetch perceptual duplicate groups ---
echo "Fetching perceptual duplicate groups from /api/duplicates..."
PHASH_RAW="${OUTPUT_DIR}/_phash_raw.json"
if ! api_get "/api/duplicates" > "$PHASH_RAW" 2>/dev/null; then
  stop "Failed to fetch /api/duplicates"
fi
PHASH_SIZE=$(wc -c < "$PHASH_RAW" | tr -d ' ')
echo "  raw response: ${PHASH_SIZE} bytes"

# --- Step 3: Paginate all assets for SHA grouping ---
echo "Scanning all assets for SHA checksum grouping..."
ALL_ASSETS="${OUTPUT_DIR}/_all_checksums.jsonl"
: > "$ALL_ASSETS"

PAGE=1
PAGE_SIZE=1000
TOTAL_FETCHED=0

while true; do
  BATCH=$(api_post "/api/search/metadata" "{\"page\":${PAGE},\"size\":${PAGE_SIZE}}" 2>/dev/null) || break
  COUNT=$(echo "$BATCH" | python3 -c "
import sys, json
d = json.load(sys.stdin)
items = d.get('assets', {}).get('items', [])
for a in items:
    print(json.dumps({'id': a['id'], 'checksum': a.get('checksum',''), 'originalFileName': a.get('originalFileName',''), 'isTrashed': a.get('isTrashed', False)}))
print('---COUNT:' + str(len(items)), file=sys.stderr)
" 2>&1 1>>"$ALL_ASSETS")
  N=$(echo "$COUNT" | sed -n 's/---COUNT://p')
  N="${N:-0}"
  TOTAL_FETCHED=$((TOTAL_FETCHED + N))

  if [[ "$N" -lt "$PAGE_SIZE" ]]; then
    break
  fi
  PAGE=$((PAGE + 1))

  # Progress every 10 pages
  if [[ $((PAGE % 10)) -eq 0 ]]; then
    echo "  page ${PAGE}: ${TOTAL_FETCHED} assets fetched..."
  fi
done
echo "  total assets scanned: ${TOTAL_FETCHED}"

# --- Step 4: Process into output files ---
echo "Processing scan results..."
python3 - "$PHASH_RAW" "$ALL_ASSETS" "$OUTPUT_DIR" "$RUN_TS" << 'PYEOF'
import json
import sys
from collections import defaultdict

phash_raw_path = sys.argv[1]
all_checksums_path = sys.argv[2]
output_dir = sys.argv[3]
run_ts = sys.argv[4]

# --- Load pHash groups ---
with open(phash_raw_path) as f:
    phash_raw = json.load(f)

phash_groups = []
for g in phash_raw:
    group = {
        "duplicate_id": g["duplicateId"],
        "type": "phash",
        "asset_count": len(g.get("assets", [])),
        "assets": []
    }
    for a in g.get("assets", []):
        exif = a.get("exifInfo", {}) or {}
        asset = {
            "id": a["id"],
            "checksum": a.get("checksum", ""),
            "thumbhash": a.get("thumbhash"),
            "original_filename": a.get("originalFileName", ""),
            "original_path": a.get("originalPath", ""),
            "original_mime_type": a.get("originalMimeType", ""),
            "local_date_time": a.get("localDateTime"),
            "file_created_at": a.get("fileCreatedAt"),
            "is_trashed": a.get("isTrashed", False),
            "is_favorite": a.get("isFavorite", False),
            "is_archived": a.get("isArchived", False),
            "visibility": a.get("visibility"),
            "type": a.get("type", "IMAGE"),
            "exif": {
                "make": exif.get("make"),
                "model": exif.get("model"),
                "lens_model": exif.get("lensModel"),
                "f_number": exif.get("fNumber"),
                "exposure_time": exif.get("exposureTime"),
                "iso": exif.get("iso"),
                "focal_length": exif.get("focalLength"),
                "date_time_original": exif.get("dateTimeOriginal"),
                "latitude": exif.get("latitude"),
                "longitude": exif.get("longitude"),
                "city": exif.get("city"),
                "country": exif.get("country"),
                "state": exif.get("state"),
                "width": exif.get("exifImageWidth"),
                "height": exif.get("exifImageHeight"),
                "file_size": exif.get("fileSizeInByte"),
                "orientation": exif.get("orientation"),
                "description": exif.get("description"),
                "rating": exif.get("rating"),
            }
        }
        group["assets"].append(asset)
    phash_groups.append(group)

with open(f"{output_dir}/phash_groups.json", "w") as f:
    json.dump({"run_ts": run_ts, "type": "phash", "group_count": len(phash_groups), "groups": phash_groups}, f, indent=2)

# --- Build SHA groups from all-assets scan ---
sha_map = defaultdict(list)
with open(all_checksums_path) as f:
    for line in f:
        line = line.strip()
        if not line:
            continue
        try:
            rec = json.loads(line)
            cs = rec.get("checksum", "")
            if cs:
                sha_map[cs].append(rec)
        except json.JSONDecodeError:
            continue

# Only keep groups with 2+ assets (actual duplicates)
sha_groups = []
for checksum, assets in sorted(sha_map.items()):
    if len(assets) >= 2:
        sha_groups.append({
            "checksum": checksum,
            "type": "sha",
            "asset_count": len(assets),
            "assets": assets
        })

with open(f"{output_dir}/sha_groups.json", "w") as f:
    json.dump({"run_ts": run_ts, "type": "sha", "group_count": len(sha_groups), "groups": sha_groups}, f, indent=2)

# --- Summary ---
phash_asset_ids = set()
for g in phash_groups:
    for a in g["assets"]:
        phash_asset_ids.add(a["id"])

sha_asset_ids = set()
for g in sha_groups:
    for a in g["assets"]:
        sha_asset_ids.add(a["id"])

# Group size distributions
from collections import Counter
phash_sizes = Counter(g["asset_count"] for g in phash_groups)
sha_sizes = Counter(g["asset_count"] for g in sha_groups)

summary = {
    "run_ts": run_ts,
    "total_assets_scanned": sum(len(v) for v in sha_map.values()),
    "phash": {
        "group_count": len(phash_groups),
        "asset_count": len(phash_asset_ids),
        "size_distribution": dict(sorted(phash_sizes.items()))
    },
    "sha": {
        "group_count": len(sha_groups),
        "asset_count": len(sha_asset_ids),
        "size_distribution": dict(sorted(sha_sizes.items()))
    },
    "overlap": {
        "assets_in_both": len(phash_asset_ids & sha_asset_ids)
    }
}

with open(f"{output_dir}/scan_summary.json", "w") as f:
    json.dump(summary, f, indent=2)

# Print summary
print(f"  pHash groups: {summary['phash']['group_count']} ({summary['phash']['asset_count']} assets)")
print(f"  SHA groups:   {summary['sha']['group_count']} ({summary['sha']['asset_count']} assets)")
print(f"  Overlap:      {summary['overlap']['assets_in_both']} assets in both")
print(f"  Total scanned: {summary['total_assets_scanned']}")
PYEOF

# --- Cleanup temp files ---
rm -f "${OUTPUT_DIR}/_phash_raw.json" "${OUTPUT_DIR}/_all_checksums.jsonl"

echo
echo "Scan complete."
echo "output_dir: ${OUTPUT_DIR}"
echo "status: OK"
