# Operational Gaps - Runtime SSOT Discoveries
#
# These are NOT extraction gaps (see docs/core/AGENTIC_GAP_MAP.md)
# These are issues agents discover DURING work that are out of scope to fix immediately
#
# Review periodically, create cleanup loops for open gaps
#
# Status: authoritative
# Last verified: 2026-02-07

version: 1
updated: "2026-02-09T03:00Z"

gaps:
  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-INFRA-VM-RESTRUCTURE-20260206
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-001
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: stale-ssot
    doc: "ops/bindings/docker.compose.targets.yaml"
    description: |
      Still lists cloudflared, pihole, infisical (secrets) under docker-host
      after Phase 1 migration to infra-core. Services are now on VM 204.
    severity: low
    status: fixed
    fixed_in: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    notes: "Moved cloudflared/pihole/secrets to infra-core target, added vaultwarden"

  - id: GAP-OP-002
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "docs/governance/SERVICE_REGISTRY.yaml"
    description: |
      No infisical entry existed before migration. Agent had to discover
      the stack by inspecting Docker containers on docker-host.
    severity: medium
    status: fixed
    fixed_in: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    notes: "Entry added during migration work"

  - id: GAP-OP-003
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: agent-behavior
    doc: null
    description: |
      Agent guessed path ~/stacks/infisical/ instead of consulting
      docker.compose.targets.yaml binding first. SSOT had the correct
      path (~/stacks/secrets). This is a pattern issue, not a data issue.
    severity: low
    status: fixed
    fixed_in: "gap-sweep-20260208"
    notes: "Documented 'consult bindings first' in SESSION_PROTOCOL.md (Trace truth) and docs/brain/rules.md (Entry Points table)"

  - id: GAP-OP-004
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: duplicate-truth
    doc: "ops/bindings/infra.placement.policy.yaml"
    description: |
      Canonical infra-core/observability services were intended for shop Proxmox
      (pve) but relocation commands relied on manifest values without a
      canonical site/hypervisor placement lock. This allowed wrong-instance
      execution risk when target metadata drifted.
    severity: high
    status: fixed
    fixed_in: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    notes: "Added infra placement policy SSOT + D37 lock + command-time enforcement in relocation/provision scripts"

  - id: GAP-OP-005
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: stale-ssot
    doc: "ops/bindings/ssh.targets.yaml"
    description: |
      Hypervisor SSH target IDs are distinct (`pve`, `proxmox-home`) but runtime
      host identity is ambiguous: `proxmox-home` currently reports hostname `pve`.
      This creates decision-risk during relocation/provisioning because agents and
      operators can confuse shop/home hypervisors.
    severity: high
    status: fixed
    fixed_in: "LOOP-GOV-SPINE-SEAL-20260207"
    notes: |
      Guardrail added: infra.hypervisor.identity capability + mutating infra
      preconditions now fail on hostname/machine-id ambiguity. Host-level fix
      applied: proxmox-home canonical hostname updated from `pve` to
      `proxmox-home` via capability `infra.hypervisor.hostname.set`
      (receipt: RCAP-20260207-113039__infra.hypervisor.hostname.set__Rxrbe80548).

  - id: GAP-OP-006
    discovered_by: "LOOP-GOV-SPINE-SEAL-20260207"
    discovered_at: "2026-02-07"
    type: runtime-bug
    doc: "ops/plugins/infra/bin/infra-relocation-preflight"
    description: |
      Preflight host scan silently skipped service source hosts because it used
      an invalid yq expression (`.services[].from_host // empty`) and suppressed
      parser errors. The command also reported missing SSH targets/unreachable
      hosts as informational output instead of a failing gate, allowing
      decision flow to continue without required host evidence.
    severity: high
    status: fixed
    fixed_in: "LOOP-GOV-SPINE-SEAL-20260207"
    notes: |
      Fixed by correcting host extraction, deriving required hosts from active
      VM/service states, and enforcing hard failure on required host/SSOT
      mismatches.

  - id: GAP-OP-007
    discovered_by: "LOOP-GOV-SPINE-SEAL-20260207"
    discovered_at: "2026-02-07"
    type: runtime-bug
    doc: "ops/plugins/infra/bin/infra-relocation-service-transition"
    description: |
      Service and relocation state transitions allowed out-of-order decision
      updates (cutover/migrated) with warning-only behavior. This permitted
      metadata progression even when relocation state had not reached cutover.
    severity: medium
    status: fixed
    fixed_in: "LOOP-GOV-SPINE-SEAL-20260207"
    notes: |
      Transition scripts now hard-fail when relocation state and service/vm
      status are misaligned, and cutover state transition includes an execute-time
      vm readiness gate.

  - id: GAP-OP-008
    discovered_by: "LOOP-GOV-SPINE-SEAL-20260207"
    discovered_at: "2026-02-07"
    type: runtime-bug
    doc: "surfaces/verify/drift-gate.sh"
    description: |
      Top-level `spine.verify` can pass while `infra.hypervisor.identity` fails.
      During active relocation this produces a green global health signal even
      when hypervisor identity ambiguity remains unresolved.
    severity: high
    status: fixed
    fixed_in: "LOOP-GOV-SPINE-SEAL-20260207"
    notes: |
      Added D39 gate (`surfaces/verify/d39-infra-hypervisor-identity-lock.sh`)
      and wired it into `drift-gate.sh`. During active relocation states,
      `spine.verify` now fails if hypervisor identity invariants are violated.

  # ─────────────────────────────────────────────────────────────
  # Discovered during soak-window parallel execution 2026-02-07
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-009
    discovered_by: "soak-window-parallel-execution-20260207"
    discovered_at: "2026-02-07"
    type: stale-ssot
    doc: "surfaces/verify/contracts-gate.sh"
    description: |
      contracts-gate.sh (T5) referenced contracts at docs/ (e.g. docs/RECEIPTS_CONTRACT.md)
      but canonical files live at docs/core/ (e.g. docs/core/RECEIPTS_CONTRACT.md).
      Path mismatch caused ops verify to fail at foundation gate. The files exist;
      the gate paths were stale.
    severity: high
    status: fixed
    fixed_in: "soak-window-parallel-execution-20260207"
    notes: "Fixed contracts-gate.sh paths: docs/ → docs/core/ for all three contracts"

  - id: GAP-OP-010
    discovered_by: "soak-window-parallel-execution-20260207"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "ops/bindings/ssh.targets.yaml"
    description: |
      media-stack host has docker.compose.targets.yaml binding (ssh_target: media-stack)
      but no corresponding entry in ssh.targets.yaml. Prevents governed SSH access
      for media stack RCA work.
    severity: medium
    status: fixed
    fixed_in: "soak-window-parallel-execution-20260207"
    notes: "Added media-stack target (100.117.1.53, root, shop/pve) to ssh.targets.yaml"

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-HOST-CANONICALIZATION-20260207
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-011
    discovered_by: "LOOP-HOST-CANONICALIZATION-20260207"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "surfaces/verify/drift-gate.sh"
    description: |
      Hidden-root coverage missing from active drift gates. host.drift.audit
      passes while unmanaged hidden roots and a secret-bearing legacy file
      (~/.config/ronny-ops/env.sh) exist at home root. No gate enforces
      hidden-root inventory or forbidden pattern scanning.
    severity: high
    status: fixed
    fixed_in: "LOOP-HOST-CANONICALIZATION-20260207"
    notes: "D41 hidden-root governance lock + D42 code path case lock + env.sh deleted + backups quarantined"

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-INFRA-CADDY-AUTH-20260207 pre-work
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-012
    discovered_by: "LOOP-INFRA-CADDY-AUTH-20260207"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "Infisical project: infrastructure (prod)"
    description: |
      Required Authentik bootstrap secrets are not present in Infisical
      infrastructure/prod: AUTHENTIK_SECRET_KEY and AUTHENTIK_DB_PASSWORD.
      This blocks safe execute of staged caddy-auth stack deployment.
    severity: high
    status: fixed
    fixed_in: "LOOP-INFRA-CADDY-AUTH-20260207"
    notes: "Created in infrastructure/prod at /spine/vm-infra/caddy-auth (no root-path duplicates)."

  - id: GAP-OP-013
    discovered_by: "LOOP-INFRA-CADDY-AUTH-20260207"
    discovered_at: "2026-02-07"
    type: duplicate-truth
    doc: "Infisical project: infrastructure (prod)"
    description: |
      Infrastructure project still stores legacy keys at root path `/` (flat
      namespace). This risks future collisions with new VM-infra secrets unless
      all new infra keys are constrained under `/spine/*` namespaces.
    severity: medium
    status: fixed
    fixed_in: "secrets-namespace-migration-20260207"
    notes: |
      Enforcement added: secrets.namespace.status + secrets.namespace.policy freeze.
      Migration map staged at ops/staged/SECRETS_NAMESPACE_MIGRATION_MAP.md.
      P1 complete for /spine/platform/security (9 root duplicates removed) and
      P2 complete for /spine/network/edge (10 root duplicates removed) and
      P3 complete for /spine/storage/nas (6 root duplicates removed) and
      P4 complete for /spine/integrations/commerce-mail (9 root duplicates removed) and
      P5 complete for /spine/services/* (11 root duplicates removed) and
      P6 complete for /spine/ai/providers (4 root duplicates removed).
      Root key count reduced from baseline 49 -> 0 (49 removed).
      Evidence: ops/staged/SECRETS_NAMESPACE_P1_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P2_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P3_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P4_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P5_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P6_EXECUTION_20260207.md,
      RCAP-20260207-171547__secrets.p2.root_cleanup.execute__Rchmg11028,
      RCAP-20260207-180752__secrets.namespace.status__R4k6j64294.

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-GOV-CLI-TOOL-DISCOVERY-20260207
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-014
    discovered_by: "LOOP-GOV-CLI-TOOL-DISCOVERY-20260207"
    discovered_at: "2026-02-07"
    type: agent-behavior
    doc: null
    description: |
      Agent could not discover qrencode despite it being registered in
      maker.tools.inventory.yaml. The tool is domain-scoped (maker plugin)
      with no cross-domain discovery path. Session protocol, brain context,
      and rules all lack tool discovery guidance. Agent searched workbench
      repo, MCP catalogs, and file contents but never found the binding.
    severity: medium
    status: fixed
    fixed_in: "LOOP-GOV-CLI-TOOL-DISCOVERY-20260207"
    notes: |
      Fixed by: cli.tools.inventory.yaml (cross-domain catalog),
      generate-context.sh (tool injection into agent context),
      D44 drift gate (discovery chain validation),
      SESSION_PROTOCOL.md + rules.md updates (agent guidance).

  # ─────────────────────────────────────────────────────────────
  # Discovered during OL_HOME_BASELINE_FINISH audit 2026-02-07
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-015
    discovered_by: "OL_HOME_BASELINE_FINISH"
    discovered_at: "2026-02-07"
    type: runtime-bug
    doc: "docs/governance/MINILAB_SSOT.md"
    description: |
      PVE node-name mismatch on proxmox-home: hostname changed from `pve` to
      `proxmox-home` (GAP-OP-005 fix) but PVE node was not migrated. All VM/LXC
      configs live under /etc/pve/nodes/pve/ while PVE tools look under
      /etc/pve/nodes/proxmox-home/. Breaks qm list, pct list, pct exec, vzdump
      jobs. VMs running pre-rename continue but are unmanageable. VM 101 (immich)
      is stopped and cannot be restarted.
    severity: critical
    status: fixed
    fixed_in: "gap-sweep-20260208"
    notes: "Fixed: hostname reverted to pve (matching PVE node name). qm list, pct list, vzdump all functional. Tailscale hostname remains proxmox-home (independent). Exception recorded in naming.policy.yaml."

  - id: GAP-OP-017
    discovered_by: "LOOP-GOV-CLAUDE-ENTRYPOINT-LOCK-20260208"
    discovered_at: "2026-02-08"
    type: duplicate-truth
    doc: "~/.claude/CLAUDE.md"
    description: |
      Claude home config (CLAUDE.md, commands/ctx.md, settings.json, settings.local.json)
      contains standalone governance sections (Authority Order, Immutable Invariants, etc.)
      and uppercase ~/Code/ path references. No Claude-equivalent of D32 (Codex instruction
      source lock) exists. Runtime scripts reference .brain/ paths but no .brain/ directory
      exists — actual files live at docs/brain/. SESSION_PROTOCOL.md also references .brain/.
    severity: medium
    status: fixed
    fixed_in: "LOOP-GOV-CLAUDE-ENTRYPOINT-LOCK-20260208"
    notes: |
      Fixed by: CLAUDE.md rewritten to redirect shim, path case canonicalized across
      all governed Claude files, .brain/ references replaced with docs/brain/ in runtime
      scripts and SESSION_PROTOCOL.md, D46 + D47 drift gates added, agent.entrypoint.lock.yaml
      binding created, host-claude-entrypoint-lock plugin with status/enforce/execute modes.

  # ─────────────────────────────────────────────────────────────
  # Discovered during PVE baseline SSH audit 2026-02-07
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-018
    discovered_by: "OL_SHOP_BASELINE_FINISH"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "/etc/pve/jobs.cfg"
    description: |
      VM 204 (infra-core) is not included in the vzdump backup job on pve.
      The job covers VMs 200-203 but was created before VM 204 existed.
      infra-core runs cloudflared, pihole, infisical, vaultwarden, caddy,
      and authentik — all critical services with no hypervisor-level backup.
    severity: high
    status: fixed
    fixed_in: "LOOP-INFRA-SSOT-ALIGNMENT-20260208"
    notes: |
      Fixed: added vmid 204 to vzdump job in /etc/pve/jobs.cfg via SSH.
      Job now covers 200,201,202,203,204. Add VM 205 when provisioned.

  - id: GAP-OP-019
    discovered_by: "OL_SHOP_BASELINE_FINISH"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "pve crontab"
    description: |
      No ZFS scrub scheduled for the media pool on pve. Only the tank pool
      has a weekly scrub (`0 3 * * 0 zpool scrub tank`). The media pool's
      last scrub was CANCELED on 2026-01-11. Media pool is RAIDZ1 with 4x
      Seagate ST8000AS0002 (SMR archive drives) — higher risk profile than
      tank (RAIDZ2 enterprise SAS).
    severity: medium
    status: fixed
    fixed_in: "LOOP-INFRA-SSOT-ALIGNMENT-20260208"
    notes: |
      Fixed: added `0 4 * * 0 zpool scrub media` to pve crontab via SSH.
      Runs Sundays at 4am, offset from tank scrub (3am).

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-MEDIA-STACK-RCA-20260205 / ARCH-20260208
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-021
    discovered_by: "LOOP-MEDIA-STACK-RCA-20260205"
    discovered_at: "2026-02-07"
    type: agent-behavior
    doc: "ops/staged/MEDIA_RCA_DECISION_NOTE.md"
    description: |
      RCA decision note stated /opt/appdata/*.db files were "stale copies, not
      actively used." In reality these were active symlink targets created Dec 23,
      serving all 5 main databases. An agent following the decision note would have
      assumed the entire database layer was on NFS and may have attempted redundant
      or harmful migrations. The inaccuracy persisted for 2 days before correction.
    severity: medium
    status: fixed
    fixed_in: "LOOP-MEDIA-STACK-ARCH-20260208"
    notes: |
      Root cause: initial RCA SSH session saw files at /opt/appdata/ and assumed
      copies without checking symlink state (ls -la vs ls -l). Fixed: decision note
      corrected. Lesson: always use `ls -la` or `file` command to distinguish
      symlinks from regular files during NFS topology discovery.

  - id: GAP-OP-022
    discovered_by: "LOOP-MEDIA-STACK-ARCH-20260208"
    discovered_at: "2026-02-08"
    type: agent-behavior
    doc: null
    description: |
      Phase A symlink migration broke trailarr and posterizarr because their
      compose services lacked the /opt/appdata:/opt/appdata bind mount that
      radarr/sonarr/lidarr/prowlarr/jellyfin already had (Dec 23 setup).
      The playbook did not include a pre-check for container mount visibility.
      Failure mode: sqlite3.OperationalError with no mention of symlinks,
      making diagnosis non-obvious.
    severity: medium
    status: fixed
    fixed_in: "LOOP-MEDIA-STACK-ARCH-20260208"
    notes: |
      Fixed: added /opt/appdata:/opt/appdata volume to trailarr and posterizarr
      in docker-compose.yml on media-stack, containers recreated via compose up -d.
      Lesson codified in docs/brain/lessons/MEDIA_STACK_LESSONS.md.
      Future mitigation: any symlink migration playbook MUST include a
      pre-flight step: `docker inspect <service> --format '{{range .Mounts}}...'`
      to verify the symlink target path is visible inside the container.

  - id: GAP-OP-023
    discovered_by: "LOOP-MEDIA-STACK-ARCH-20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "ops/staged/MEDIA_STACK_ARCH_PHASE_A_PLAYBOOK_20260207.md"
    description: |
      Phase A playbook listed 5 target services but missed sonarr logs.db (2.2MB,
      constant writes) which was also on NFS. The playbook was written from the
      ARCH scope doc which only listed "databases STILL on NFS" but omitted logs.db
      files for services whose main .db was already symlinked. Discovery chain:
      scope doc partial → playbook incomplete → extra target found during execution.
    severity: low
    status: fixed
    fixed_in: "LOOP-MEDIA-STACK-ARCH-20260208"
    notes: |
      sonarr logs.db migrated during Phase A execution. Lesson: when scoping
      NFS-to-local migrations, enumerate ALL .db files per service (not just the
      main database). Use: find /mnt/docker/volumes/<service> -name '*.db' -type f

  - id: GAP-OP-024
    discovered_by: "LOOP-MEDIA-STACK-ARCH-20260208"
    discovered_at: "2026-02-08"
    type: agent-behavior
    doc: "ops/staged/MEDIA_STACK_ARCH_PHASE_A_PLAYBOOK_20260207.md"
    description: |
      Phase A playbook listed introskipper.db path as
      /mnt/docker/volumes/jellyfin/config/data/introskipper.db but actual path was
      /mnt/docker/volumes/jellyfin/config/data/introskipper/introskipper.db (extra
      directory level). Path was assumed from scope doc without SSH verification.
      Similarly, trailarr had a logs/logs.db not mentioned in the playbook.
    severity: low
    status: fixed
    fixed_in: "LOOP-MEDIA-STACK-ARCH-20260208"
    notes: |
      Correct paths discovered via `find` during execution. Lesson: playbooks for
      remote file operations MUST include a preflight `find` or `ls -la` step to
      confirm actual paths before mutation. Never trust doc-assumed paths for
      file-level operations.

  # ─────────────────────────────────────────────────────────────
  # Discovered during pre-vm-creation reliability hardening 2026-02-08
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-025
    discovered_by: "pre-vm-creation-reliability-hardening-20260208"
    discovered_at: "2026-02-08"
    type: stale-ssot
    doc: "ops/bindings/ssh.targets.yaml"
    description: |
      media-stack SSH target had the wrong user (root vs actual), causing
      ssh.target.status to fail with auth denied and misleading downstream
      tooling that relies on governed SSH.
    severity: medium
    status: fixed
    fixed_in: "pre-vm-creation-reliability-hardening-20260208"
    notes: |
      Fixed: media-stack entry updated to user=media. ssh.target.status confirms
      OK (192ms). Docker compose status resolves correctly via binding.

  - id: GAP-OP-026
    discovered_by: "pre-vm-creation-reliability-hardening-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "ops/plugins/docker/bin/docker-compose-status"
    description: |
      docker.compose.status assumed `ssh <id>` works via ~/.ssh/config aliases
      and did not resolve ssh_target via ops/bindings/ssh.targets.yaml. This
      produced false UNREACHABLE signals even when the governed SSH binding was
      correct (infra-core, observability).
    severity: high
    status: fixed
    fixed_in: "pre-vm-creation-reliability-hardening-20260208"
    notes: |
      Fixed: docker-compose-status now resolves host/user/port from ssh.targets.yaml
      (lines 170-178), uses binding defaults for StrictHostKeyChecking=no and
      UserKnownHostsFile=/dev/null. SSH error classification added. Verified working
      for infra-core (13 stacks), observability (5 stacks), dev-tools (1 stack).

  - id: GAP-OP-027
    discovered_by: "pre-vm-creation-reliability-hardening-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "ops/plugins/backup/bin/backup-status"
    description: |
      backup.status validated host IDs exist in ssh.targets.yaml but then used
      `ssh \"$HOST\"` (alias) and StrictHostKeyChecking=accept-new (known_hosts
      writes), violating HOME drift constraints and breaking when aliases are not
      present.
    severity: high
    status: fixed
    fixed_in: "pre-vm-creation-reliability-hardening-20260208"
    notes: |
      Fixed: backup-status now resolves host/user/port from ssh.targets.yaml
      (lines 107-118), uses binding defaults for StrictHostKeyChecking and
      UserKnownHostsFile=/dev/null. SSH error classification added. Verified
      working for pve and nas targets.

  - id: GAP-OP-028
    discovered_by: "pre-vm-creation-reliability-hardening-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "ops/bindings/secrets.binding.yaml + ops/plugins/secrets/bin/*"
    description: |
      Secrets capabilities failed when Infisical API URL was behind forward-auth
      (HTTP 302/HTML), causing jq parse errors and broken automation. A binding-
      supported internal API endpoint is required, and scripts must prefer it and
      fail with actionable errors on redirects/non-JSON responses.
    severity: high
    status: fixed
    fixed_in: "pre-vm-creation-reliability-hardening-20260208"
    notes: |
      Fixed: secrets.binding.yaml has internal_api_url (http://100.92.91.128:8088,
      direct Infisical bypassing Authentik). All secrets plugins prefer
      internal_api_url over api_url via yq fallthrough (.internal_api_url // .api_url).
      Verified in secrets-namespace-status, secrets-exec, secrets-projects-status,
      secrets-binding.

  - id: GAP-OP-016
    discovered_by: "OL_HOME_BASELINE_FINISH"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: null
    description: |
      No naming governance policy exists. Hostname, PVE node name, Tailscale
      hostname, ssh.targets.yaml ID, and DEVICE_IDENTITY_SSOT entries are all
      managed independently with no canonical mapping or rename procedure.
      This caused GAP-OP-015 (hostname changed without PVE node migration)
      and has caused repeated confusion (media-stack site attribution, hypervisor
      identity ambiguity).
    severity: high
    status: fixed
    fixed_in: "LOOP-NAMING-GOVERNANCE-20260207"
    notes: |
      Fixed by: naming.policy.yaml (canonical names + surface authority + rename
      procedure + allowed exceptions) + D45 naming consistency lock gate
      (cross-file verification wired into drift-gate.sh). Also fixed vault kind
      misclassification (lxc→vm) in placement policy and DEVICE_IDENTITY_SSOT.

  # ─────────────────────────────────────────────────────────────
  # Discovered during gap sweep 2026-02-08
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-020
    discovered_by: "gap-sweep-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "ops/commands/loops.sh"
    description: |
      ops loops list --open crashes with jq error at open_loops.jsonl:121.
      Close records use "id" key but the parser expects "loop_id", producing
      "Cannot index object with null". Additionally, open_loops.jsonl has
      54 open entries with massive duplication and 3 close records that never
      filter out closed loops. The loops subsystem is the spine's work-tracking
      surface and currently fails on every invocation.
    severity: high
    status: fixed
    fixed_in: "gap-sweep-20260208"
    notes: |
      Fixed: loops.sh jq filter now normalizes close records, deduplicates by loop_id,
      and applies close status. open_loops.jsonl compacted from 121 to 61 lines.
      6 completed loops closed. 9 open loops remain.

  # Discovered during CODE_AUDIT_20260208
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-029
    discovered_by: "CODE_AUDIT_20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "ops/bindings/secrets.namespace.policy.yaml"
    description: |
      secrets.namespace.status failed because the required Gitea secrets were
      missing from Infisical at /spine/vm-infra/gitea. This blocks governed
      automation and leaves the dev-tools deployment partially undocumented.
    severity: high
    status: fixed
    fixed_in: "CODE_AUDIT_20260208"
    notes: |
      Fixed by writing the required keys to Infisical under /spine/vm-infra/gitea:
      GITEA_ADMIN_PASSWORD, GITEA_API_TOKEN, GITEA_RUNNER_TOKEN,
      GITEA_OAUTH_CLIENT_ID, GITEA_OAUTH_CLIENT_SECRET. Verified via
      secrets.namespace.status (receipt RCAP-20260208-105831__secrets.namespace.status__Rwaub7817).

  - id: GAP-OP-030
    discovered_by: "CODE_AUDIT_20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "ops/plugins/cloudflare/bin/cloudflare-tunnel-ingress-status"
    description: |
      There was no spine-native way to export Cloudflare tunnel ingress rules.
      This forced manual dashboard checks and allowed docs-vs-dashboard drift.
    severity: medium
    status: fixed
    fixed_in: "CODE_AUDIT_20260208"
    notes: |
      Added cloudflare.tunnel.ingress.status (API export) and
      cloudflare.domain_routing.diff (diff vs DOMAIN_ROUTING_REGISTRY.yaml).

  - id: GAP-OP-031
    discovered_by: "CODE_AUDIT_20260208"
    discovered_at: "2026-02-08"
    type: stale-ssot
    doc: "docs/governance/DOMAIN_ROUTING_REGISTRY.yaml"
    description: |
      DOMAIN_ROUTING_REGISTRY.yaml drifted from the Cloudflare tunnel ingress
      configuration, creating extra/missing hostname entries and agent confusion
      (docs indicated services/paths that did not exist).
    severity: medium
    status: fixed
    fixed_in: "CODE_AUDIT_20260208"
    notes: |
      Registry updated and verified against live ingress export:
      cloudflare.domain_routing.diff status OK (receipt RCAP-20260208-110109__cloudflare.domain_routing.diff__Rwa0y18221).

  - id: GAP-OP-032
    discovered_by: "CODE_AUDIT_20260208"
    discovered_at: "2026-02-08"
    type: stale-ssot
    doc: "ops/bindings/docker.compose.targets.yaml"
    description: |
      docker.compose.targets.yaml referenced a docker-host storage stack path
      that does not exist, producing false negatives in docker.compose.status
      and confusing agents about the MinIO deployment source.
    severity: medium
    status: fixed
    fixed_in: "CODE_AUDIT_20260208"
    notes: |
      Removed the invalid compose target and aligned compose authority docs to the
      current SSOT model (VM-infra under ops/staged/**; live stack locations via
      docker.compose.targets.yaml).

  - id: GAP-OP-033
    discovered_by: "CODE_AUDIT_20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "ops/commands/cap.sh + surfaces/verify/drift-gate.sh"
    description: |
      ops capability execution was not worktree-safe: receipts and drift gates
      assumed the canonical repo CWD and scanned worktree-only paths, causing
      incorrect failures when operating from isolated worktrees.
    severity: high
    status: fixed
    fixed_in: "CODE_AUDIT_20260208"
    notes: |
      Introduced SPINE_CODE (current code root) distinct from SPINE_REPO
      (canonical receipts/runtime root). Updated capability CWDs to SPINE_CODE and
      hardened drift-gate scripts to ignore .worktrees and to compare canonical
      instruction sources.

  - id: GAP-OP-034
    discovered_by: "CODE_AUDIT_20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "docs/governance/BACKUP_GOVERNANCE.md"
    description: |
      Backup governance defines a requirement for app-level dumps per stack, but
      there is no documented, tested app-level backup/restore procedure for
      Authentik or Gitea (even if VM-level vzdump exists).
    severity: medium
    status: fixed
    fixed_in: "2026-02-08_app_backup_runbooks"
    notes: |
      Follow-up: add stack READMEs or runbooks covering export, restore, and a
      periodic restore test for both Authentik and Gitea.
      Evidence:
      - docs/governance/AUTHENTIK_BACKUP_RESTORE.md
      - docs/governance/GITEA_BACKUP_RESTORE.md

  # ─────────────────────────────────────────────────────────────
  # Discovered during OL_SHOP_BASELINE_FINISH remote audit 2026-02-08
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-037
    discovered_by: "OL_SHOP_BASELINE_FINISH"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "docs/governance/SHOP_SERVER_SSOT.md"
    description: |
      Dell MD1400 DAS shelf is physically cabled to R730XD (pve) via SAS cable
      (Dell 0GYK61) but zero drives are visible to the OS. Two root causes:
      (1) PM8072 SAS controller PCI vendor ID is 0x11f8 (Microchip, post-acquisition)
      but the Linux pm80xx driver only recognizes 0x117C (PMC-Sierra, pre-acquisition) —
      driver does not auto-bind. (2) Hot-loading the driver via new_id fails because
      the PM8072 firmware requires cold-boot initialization — MPI handshake times out
      with "FW is not ready" (chip_init failed, ret=-16/EBUSY).
      Result: entire MD1400 shelf storage is inaccessible. Unknown drive count and
      capacity sitting completely dark.
    severity: critical
    status: open
    fixed_in: null
    notes: |
      Fix requires: (1) persistent modprobe config with install hook to inject PCI ID
      (echo "11f8 8072" > /sys/bus/pci/drivers/pm80xx/new_id), (2) cold boot of pve
      (power off/on, not reboot) for PM8072 firmware initialization. Maintenance window
      needed — all 10 VMs go down. Loop: LOOP-MD1400-SAS-RECOVERY-20260208.
      Upstream: PCI ID gap in Linux pm80xx driver (kernel 6.14.8-2-pve).

  - id: GAP-OP-038
    discovered_by: "OL_SHOP_BASELINE_FINISH"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "/etc/pve/jobs.cfg"
    description: |
      vzdump backup job on pve covers VMs 200-204 only. VMs 205 (observability),
      206 (dev-tools), 207 (ai-consolidation), 209 (download-stack), and
      210 (streaming-stack) are running with no hypervisor-level backup.
      5 VMs provisioned since the backup job was last updated.
    severity: high
    status: fixed
    fixed_in: "LOOP-BACKUP-STABILIZATION-20260208"
    notes: |
      P1 fix: vzdump vmid list updated to 204,205,206,201,202,203,200 (reordered
      so critical infra-core runs first, heaviest VM 200 last). VMs 207/209/210
      deferred to P2 (enable as each stabilizes). Orphaned 34GB partial backup
      from power-outage crash cleaned up.

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-CAMERA-BASELINE-20260208
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-039
    discovered_by: "LOOP-CAMERA-BASELINE-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "docs/governance/CAMERA_SSOT.md"
    description: |
      NVR channels 2, 3, 4 report chanDetectResult=notExist via ISAPI. Cameras
      are powered off or disconnected from the Netgear PoE switch that feeds the
      NVR's internal 192.168.254.0/24 network. Requires physical visit to the
      upstairs 9U rack to inspect PoE switch ports and camera cabling.
    severity: medium
    status: fixed
    fixed_in: "LOOP-CAMERA-BASELINE-20260208"
    notes: |
      Subsumed by LOOP-CAMERA-BASELINE-20260208 Phase P2 + OL_SHOP_BASELINE_FINISH.
      Physical visit required — cameras are still offline but gap tracking is redundant
      with loop ownership. Fix will occur during on-site audit.

  - id: GAP-OP-040
    discovered_by: "LOOP-CAMERA-BASELINE-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "docs/governance/CAMERA_SSOT.md"
    description: |
      NVR channel 5 has IP address 192.168.254.7 which conflicts with channel 4
      (same IP). ISAPI reports chanDetectResult=ipAddrConflict for ch5. This is
      a configuration error in the NVR channel assignment, not a physical issue.
    severity: low
    status: fixed
    fixed_in: "LOOP-CAMERA-BASELINE-20260208"
    notes: |
      Fixed: ch5 IP reassigned from 192.168.254.7 to 192.168.254.5 via ISAPI PUT
      (2026-02-08). NVR accepted change (statusCode=1/OK). Ch5 now shows
      netUnreachable (camera physically disconnected, not IP conflict).
      Previous name "SCREEN PRINT" auto-reset to "IPCamera 05" by NVR.

  - id: GAP-OP-041
    discovered_by: "LOOP-CAMERA-BASELINE-20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "Infisical project: infrastructure (prod)"
    description: |
      Shop infrastructure credentials (NVR, iDRAC, Dell switch) were referenced
      in SHOP_SERVER_SSOT.md and CAMERA_SSOT.md at Infisical paths
      /spine/shop/nvr/*, /spine/shop/idrac/*, /spine/shop/switch/* but all three
      paths were empty. Agent could not diagnose camera IP conflict without
      asking the user for credentials interactively. SSOT docs implied secrets
      existed when they did not.
    severity: medium
    status: fixed
    fixed_in: "LOOP-CAMERA-BASELINE-20260208"
    notes: |
      Fixed: all three credential sets stored in Infisical (6 keys total):
      /spine/shop/nvr (NVR_ADMIN_USER, NVR_ADMIN_PASSWORD),
      /spine/shop/idrac (IDRAC_ADMIN_USER, IDRAC_ADMIN_PASSWORD),
      /spine/shop/switch (SWITCH_ADMIN_USER, SWITCH_ADMIN_PASSWORD).

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-BACKUP-STABILIZATION-20260208
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-042
    discovered_by: "LOOP-BACKUP-STABILIZATION-20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "ops/bindings/backup.inventory.yaml"
    description: |
      No SSH key authentication existed from pve or infra-core to the NAS
      (Synology 918+ at house, Tailscale). Blocked all offsite backup sync.
      The backup binding had an enabled offsite target (vm-200-docker-host-offsite)
      and NAS directories existed, but no sync mechanism or SSH trust was ever
      established. backup.status reported no_matches — a false signal that
      appeared like a path problem when the real issue was no connectivity.
    severity: high
    status: fixed
    fixed_in: "LOOP-BACKUP-STABILIZATION-20260208"
    notes: |
      Fixed: pve RSA key and infra-core ed25519 key added to
      ronadmin@nas authorized_keys. Verified bidirectional SSH from both hosts.
      Offsite rsync script and app backup scripts both use this trust path.

  - id: GAP-OP-043
    discovered_by: "LOOP-BACKUP-STABILIZATION-20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "ops/bindings/backup.inventory.yaml"
    description: |
      Vaultwarden backup stopped when the service migrated from proxmox-home
      (LXC 102) to infra-core (VM 204) on 2026-02-05. No backup cron was
      re-established on the new host. Last NAS backup was 2026-02-05 02:45,
      creating a 3-day gap (Feb 5-8) where vaultwarden had zero backups.
      The migration playbook had no post-migration backup verification step.
      Additionally, the old host had no backup cron — the mechanism was unknown.
    severity: high
    status: fixed
    fixed_in: "LOOP-BACKUP-STABILIZATION-20260208"
    notes: |
      Fixed: /usr/local/bin/vaultwarden-backup.sh on infra-core creates daily
      tar.gz of vw-data dir, rsyncs to NAS. Cron at 02:45 daily. First backup
      verified (1.5MB, synced to NAS successfully). Lesson: service migration
      playbooks MUST include a backup continuity step — verify backup mechanism
      is re-established on the new host before closing the migration.

  - id: GAP-OP-044
    discovered_by: "LOOP-BACKUP-STABILIZATION-20260208"
    discovered_at: "2026-02-08"
    type: duplicate-truth
    doc: "ops/bindings/backup.inventory.yaml"
    description: |
      The backup binding had vm-200-docker-host-offsite enabled=true with a NAS
      path that existed but was always empty. backup.status reported this as
      no_matches, which appeared to be a path misconfiguration. In reality, no
      sync mechanism (rsync, scp, Hyper Backup pull) was ever wired between pve
      and NAS. The enabled=true flag + existing NAS directory created a false
      impression that offsite backup was configured but broken, when it had never
      been set up at all.
    severity: medium
    status: fixed
    fixed_in: "LOOP-BACKUP-STABILIZATION-20260208"
    notes: |
      Fixed: vm-200-docker-host-offsite disabled (186GB exceeds shop upload
      bandwidth). New vm-offsite-critical target covers VMs 204-210 via
      /usr/local/bin/vzdump-offsite-sync.sh on pve (cron 09:00, rsync with
      1500 KB/s bwlimit over Tailscale). Lesson: never enable a backup target
      in the binding without a corresponding sync mechanism. enabled=true must
      mean "this is actively running and verifiable."

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-MEDIA-AGENT-WORKBENCH-20260208
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-045
    discovered_by: "LOOP-MEDIA-AGENT-WORKBENCH-20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "ops/agents/ (expected directory) + generate-context.sh"
    description: |
      No agent discovery mechanism exists. ops/agents/ directory is absent.
      generate-context.sh injects CLI tools but not domain-specific agents.
      agents_verify.sh exists but is not wired into spine.verify. No routing
      rules map problem domains (media, infra, finance, etc.) to specialized
      agents. A fresh Claude Code session asking about a media issue has no
      path to discover the media agent in workbench/agents/media/.
    severity: high
    status: fixed
    fixed_in: "LOOP-AGENT-DISCOVERY-GOVERNANCE-20260208"
    notes: |
      Fixed: (1) ops/bindings/agents.registry.yaml created with routing rules,
      (2) generate-context.sh injects "Available Agents" section,
      (3) D49 agent discovery lock wired into drift-gate.sh,
      (4) agents_verify.sh rewritten for contract model,
      (5) AGENTS_GOVERNANCE.md updated. media-agent registered as first entry.

  - id: GAP-OP-046
    discovered_by: "LOOP-MEDIA-AGENT-WORKBENCH-20260208"
    discovered_at: "2026-02-08"
    type: duplicate-truth
    doc: "AGENTS_LOCATION.md, CORE_AGENTIC_SCOPE.md, WORKBENCH_CONTRACT.md, REPO_STRUCTURE_AUTHORITY.md"
    description: |
      Agent implementation path governance is contradictory across 4 docs.
      AGENTS_LOCATION.md says "agentic-spine/agents/" (directory doesn't exist).
      CORE_AGENTIC_SCOPE.md references agents/active/ and agents/contracts/
      (neither exists; actual contracts are in ops/agents/). Workbench .gitignore
      blocks agents/ as "Runtime (spine owns these)". REPO_STRUCTURE_AUTHORITY.md
      doesn't list agents/ as an allowed workbench root. WORKBENCH_CONTRACT.md
      is silent on agents. All docs predate D49 agent discovery architecture
      (2026-02-08). Result: P1 media-agent artifacts in workbench/agents/media/
      are untrackable by git.
    severity: high
    status: fixed
    fixed_in: "LOOP-MEDIA-AGENT-WORKBENCH-20260208"
    notes: |
      Fixed: (1) AGENTS_LOCATION.md updated to reflect ops/agents/ for contracts,
      workbench/agents/<domain>/ for implementations. (2) CORE_AGENTIC_SCOPE.md
      updated: agents/active/ → ops/agents/ (contracts), implementation in workbench.
      (3) Workbench .gitignore: added !agents/ exception. (4) REPO_STRUCTURE_AUTHORITY.md:
      added agents/ to allowed root entries. (5) WORKBENCH_CONTRACT.md: added agent
      placement rule.

  # ─────────────────────────────────────────────────────────────
  # Discovered during macbook alignment hardening 2026-02-09
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-047
    discovered_by: "macbook-alignment-20260209"
    discovered_at: "2026-02-09"
    type: unclear-doc
    doc: "docs/governance/CLAUDE_ENTRYPOINT_SHIM.md + host.claude.entrypoint.*"
    description: |
      Claude Code home state (~/.claude) contains a mix of governed entrypoint
      surfaces (CLAUDE.md, commands/ctx.md, settings.json, settings.local.json)
      and volatile user-local runtime state (history, cache, transcripts, etc.).
      Without a single "apply + verify" command, agents and operators can drift
      these entrypoint surfaces over time, causing inconsistent governance
      injection across sessions.
    severity: high
    status: fixed
    fixed_in: "macbook-alignment-20260209"
    notes: |
      Fixed by adding:
      - host.macbook.managed_configs.apply (enforces managed symlinks and runs
        host-claude-entrypoint-lock --execute)
      - host.macbook.drift.check (enforced read-only drift check)
      Receipts:
      - RCAP-20260208-202527__host.macbook.managed_configs.apply__R2d9081898
      - RCAP-20260208-202527__host.macbook.drift.check__R4yty81893

  - id: GAP-OP-048
    discovered_by: "macbook-alignment-20260209"
    discovered_at: "2026-02-09"
    type: missing-entry
    doc: "ops/plugins/host/"
    description: |
      No read-only macbook drift gate existed to enforce host invariants beyond
      the start routine. Drift could accumulate silently (managed symlinks,
      CLAUDE entrypoint shim) until an agent session failed.
    severity: medium
    status: fixed
    fixed_in: "macbook-alignment-20260209"
    notes: |
      Fixed: host.macbook.drift.check capability added (receipt:
      RCAP-20260208-202527__host.macbook.drift.check__R4yty81893).

  - id: GAP-OP-049
    discovered_by: "macbook-alignment-20260209"
    discovered_at: "2026-02-09"
    type: missing-entry
    doc: "workbench/dotfiles/macbook/README.md"
    description: |
      Managed tool configs existed in home (~/.codex/config.toml,
      ~/.config/opencode/opencode.json) but were not governed as reproducible
      dotfiles, so new machines (or drifted machines) would revert to ad-hoc
      defaults and surprise other agents.
    severity: low
    status: fixed
    fixed_in: "macbook-alignment-20260209"
    notes: |
      Fixed by storing these configs in workbench dotfiles and enforcing
      symlinks via host.macbook.managed_configs.apply. Verified by:
      - workbench verifier scripts/root/verify_laptop_hotkeys.sh
      - host.macbook.drift.check capability

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-DEV-TOOLS-GITEA-STANDARDIZATION-20260209
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-050
    discovered_by: "LOOP-DEV-TOOLS-GITEA-STANDARDIZATION-20260209"
    discovered_at: "2026-02-09"
    type: missing-entry
    doc: "ops/staged/dev-tools/"
    description: |
      No automated backup cron exists for Gitea app-level dumps (gitea dump +
      pg_dump). Only VM-level vzdump protects data. A Gitea-specific corruption
      or misconfiguration would require full VM restore instead of targeted
      app-level recovery. Vaultwarden and Infisical both have automated
      app-level backups; Gitea does not.
    severity: high
    status: fixed
    fixed_in: "LOOP-DEV-TOOLS-GITEA-STANDARDIZATION-20260209"
    notes: |
      Fixed: gitea-backup.sh deployed to /usr/local/bin on dev-tools (VM 206).
      Cron installed: 55 2 * * * (daily). SSH key for NAS access generated and
      authorized. Manual test verified: gitea dump (3.8M) + pg_dump (80K) ->
      NAS /volume1/backups/apps/gitea/. Retention: 7 daily.

  - id: GAP-OP-051
    discovered_by: "LOOP-DEV-TOOLS-GITEA-STANDARDIZATION-20260209"
    discovered_at: "2026-02-09"
    type: missing-entry
    doc: "ops/bindings/backup.inventory.yaml"
    description: |
      No app-gitea entry existed in backup.inventory.yaml. backup.status
      has no visibility into Gitea app-level backup freshness. Other app
      backups (app-infisical, app-vaultwarden) are tracked; Gitea is not.
    severity: high
    status: fixed
    fixed_in: "LOOP-DEV-TOOLS-GITEA-STANDARDIZATION-20260209"
    notes: |
      Fixed: app-gitea entry added to backup.inventory.yaml with
      stale_after_hours=26, classification=critical, NAS path
      /volume1/backups/apps/gitea. Will show STALE until P5 deploys cron.

  - id: GAP-OP-052
    discovered_by: "LOOP-DEV-TOOLS-GITEA-STANDARDIZATION-20260209"
    discovered_at: "2026-02-09"
    type: runtime-bug
    doc: "Gitea push mirror configuration"
    description: |
      Push mirror from Gitea to GitHub uses an OAuth token (gho_*) that
      has an expiration date. When it expires, mirror sync will silently
      fail. No monitoring or alerting exists for mirror sync failures.
      A classic PAT (non-expiring or long-lived) is more durable.
    severity: medium
    status: fixed
    fixed_in: "LOOP-DEV-TOOLS-GITEA-STANDARDIZATION-20260209"
    notes: |
      Fixed: GitHub classic PAT stored in Infisical at
      /spine/vm-infra/gitea/GITHUB_PUSH_MIRROR_TOKEN. Push mirrors for
      both agentic-spine and workbench deleted+recreated with durable PAT.
      Sync verified (last_error empty, fresh timestamps). Namespace policy
      updated with GITHUB_PUSH_MIRROR_TOKEN in required_key_paths.

  - id: GAP-OP-053
    discovered_by: "LOOP-DEV-TOOLS-GITEA-STANDARDIZATION-20260209"
    discovered_at: "2026-02-09"
    type: missing-entry
    doc: ".gitea/workflows/"
    description: |
      Gitea Actions runner is deployed on VM 206 but no CI workflow
      exists. Runner is idle. No push-triggered verification of spine
      drift gates. GitHub Actions still runs but will become secondary
      once Gitea is canonical origin.
    severity: medium
    status: fixed
    fixed_in: "LOOP-DEV-TOOLS-GITEA-STANDARDIZATION-20260209"
    notes: |
      Fixed: .gitea/workflows/verify.yml created. Triggers on push to
      main and pull_request. Runs surfaces/verify/drift-gate.sh on
      ubuntu-latest runner. D50 drift gate validates workflow exists.

  - id: GAP-OP-054
    discovered_by: "LOOP-DEV-TOOLS-GITEA-STANDARDIZATION-20260209"
    discovered_at: "2026-02-09"
    type: missing-entry
    doc: null
    description: |
      No documented browser test exists for the Authentik SSO flow
      protecting Gitea (git.ronny.works). The SSO integration was
      configured during LOOP-DEV-TOOLS-DEPLOY-20260208 but the login
      flow has not been manually verified end-to-end via browser.
    severity: low
    status: open
    fixed_in: null
    notes: |
      Fix in P6: manual browser test of git.ronny.works -> Authentik
      login -> Gitea. Document result in loop scope closeout.

  - id: GAP-OP-055
    discovered_by: "LOOP-DEV-TOOLS-GITEA-STANDARDIZATION-20260209"
    discovered_at: "2026-02-09"
    type: missing-entry
    doc: "docs/governance/SERVICE_REGISTRY.yaml"
    description: |
      Gitea monitoring is limited to HTTP health probe (services.health.yaml).
      No Prometheus metrics endpoint, no Grafana dashboard, no alerts for
      runner offline, mirror sync stale, disk usage, or API latency.
      Other critical services (Prometheus itself, Loki) have deeper
      observability. Gitea will become the canonical code forge.
    severity: low
    status: open
    fixed_in: null
    notes: |
      Fix deferred to future LOOP-GITEA-OBSERVABILITY. Requires
      enabling Gitea [metrics] in app.ini, adding Prometheus scrape
      target on VM 205, building Grafana dashboard.

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-OBSERVABILITY-HYGIENE-20260208
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-056
    discovered_by: "LOOP-OBSERVABILITY-HYGIENE-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "ops/staged/observability/prometheus/prometheus.yml"
    description: |
      Prometheus scrape targets for loki (localhost:3100) and node-observability
      (localhost:9100) resolve to [::1] (IPv6 loopback) inside the Prometheus
      Docker container. Both services bind to the host network only. Result:
      2 of 7 scrape targets permanently DOWN. Prometheus API shows
      "dial tcp [::1]:3100: connection refused" for both.
    severity: high
    status: fixed
    fixed_in: "LOOP-OBSERVABILITY-HYGIENE-20260208"
    notes: |
      Fixed: replaced localhost with 100.120.163.70 (observability Tailscale IP)
      for both targets in prometheus.yml (staged + live). All targets now UP.

  - id: GAP-OP-057
    discovered_by: "LOOP-OBSERVABILITY-HYGIENE-20260208"
    discovered_at: "2026-02-08"
    type: stale-ssot
    doc: "ops/staged/observability/prometheus/prometheus.yml"
    description: |
      Prometheus scrape config includes infra-core-cadvisor (100.92.91.128:8080)
      and docker-host-cadvisor (100.92.156.118:8080) targets. cAdvisor is not
      deployed on either host. Both targets permanently DOWN with "connection
      refused". These were aspirational targets from the initial config, never
      backed by actual deployments.
    severity: medium
    status: fixed
    fixed_in: "LOOP-OBSERVABILITY-HYGIENE-20260208"
    notes: |
      Fixed: removed both cAdvisor job entries from prometheus.yml (staged + live).
      Can be re-added when/if cAdvisor is deployed in a future loop.

  - id: GAP-OP-058
    discovered_by: "LOOP-OBSERVABILITY-HYGIENE-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "ops/staged/observability/loki/docker-compose.yml"
    description: |
      Loki Docker health check reports container as unhealthy despite the service
      responding 200 on /ready from the host. Two compounding root causes:
      (1) grafana/loki:latest is a distroless image with no shell, wget, or nc —
      the wget-based health check can never succeed regardless of timing.
      (2) Loki's compactor waits 10 minutes for ring stability before /ready
      returns 200, so even a working health check would need >10min start_period.
    severity: low
    status: fixed
    fixed_in: "LOOP-OBSERVABILITY-HYGIENE-20260208"
    notes: |
      Fixed: disabled in-container health check (test: ["NONE"]) since distroless
      image has no health check tools. Health monitored externally via
      services.health.yaml (GET /ready on 100.120.163.70:3100).

  - id: GAP-OP-059
    discovered_by: "LOOP-OBSERVABILITY-HYGIENE-20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "ops/bindings/secrets.binding.yaml"
    description: |
      Grafana admin password secret path is undocumented. Other infra services
      (caddy-auth, gitea, vaultwarden) have documented Infisical paths under
      /spine/vm-infra/<service>/. Grafana follows the same pattern but is not
      explicitly documented in any binding or runbook.
    severity: low
    status: open
    fixed_in: null
    notes: |
      Expected path: /spine/vm-infra/grafana/GRAFANA_ADMIN_PASSWORD.
      Deferred: requires creating the Infisical folder + secret. Non-blocking
      since Grafana is currently using the default admin password set during
      initial deployment.

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-VAULTWARDEN-GOVERNANCE-20260209
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-060
    discovered_by: "LOOP-VAULTWARDEN-GOVERNANCE-20260209"
    discovered_at: "2026-02-09"
    type: missing-entry
    doc: "ops/bindings/secrets.namespace.policy.yaml"
    description: |
      Vaultwarden ADMIN_TOKEN is not stored in Infisical. It exists only in
      the host-local .env on infra-core. Every other infra-core service
      (Gitea, Caddy-Auth, Infisical itself) has secrets at
      /spine/vm-infra/<service>/. Vaultwarden is the sole exception.
    severity: high
    status: fixed
    fixed_in: "LOOP-VAULTWARDEN-GOVERNANCE-20260209"
    notes: |
      Fixed: VAULTWARDEN_ADMIN_TOKEN stored at /spine/vm-infra/vaultwarden/
      in Infisical. Folder created via API. Verified: key, path, value length.
      Non-secret config (DOMAIN, LOG_LEVEL, TZ, etc.) stays in .env.

  - id: GAP-OP-061
    discovered_by: "LOOP-VAULTWARDEN-GOVERNANCE-20260209"
    discovered_at: "2026-02-09"
    type: missing-entry
    doc: "ops/bindings/secrets.namespace.policy.yaml"
    description: |
      secrets.namespace.policy.yaml has no vaultwarden entries in
      required_key_paths, planned_key_paths, or forbidden_root_prefixes.
      D43 cannot enforce vaultwarden secret placement.
    severity: medium
    status: fixed
    fixed_in: "LOOP-VAULTWARDEN-GOVERNANCE-20260209"
    notes: |
      Fixed: VAULTWARDEN_ADMIN_TOKEN added to required_key_paths at
      /spine/vm-infra/vaultwarden/. VAULTWARDEN_ added to
      forbidden_root_prefixes. D43 PASS confirmed.

  - id: GAP-OP-062
    discovered_by: "LOOP-VAULTWARDEN-GOVERNANCE-20260209"
    discovered_at: "2026-02-09"
    type: missing-entry
    doc: "docs/governance/"
    description: |
      No restore runbook exists for Vaultwarden. GITEA_BACKUP_RESTORE.md,
      INFISICAL_BACKUP_RESTORE.md, and AUTHENTIK_BACKUP_RESTORE.md all exist.
      Vaultwarden is Tier 1 Critical Infrastructure with no documented
      restore procedure.
    severity: medium
    status: fixed
    fixed_in: "LOOP-VAULTWARDEN-GOVERNANCE-20260209"
    notes: |
      Fixed: created docs/governance/VAULTWARDEN_BACKUP_RESTORE.md following
      the Infisical pattern. Covers: backup (tar.gz + NAS rsync), restore
      (stop, replace vw-data, start), secrets recovery (from Infisical),
      break-glass access, quarterly restore test requirement.

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-CADDY-PROTO-FIX-20260209
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-063
    discovered_by: "LOOP-CADDY-PROTO-FIX-20260209"
    discovered_at: "2026-02-09"
    type: missing-process
    doc: "ops/bindings/deploy.dependencies.yaml"
    description: |
      No governance artifact tracks deployment dependencies between services.
      When Caddy config changed (X-Forwarded-Proto fix), Authentik OIDC
      discovery URLs switched from http:// to https://, but Gitea cached
      the old http:// issuer. SSO failed with JWT issuer mismatch until
      Gitea was restarted. No gate or checklist existed to enforce the
      downstream restart.
    severity: high
    status: fixed
    fixed_in: "LOOP-CADDY-PROTO-FIX-20260209"
    notes: |
      Fixed: created deploy.dependencies.yaml (dependency chain map),
      D51 caddy-proto-lock (validates X-Forwarded-Proto https on all
      Authentik proxy blocks in staged Caddyfile). Gate prevents the
      proto header from being removed; binding documents restart chain.

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-UDR6-SHOP-CUTOVER-20260209
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-064
    discovered_by: "LOOP-UDR6-SHOP-CUTOVER-20260209"
    discovered_at: "2026-02-09"
    type: missing-process
    doc: "docs/governance/SHOP_SERVER_SSOT.md"
    description: |
      Shop LAN has no dedicated gateway/router. T-Mobile 5G Home Internet
      gateway is fully locked (no DHCP/DNS control, no bridge mode). DHCP
      and DNS are unmanaged by the homelab operator. Pi-hole exists but
      DHCP clients don't use it. VLANs impossible without a VLAN-capable
      router. All SSOT docs reference 192.168.12.0/24 (T-Mobile subnet).
    severity: high
    status: fixed
    fixed_in: "LOOP-UDR6-SHOP-CUTOVER-20260209"
    notes: |
      Fixed 2026-02-09: UDR6 deployed as shop gateway (192.168.1.1).
      All VMs re-IPed to VMID-based 192.168.1.0/24. Pi-hole DNS active.
      CF tunnel, NFS, all Docker stacks verified live.
      Device re-IP (iDRAC, NVR, switch) deferred — BMC initializing.
      Receipt: ADHOC_20260209_160037_UDR6_CUTOVER_P2.

  # ─────────────────────────────────────────────────────────────
  # Discovered during change pack governance post-mortem 2026-02-09
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-065
    discovered_by: "LOOP-UDR6-SHOP-CUTOVER-20260209"
    discovered_at: "2026-02-09"
    type: missing-process
    doc: "docs/governance/CHANGE_PACK_TEMPLATE.md"
    description: |
      UDR6 cutover executed without a governed preflight process, no filled
      change pack, no LAN-only device bindings in ssh.targets.yaml, and no
      cutover sequencing rules. Mid-cutover discoveries (docker-host re-IP,
      infra-core DNS, grafana tunnel routing) required improvisation.
    severity: high
    status: fixed
    fixed_in: "change-pack-governance-20260209"
    notes: |
      Fixed by: CHANGE_PACK_TEMPLATE.md (enhanced with rollback map, cutover
      sequence, deferred items, doc sweep, sign-off), cutover.sequencing.yaml
      (5-phase mandatory ordering), LAN-only device entries in ssh.targets.yaml
      (switch-shop, idrac-shop, nvr-shop), network.lan.device.status capability
      (ping via probe_via), network.cutover.preflight capability (composite GO/NO-GO),
      D53 change-pack-integrity-lock drift gate, retrospective UDR6 change pack,
      NETWORK_RUNBOOK.md Section 9 (change pack process).

  # ─────────────────────────────────────────────────────────────
  # Discovered during drift gates consolidation proposal 2026-02-10
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-066
    discovered_by: "LOOP-DRIFT-GATES-CONSOLIDATION-20260210"
    discovered_at: "2026-02-10"
    type: missing-process
    doc: "surfaces/verify/drift-gate.sh"
    description: |
      Drift-gate suite has grown to 50+ checks without an explicit taxonomy or
      consolidation strategy. Agents see this as sprawl: multiple STOPs for the
      same remediation path (notably secrets readiness) and unclear separation
      between irreversible safety locks vs operator guidance checks.
    severity: medium
    status: open
    fixed_in: null
    notes: |
      Proposed fix (audit-only): docs/governance/_audits/2026-02-10-drift-gates-consolidation-proposal.md
      Follow-up: introduce composite locks (secrets readiness, agent entry surface,
      infra identity cohesion) and a DRIFT_VERBOSE mode to keep subchecks available.

  # ─────────────────────────────────────────────────────────────
  # Discovered during mailroom audit 2026-02-10
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-067
    discovered_by: "mailroom-audit-20260210"
    discovered_at: "2026-02-10"
    type: missing-process
    doc: "mailroom/outbox/CLAUDE__RESULT.md"
    description: |
      Certification audit produced 31 findings (5 critical, 12 moderate, 14 minor)
      to CLAUDE__RESULT.md but no triage pipeline exists to convert findings into
      gaps or loops. Audit results sit in outbox with no follow-up workflow.
      21 findings went untracked until manual mailroom audit discovered them.
    severity: high
    status: fixed
    fixed_in: "fix/gap-067-audit-triage"
    notes: |
      Fixed: spine.audit.triage capability reads *RESULT.md files from outbox,
      extracts findings by severity, cross-references against operational.gaps.yaml
      and open_loops.jsonl, and produces a TRACKED/UNTRACKED/STALE triage report.

  - id: GAP-OP-068
    discovered_by: "mailroom-audit-20260210"
    discovered_at: "2026-02-10"
    type: missing-process
    doc: "docs/governance/SSOT_REGISTRY.yaml"
    description: |
      SSOT documents have last_reviewed fields but no drift gate checks temporal
      freshness. A file can be structurally valid but weeks stale. 11 of 25
      SSOT_REGISTRY entries had last_reviewed dates 2-3 weeks old. No automated
      signal for staleness.
    severity: medium
    status: fixed
    fixed_in: "fix/gap-068-ssot-freshness"
    notes: |
      Fixed: D58 drift gate added. Checks all SSOT last_reviewed dates against
      SSOT_FRESHNESS_DAYS threshold (default 21). Gate script at
      surfaces/verify/d58-ssot-freshness-lock.sh.

  - id: GAP-OP-069
    discovered_by: "mailroom-audit-20260210"
    discovered_at: "2026-02-10"
    type: missing-process
    doc: "surfaces/verify/drift-gate.sh"
    description: |
      D45 naming consistency checks that hosts in naming.policy.yaml appear in
      their declared bindings. But no reverse check exists — a host can exist in
      ssh.targets.yaml and be absent from SERVICE_REGISTRY.yaml or
      DEVICE_IDENTITY_SSOT.md. Each registry is validated in isolation.
      Missing automation-stack host in SERVICE_REGISTRY (C5) was one result.
    severity: medium
    status: fixed
    fixed_in: "fix/gap-069-registry-completeness"
    notes: |
      Fixed: D59 cross-registry completeness lock added. Bidirectional checks:
      (1) naming.policy ssh_target=true hosts must appear in ssh.targets.yaml,
      (2) shop-site SSH targets (non-lan-only, non-proxmox, non-decommissioned)
      must appear in SERVICE_REGISTRY.yaml hosts section.

  - id: GAP-OP-070
    discovered_by: "mailroom-audit-20260210"
    discovered_at: "2026-02-10"
    type: missing-process
    doc: null
    description: |
      When entities are deprecated (tools, docs, paths, services), no enforcement
      ensures stale references are cleaned up. Examples: context.md referenced
      deprecated `mint ask`, AGENT_BOUNDARIES.md referenced superseded
      INFRASTRUCTURE_AUTHORITY.md. Nothing greps governance docs for known-
      deprecated strings.
    severity: low
    status: fixed
    fixed_in: "fix/gap-070-deprecation-sweeper"
    notes: |
      Proposed fix: create ops/bindings/deprecated.terms.yaml listing deprecated
      strings + their replacements. Add a drift gate that greps governance docs
      for any listed deprecated term.

  - id: GAP-OP-071
    discovered_by: "mailroom-audit-20260210"
    discovered_at: "2026-02-10"
    type: missing-process
    doc: "ops/bindings/naming.policy.yaml"
    description: |
      naming.policy.yaml documents a 9-step rename_procedure but it is
      documentation only — not enforced. When hosts are added, removed, or
      renamed, operators must manually remember every file to update. The VM 201
      decommission required updating 6 files; no automated check ensured all
      were hit until spine.verify caught D45 failure.
    severity: medium
    status: open
    fixed_in: null
    notes: |
      Proposed fix: add a spine.ripple.check capability that, given a host
      change, lists every file needing updates based on the naming.policy
      surfaces configuration.

# ─────────────────────────────────────────────────────────────
# Gap Types
# ─────────────────────────────────────────────────────────────
#
# stale-ssot      - Doc has outdated information
# missing-entry   - Expected entry doesn't exist
# agent-behavior  - Agent pattern that caused friction (not a doc issue)
# unclear-doc     - Doc exists but is ambiguous or incomplete
# duplicate-truth - Multiple docs claim authority for same thing
# runtime-bug     - Runtime implementation behavior violates governance intent
