# Operational Gaps - Runtime SSOT Discoveries
#
# These are NOT extraction gaps (see docs/core/AGENTIC_GAP_MAP.md)
# These are issues agents discover DURING work that are out of scope to fix immediately
#
# Review periodically, create cleanup loops for open gaps
#
# Status: authoritative
# Last verified: 2026-02-07

version: 1
updated: "2026-02-08T18:30Z"

gaps:
  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-INFRA-VM-RESTRUCTURE-20260206
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-001
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: stale-ssot
    doc: "ops/bindings/docker.compose.targets.yaml"
    description: |
      Still lists cloudflared, pihole, infisical (secrets) under docker-host
      after Phase 1 migration to infra-core. Services are now on VM 204.
    severity: low
    status: fixed
    fixed_in: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    notes: "Moved cloudflared/pihole/secrets to infra-core target, added vaultwarden"

  - id: GAP-OP-002
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "docs/governance/SERVICE_REGISTRY.yaml"
    description: |
      No infisical entry existed before migration. Agent had to discover
      the stack by inspecting Docker containers on docker-host.
    severity: medium
    status: fixed
    fixed_in: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    notes: "Entry added during migration work"

  - id: GAP-OP-003
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: agent-behavior
    doc: null
    description: |
      Agent guessed path ~/stacks/infisical/ instead of consulting
      docker.compose.targets.yaml binding first. SSOT had the correct
      path (~/stacks/secrets). This is a pattern issue, not a data issue.
    severity: low
    status: fixed
    fixed_in: "gap-sweep-20260208"
    notes: "Documented 'consult bindings first' in SESSION_PROTOCOL.md (Trace truth) and docs/brain/rules.md (Entry Points table)"

  - id: GAP-OP-004
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: duplicate-truth
    doc: "ops/bindings/infra.placement.policy.yaml"
    description: |
      Canonical infra-core/observability services were intended for shop Proxmox
      (pve) but relocation commands relied on manifest values without a
      canonical site/hypervisor placement lock. This allowed wrong-instance
      execution risk when target metadata drifted.
    severity: high
    status: fixed
    fixed_in: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    notes: "Added infra placement policy SSOT + D37 lock + command-time enforcement in relocation/provision scripts"

  - id: GAP-OP-005
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: stale-ssot
    doc: "ops/bindings/ssh.targets.yaml"
    description: |
      Hypervisor SSH target IDs are distinct (`pve`, `proxmox-home`) but runtime
      host identity is ambiguous: `proxmox-home` currently reports hostname `pve`.
      This creates decision-risk during relocation/provisioning because agents and
      operators can confuse shop/home hypervisors.
    severity: high
    status: fixed
    fixed_in: "LOOP-GOV-SPINE-SEAL-20260207"
    notes: |
      Guardrail added: infra.hypervisor.identity capability + mutating infra
      preconditions now fail on hostname/machine-id ambiguity. Host-level fix
      applied: proxmox-home canonical hostname updated from `pve` to
      `proxmox-home` via capability `infra.hypervisor.hostname.set`
      (receipt: RCAP-20260207-113039__infra.hypervisor.hostname.set__Rxrbe80548).

  - id: GAP-OP-006
    discovered_by: "LOOP-GOV-SPINE-SEAL-20260207"
    discovered_at: "2026-02-07"
    type: runtime-bug
    doc: "ops/plugins/infra/bin/infra-relocation-preflight"
    description: |
      Preflight host scan silently skipped service source hosts because it used
      an invalid yq expression (`.services[].from_host // empty`) and suppressed
      parser errors. The command also reported missing SSH targets/unreachable
      hosts as informational output instead of a failing gate, allowing
      decision flow to continue without required host evidence.
    severity: high
    status: fixed
    fixed_in: "LOOP-GOV-SPINE-SEAL-20260207"
    notes: |
      Fixed by correcting host extraction, deriving required hosts from active
      VM/service states, and enforcing hard failure on required host/SSOT
      mismatches.

  - id: GAP-OP-007
    discovered_by: "LOOP-GOV-SPINE-SEAL-20260207"
    discovered_at: "2026-02-07"
    type: runtime-bug
    doc: "ops/plugins/infra/bin/infra-relocation-service-transition"
    description: |
      Service and relocation state transitions allowed out-of-order decision
      updates (cutover/migrated) with warning-only behavior. This permitted
      metadata progression even when relocation state had not reached cutover.
    severity: medium
    status: fixed
    fixed_in: "LOOP-GOV-SPINE-SEAL-20260207"
    notes: |
      Transition scripts now hard-fail when relocation state and service/vm
      status are misaligned, and cutover state transition includes an execute-time
      vm readiness gate.

  - id: GAP-OP-008
    discovered_by: "LOOP-GOV-SPINE-SEAL-20260207"
    discovered_at: "2026-02-07"
    type: runtime-bug
    doc: "surfaces/verify/drift-gate.sh"
    description: |
      Top-level `spine.verify` can pass while `infra.hypervisor.identity` fails.
      During active relocation this produces a green global health signal even
      when hypervisor identity ambiguity remains unresolved.
    severity: high
    status: fixed
    fixed_in: "LOOP-GOV-SPINE-SEAL-20260207"
    notes: |
      Added D39 gate (`surfaces/verify/d39-infra-hypervisor-identity-lock.sh`)
      and wired it into `drift-gate.sh`. During active relocation states,
      `spine.verify` now fails if hypervisor identity invariants are violated.

  # ─────────────────────────────────────────────────────────────
  # Discovered during soak-window parallel execution 2026-02-07
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-009
    discovered_by: "soak-window-parallel-execution-20260207"
    discovered_at: "2026-02-07"
    type: stale-ssot
    doc: "surfaces/verify/contracts-gate.sh"
    description: |
      contracts-gate.sh (T5) referenced contracts at docs/ (e.g. docs/RECEIPTS_CONTRACT.md)
      but canonical files live at docs/core/ (e.g. docs/core/RECEIPTS_CONTRACT.md).
      Path mismatch caused ops verify to fail at foundation gate. The files exist;
      the gate paths were stale.
    severity: high
    status: fixed
    fixed_in: "soak-window-parallel-execution-20260207"
    notes: "Fixed contracts-gate.sh paths: docs/ → docs/core/ for all three contracts"

  - id: GAP-OP-010
    discovered_by: "soak-window-parallel-execution-20260207"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "ops/bindings/ssh.targets.yaml"
    description: |
      media-stack host has docker.compose.targets.yaml binding (ssh_target: media-stack)
      but no corresponding entry in ssh.targets.yaml. Prevents governed SSH access
      for media stack RCA work.
    severity: medium
    status: fixed
    fixed_in: "soak-window-parallel-execution-20260207"
    notes: "Added media-stack target (100.117.1.53, root, shop/pve) to ssh.targets.yaml"

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-HOST-CANONICALIZATION-20260207
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-011
    discovered_by: "LOOP-HOST-CANONICALIZATION-20260207"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "surfaces/verify/drift-gate.sh"
    description: |
      Hidden-root coverage missing from active drift gates. host.drift.audit
      passes while unmanaged hidden roots and a secret-bearing legacy file
      (~/.config/ronny-ops/env.sh) exist at home root. No gate enforces
      hidden-root inventory or forbidden pattern scanning.
    severity: high
    status: fixed
    fixed_in: "LOOP-HOST-CANONICALIZATION-20260207"
    notes: "D41 hidden-root governance lock + D42 code path case lock + env.sh deleted + backups quarantined"

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-INFRA-CADDY-AUTH-20260207 pre-work
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-012
    discovered_by: "LOOP-INFRA-CADDY-AUTH-20260207"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "Infisical project: infrastructure (prod)"
    description: |
      Required Authentik bootstrap secrets are not present in Infisical
      infrastructure/prod: AUTHENTIK_SECRET_KEY and AUTHENTIK_DB_PASSWORD.
      This blocks safe execute of staged caddy-auth stack deployment.
    severity: high
    status: fixed
    fixed_in: "LOOP-INFRA-CADDY-AUTH-20260207"
    notes: "Created in infrastructure/prod at /spine/vm-infra/caddy-auth (no root-path duplicates)."

  - id: GAP-OP-013
    discovered_by: "LOOP-INFRA-CADDY-AUTH-20260207"
    discovered_at: "2026-02-07"
    type: duplicate-truth
    doc: "Infisical project: infrastructure (prod)"
    description: |
      Infrastructure project still stores legacy keys at root path `/` (flat
      namespace). This risks future collisions with new VM-infra secrets unless
      all new infra keys are constrained under `/spine/*` namespaces.
    severity: medium
    status: fixed
    fixed_in: "secrets-namespace-migration-20260207"
    notes: |
      Enforcement added: secrets.namespace.status + secrets.namespace.policy freeze.
      Migration map staged at ops/staged/SECRETS_NAMESPACE_MIGRATION_MAP.md.
      P1 complete for /spine/platform/security (9 root duplicates removed) and
      P2 complete for /spine/network/edge (10 root duplicates removed) and
      P3 complete for /spine/storage/nas (6 root duplicates removed) and
      P4 complete for /spine/integrations/commerce-mail (9 root duplicates removed) and
      P5 complete for /spine/services/* (11 root duplicates removed) and
      P6 complete for /spine/ai/providers (4 root duplicates removed).
      Root key count reduced from baseline 49 -> 0 (49 removed).
      Evidence: ops/staged/SECRETS_NAMESPACE_P1_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P2_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P3_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P4_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P5_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P6_EXECUTION_20260207.md,
      RCAP-20260207-171547__secrets.p2.root_cleanup.execute__Rchmg11028,
      RCAP-20260207-180752__secrets.namespace.status__R4k6j64294.

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-GOV-CLI-TOOL-DISCOVERY-20260207
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-014
    discovered_by: "LOOP-GOV-CLI-TOOL-DISCOVERY-20260207"
    discovered_at: "2026-02-07"
    type: agent-behavior
    doc: null
    description: |
      Agent could not discover qrencode despite it being registered in
      maker.tools.inventory.yaml. The tool is domain-scoped (maker plugin)
      with no cross-domain discovery path. Session protocol, brain context,
      and rules all lack tool discovery guidance. Agent searched workbench
      repo, MCP catalogs, and file contents but never found the binding.
    severity: medium
    status: fixed
    fixed_in: "LOOP-GOV-CLI-TOOL-DISCOVERY-20260207"
    notes: |
      Fixed by: cli.tools.inventory.yaml (cross-domain catalog),
      generate-context.sh (tool injection into agent context),
      D44 drift gate (discovery chain validation),
      SESSION_PROTOCOL.md + rules.md updates (agent guidance).

  # ─────────────────────────────────────────────────────────────
  # Discovered during OL_HOME_BASELINE_FINISH audit 2026-02-07
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-015
    discovered_by: "OL_HOME_BASELINE_FINISH"
    discovered_at: "2026-02-07"
    type: runtime-bug
    doc: "docs/governance/MINILAB_SSOT.md"
    description: |
      PVE node-name mismatch on proxmox-home: hostname changed from `pve` to
      `proxmox-home` (GAP-OP-005 fix) but PVE node was not migrated. All VM/LXC
      configs live under /etc/pve/nodes/pve/ while PVE tools look under
      /etc/pve/nodes/proxmox-home/. Breaks qm list, pct list, pct exec, vzdump
      jobs. VMs running pre-rename continue but are unmanageable. VM 101 (immich)
      is stopped and cannot be restarted.
    severity: critical
    status: fixed
    fixed_in: "gap-sweep-20260208"
    notes: "Fixed: hostname reverted to pve (matching PVE node name). qm list, pct list, vzdump all functional. Tailscale hostname remains proxmox-home (independent). Exception recorded in naming.policy.yaml."

  - id: GAP-OP-017
    discovered_by: "LOOP-GOV-CLAUDE-ENTRYPOINT-LOCK-20260208"
    discovered_at: "2026-02-08"
    type: duplicate-truth
    doc: "~/.claude/CLAUDE.md"
    description: |
      Claude home config (CLAUDE.md, commands/ctx.md, settings.json, settings.local.json)
      contains standalone governance sections (Authority Order, Immutable Invariants, etc.)
      and uppercase ~/Code/ path references. No Claude-equivalent of D32 (Codex instruction
      source lock) exists. Runtime scripts reference .brain/ paths but no .brain/ directory
      exists — actual files live at docs/brain/. SESSION_PROTOCOL.md also references .brain/.
    severity: medium
    status: fixed
    fixed_in: "LOOP-GOV-CLAUDE-ENTRYPOINT-LOCK-20260208"
    notes: |
      Fixed by: CLAUDE.md rewritten to redirect shim, path case canonicalized across
      all governed Claude files, .brain/ references replaced with docs/brain/ in runtime
      scripts and SESSION_PROTOCOL.md, D46 + D47 drift gates added, agent.entrypoint.lock.yaml
      binding created, host-claude-entrypoint-lock plugin with status/enforce/execute modes.

  # ─────────────────────────────────────────────────────────────
  # Discovered during PVE baseline SSH audit 2026-02-07
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-018
    discovered_by: "OL_SHOP_BASELINE_FINISH"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "/etc/pve/jobs.cfg"
    description: |
      VM 204 (infra-core) is not included in the vzdump backup job on pve.
      The job covers VMs 200-203 but was created before VM 204 existed.
      infra-core runs cloudflared, pihole, infisical, vaultwarden, caddy,
      and authentik — all critical services with no hypervisor-level backup.
    severity: high
    status: fixed
    fixed_in: "LOOP-INFRA-SSOT-ALIGNMENT-20260208"
    notes: |
      Fixed: added vmid 204 to vzdump job in /etc/pve/jobs.cfg via SSH.
      Job now covers 200,201,202,203,204. Add VM 205 when provisioned.

  - id: GAP-OP-019
    discovered_by: "OL_SHOP_BASELINE_FINISH"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "pve crontab"
    description: |
      No ZFS scrub scheduled for the media pool on pve. Only the tank pool
      has a weekly scrub (`0 3 * * 0 zpool scrub tank`). The media pool's
      last scrub was CANCELED on 2026-01-11. Media pool is RAIDZ1 with 4x
      Seagate ST8000AS0002 (SMR archive drives) — higher risk profile than
      tank (RAIDZ2 enterprise SAS).
    severity: medium
    status: fixed
    fixed_in: "LOOP-INFRA-SSOT-ALIGNMENT-20260208"
    notes: |
      Fixed: added `0 4 * * 0 zpool scrub media` to pve crontab via SSH.
      Runs Sundays at 4am, offset from tank scrub (3am).

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-MEDIA-STACK-RCA-20260205 / ARCH-20260208
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-021
    discovered_by: "LOOP-MEDIA-STACK-RCA-20260205"
    discovered_at: "2026-02-07"
    type: agent-behavior
    doc: "ops/staged/MEDIA_RCA_DECISION_NOTE.md"
    description: |
      RCA decision note stated /opt/appdata/*.db files were "stale copies, not
      actively used." In reality these were active symlink targets created Dec 23,
      serving all 5 main databases. An agent following the decision note would have
      assumed the entire database layer was on NFS and may have attempted redundant
      or harmful migrations. The inaccuracy persisted for 2 days before correction.
    severity: medium
    status: fixed
    fixed_in: "LOOP-MEDIA-STACK-ARCH-20260208"
    notes: |
      Root cause: initial RCA SSH session saw files at /opt/appdata/ and assumed
      copies without checking symlink state (ls -la vs ls -l). Fixed: decision note
      corrected. Lesson: always use `ls -la` or `file` command to distinguish
      symlinks from regular files during NFS topology discovery.

  - id: GAP-OP-022
    discovered_by: "LOOP-MEDIA-STACK-ARCH-20260208"
    discovered_at: "2026-02-08"
    type: agent-behavior
    doc: null
    description: |
      Phase A symlink migration broke trailarr and posterizarr because their
      compose services lacked the /opt/appdata:/opt/appdata bind mount that
      radarr/sonarr/lidarr/prowlarr/jellyfin already had (Dec 23 setup).
      The playbook did not include a pre-check for container mount visibility.
      Failure mode: sqlite3.OperationalError with no mention of symlinks,
      making diagnosis non-obvious.
    severity: medium
    status: fixed
    fixed_in: "LOOP-MEDIA-STACK-ARCH-20260208"
    notes: |
      Fixed: added /opt/appdata:/opt/appdata volume to trailarr and posterizarr
      in docker-compose.yml on media-stack, containers recreated via compose up -d.
      Lesson codified in docs/brain/lessons/MEDIA_STACK_LESSONS.md.
      Future mitigation: any symlink migration playbook MUST include a
      pre-flight step: `docker inspect <service> --format '{{range .Mounts}}...'`
      to verify the symlink target path is visible inside the container.

  - id: GAP-OP-023
    discovered_by: "LOOP-MEDIA-STACK-ARCH-20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "ops/staged/MEDIA_STACK_ARCH_PHASE_A_PLAYBOOK_20260207.md"
    description: |
      Phase A playbook listed 5 target services but missed sonarr logs.db (2.2MB,
      constant writes) which was also on NFS. The playbook was written from the
      ARCH scope doc which only listed "databases STILL on NFS" but omitted logs.db
      files for services whose main .db was already symlinked. Discovery chain:
      scope doc partial → playbook incomplete → extra target found during execution.
    severity: low
    status: fixed
    fixed_in: "LOOP-MEDIA-STACK-ARCH-20260208"
    notes: |
      sonarr logs.db migrated during Phase A execution. Lesson: when scoping
      NFS-to-local migrations, enumerate ALL .db files per service (not just the
      main database). Use: find /mnt/docker/volumes/<service> -name '*.db' -type f

  - id: GAP-OP-024
    discovered_by: "LOOP-MEDIA-STACK-ARCH-20260208"
    discovered_at: "2026-02-08"
    type: agent-behavior
    doc: "ops/staged/MEDIA_STACK_ARCH_PHASE_A_PLAYBOOK_20260207.md"
    description: |
      Phase A playbook listed introskipper.db path as
      /mnt/docker/volumes/jellyfin/config/data/introskipper.db but actual path was
      /mnt/docker/volumes/jellyfin/config/data/introskipper/introskipper.db (extra
      directory level). Path was assumed from scope doc without SSH verification.
      Similarly, trailarr had a logs/logs.db not mentioned in the playbook.
    severity: low
    status: fixed
    fixed_in: "LOOP-MEDIA-STACK-ARCH-20260208"
    notes: |
      Correct paths discovered via `find` during execution. Lesson: playbooks for
      remote file operations MUST include a preflight `find` or `ls -la` step to
      confirm actual paths before mutation. Never trust doc-assumed paths for
      file-level operations.

  # ─────────────────────────────────────────────────────────────
  # Discovered during pre-vm-creation reliability hardening 2026-02-08
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-025
    discovered_by: "pre-vm-creation-reliability-hardening-20260208"
    discovered_at: "2026-02-08"
    type: stale-ssot
    doc: "ops/bindings/ssh.targets.yaml"
    description: |
      media-stack SSH target had the wrong user (root vs actual), causing
      ssh.target.status to fail with auth denied and misleading downstream
      tooling that relies on governed SSH.
    severity: medium
    status: fixed
    fixed_in: "pre-vm-creation-reliability-hardening-20260208"
    notes: |
      Fixed: media-stack entry updated to user=media. ssh.target.status confirms
      OK (192ms). Docker compose status resolves correctly via binding.

  - id: GAP-OP-026
    discovered_by: "pre-vm-creation-reliability-hardening-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "ops/plugins/docker/bin/docker-compose-status"
    description: |
      docker.compose.status assumed `ssh <id>` works via ~/.ssh/config aliases
      and did not resolve ssh_target via ops/bindings/ssh.targets.yaml. This
      produced false UNREACHABLE signals even when the governed SSH binding was
      correct (infra-core, observability).
    severity: high
    status: fixed
    fixed_in: "pre-vm-creation-reliability-hardening-20260208"
    notes: |
      Fixed: docker-compose-status now resolves host/user/port from ssh.targets.yaml
      (lines 170-178), uses binding defaults for StrictHostKeyChecking=no and
      UserKnownHostsFile=/dev/null. SSH error classification added. Verified working
      for infra-core (13 stacks), observability (5 stacks), dev-tools (1 stack).

  - id: GAP-OP-027
    discovered_by: "pre-vm-creation-reliability-hardening-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "ops/plugins/backup/bin/backup-status"
    description: |
      backup.status validated host IDs exist in ssh.targets.yaml but then used
      `ssh \"$HOST\"` (alias) and StrictHostKeyChecking=accept-new (known_hosts
      writes), violating HOME drift constraints and breaking when aliases are not
      present.
    severity: high
    status: fixed
    fixed_in: "pre-vm-creation-reliability-hardening-20260208"
    notes: |
      Fixed: backup-status now resolves host/user/port from ssh.targets.yaml
      (lines 107-118), uses binding defaults for StrictHostKeyChecking and
      UserKnownHostsFile=/dev/null. SSH error classification added. Verified
      working for pve and nas targets.

  - id: GAP-OP-028
    discovered_by: "pre-vm-creation-reliability-hardening-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "ops/bindings/secrets.binding.yaml + ops/plugins/secrets/bin/*"
    description: |
      Secrets capabilities failed when Infisical API URL was behind forward-auth
      (HTTP 302/HTML), causing jq parse errors and broken automation. A binding-
      supported internal API endpoint is required, and scripts must prefer it and
      fail with actionable errors on redirects/non-JSON responses.
    severity: high
    status: fixed
    fixed_in: "pre-vm-creation-reliability-hardening-20260208"
    notes: |
      Fixed: secrets.binding.yaml has internal_api_url (http://100.92.91.128:8088,
      direct Infisical bypassing Authentik). All secrets plugins prefer
      internal_api_url over api_url via yq fallthrough (.internal_api_url // .api_url).
      Verified in secrets-namespace-status, secrets-exec, secrets-projects-status,
      secrets-binding.

  - id: GAP-OP-016
    discovered_by: "OL_HOME_BASELINE_FINISH"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: null
    description: |
      No naming governance policy exists. Hostname, PVE node name, Tailscale
      hostname, ssh.targets.yaml ID, and DEVICE_IDENTITY_SSOT entries are all
      managed independently with no canonical mapping or rename procedure.
      This caused GAP-OP-015 (hostname changed without PVE node migration)
      and has caused repeated confusion (media-stack site attribution, hypervisor
      identity ambiguity).
    severity: high
    status: fixed
    fixed_in: "LOOP-NAMING-GOVERNANCE-20260207"
    notes: |
      Fixed by: naming.policy.yaml (canonical names + surface authority + rename
      procedure + allowed exceptions) + D45 naming consistency lock gate
      (cross-file verification wired into drift-gate.sh). Also fixed vault kind
      misclassification (lxc→vm) in placement policy and DEVICE_IDENTITY_SSOT.

  # ─────────────────────────────────────────────────────────────
  # Discovered during gap sweep 2026-02-08
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-020
    discovered_by: "gap-sweep-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "ops/commands/loops.sh"
    description: |
      ops loops list --open crashes with jq error at open_loops.jsonl:121.
      Close records use "id" key but the parser expects "loop_id", producing
      "Cannot index object with null". Additionally, open_loops.jsonl has
      54 open entries with massive duplication and 3 close records that never
      filter out closed loops. The loops subsystem is the spine's work-tracking
      surface and currently fails on every invocation.
    severity: high
    status: fixed
    fixed_in: "gap-sweep-20260208"
    notes: |
      Fixed: loops.sh jq filter now normalizes close records, deduplicates by loop_id,
      and applies close status. open_loops.jsonl compacted from 121 to 61 lines.
      6 completed loops closed. 9 open loops remain.

  # Discovered during CODE_AUDIT_20260208
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-029
    discovered_by: "CODE_AUDIT_20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "ops/bindings/secrets.namespace.policy.yaml"
    description: |
      secrets.namespace.status failed because the required Gitea secrets were
      missing from Infisical at /spine/vm-infra/gitea. This blocks governed
      automation and leaves the dev-tools deployment partially undocumented.
    severity: high
    status: fixed
    fixed_in: "CODE_AUDIT_20260208"
    notes: |
      Fixed by writing the required keys to Infisical under /spine/vm-infra/gitea:
      GITEA_ADMIN_PASSWORD, GITEA_API_TOKEN, GITEA_RUNNER_TOKEN,
      GITEA_OAUTH_CLIENT_ID, GITEA_OAUTH_CLIENT_SECRET. Verified via
      secrets.namespace.status (receipt RCAP-20260208-105831__secrets.namespace.status__Rwaub7817).

  - id: GAP-OP-030
    discovered_by: "CODE_AUDIT_20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "ops/plugins/cloudflare/bin/cloudflare-tunnel-ingress-status"
    description: |
      There was no spine-native way to export Cloudflare tunnel ingress rules.
      This forced manual dashboard checks and allowed docs-vs-dashboard drift.
    severity: medium
    status: fixed
    fixed_in: "CODE_AUDIT_20260208"
    notes: |
      Added cloudflare.tunnel.ingress.status (API export) and
      cloudflare.domain_routing.diff (diff vs DOMAIN_ROUTING_REGISTRY.yaml).

  - id: GAP-OP-031
    discovered_by: "CODE_AUDIT_20260208"
    discovered_at: "2026-02-08"
    type: stale-ssot
    doc: "docs/governance/DOMAIN_ROUTING_REGISTRY.yaml"
    description: |
      DOMAIN_ROUTING_REGISTRY.yaml drifted from the Cloudflare tunnel ingress
      configuration, creating extra/missing hostname entries and agent confusion
      (docs indicated services/paths that did not exist).
    severity: medium
    status: fixed
    fixed_in: "CODE_AUDIT_20260208"
    notes: |
      Registry updated and verified against live ingress export:
      cloudflare.domain_routing.diff status OK (receipt RCAP-20260208-110109__cloudflare.domain_routing.diff__Rwa0y18221).

  - id: GAP-OP-032
    discovered_by: "CODE_AUDIT_20260208"
    discovered_at: "2026-02-08"
    type: stale-ssot
    doc: "ops/bindings/docker.compose.targets.yaml"
    description: |
      docker.compose.targets.yaml referenced a docker-host storage stack path
      that does not exist, producing false negatives in docker.compose.status
      and confusing agents about the MinIO deployment source.
    severity: medium
    status: fixed
    fixed_in: "CODE_AUDIT_20260208"
    notes: |
      Removed the invalid compose target and aligned compose authority docs to the
      current SSOT model (VM-infra under ops/staged/**; live stack locations via
      docker.compose.targets.yaml).

  - id: GAP-OP-033
    discovered_by: "CODE_AUDIT_20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "ops/commands/cap.sh + surfaces/verify/drift-gate.sh"
    description: |
      ops capability execution was not worktree-safe: receipts and drift gates
      assumed the canonical repo CWD and scanned worktree-only paths, causing
      incorrect failures when operating from isolated worktrees.
    severity: high
    status: fixed
    fixed_in: "CODE_AUDIT_20260208"
    notes: |
      Introduced SPINE_CODE (current code root) distinct from SPINE_REPO
      (canonical receipts/runtime root). Updated capability CWDs to SPINE_CODE and
      hardened drift-gate scripts to ignore .worktrees and to compare canonical
      instruction sources.

  - id: GAP-OP-034
    discovered_by: "CODE_AUDIT_20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "docs/governance/BACKUP_GOVERNANCE.md"
    description: |
      Backup governance defines a requirement for app-level dumps per stack, but
      there is no documented, tested app-level backup/restore procedure for
      Authentik or Gitea (even if VM-level vzdump exists).
    severity: medium
    status: open
    fixed_in: null
    notes: |
      Follow-up: add stack READMEs or runbooks covering export, restore, and a
      periodic restore test for both Authentik and Gitea.

  # ─────────────────────────────────────────────────────────────
  # Discovered during OL_SHOP_BASELINE_FINISH remote audit 2026-02-08
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-037
    discovered_by: "OL_SHOP_BASELINE_FINISH"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "docs/governance/SHOP_SERVER_SSOT.md"
    description: |
      Dell MD1400 DAS shelf is physically cabled to R730XD (pve) via SAS cable
      (Dell 0GYK61) but zero drives are visible to the OS. Two root causes:
      (1) PM8072 SAS controller PCI vendor ID is 0x11f8 (Microchip, post-acquisition)
      but the Linux pm80xx driver only recognizes 0x117C (PMC-Sierra, pre-acquisition) —
      driver does not auto-bind. (2) Hot-loading the driver via new_id fails because
      the PM8072 firmware requires cold-boot initialization — MPI handshake times out
      with "FW is not ready" (chip_init failed, ret=-16/EBUSY).
      Result: entire MD1400 shelf storage is inaccessible. Unknown drive count and
      capacity sitting completely dark.
    severity: critical
    status: open
    fixed_in: null
    notes: |
      Fix requires: (1) persistent modprobe config with install hook to inject PCI ID
      (echo "11f8 8072" > /sys/bus/pci/drivers/pm80xx/new_id), (2) cold boot of pve
      (power off/on, not reboot) for PM8072 firmware initialization. Maintenance window
      needed — all 10 VMs go down. Loop: LOOP-MD1400-SAS-RECOVERY-20260208.
      Upstream: PCI ID gap in Linux pm80xx driver (kernel 6.14.8-2-pve).

  - id: GAP-OP-038
    discovered_by: "OL_SHOP_BASELINE_FINISH"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "/etc/pve/jobs.cfg"
    description: |
      vzdump backup job on pve covers VMs 200-204 only. VMs 205 (observability),
      206 (dev-tools), 207 (ai-consolidation), 209 (download-stack), and
      210 (streaming-stack) are running with no hypervisor-level backup.
      5 VMs provisioned since the backup job was last updated.
    severity: high
    status: fixed
    fixed_in: "LOOP-BACKUP-STABILIZATION-20260208"
    notes: |
      P1 fix: vzdump vmid list updated to 204,205,206,201,202,203,200 (reordered
      so critical infra-core runs first, heaviest VM 200 last). VMs 207/209/210
      deferred to P2 (enable as each stabilizes). Orphaned 34GB partial backup
      from power-outage crash cleaned up.

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-CAMERA-BASELINE-20260208
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-039
    discovered_by: "LOOP-CAMERA-BASELINE-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "docs/governance/CAMERA_SSOT.md"
    description: |
      NVR channels 2, 3, 4 report chanDetectResult=notExist via ISAPI. Cameras
      are powered off or disconnected from the Netgear PoE switch that feeds the
      NVR's internal 192.168.254.0/24 network. Requires physical visit to the
      upstairs 9U rack to inspect PoE switch ports and camera cabling.
    severity: medium
    status: open
    fixed_in: null
    notes: |
      Tracked in LOOP-CAMERA-BASELINE-20260208 Phase P2. Cannot be fixed remotely —
      requires on-site inspection of Netgear PoE switch and camera power/cabling.

  - id: GAP-OP-040
    discovered_by: "LOOP-CAMERA-BASELINE-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "docs/governance/CAMERA_SSOT.md"
    description: |
      NVR channel 5 has IP address 192.168.254.7 which conflicts with channel 4
      (same IP). ISAPI reports chanDetectResult=ipAddrConflict for ch5. This is
      a configuration error in the NVR channel assignment, not a physical issue.
    severity: low
    status: fixed
    fixed_in: "LOOP-CAMERA-BASELINE-20260208"
    notes: |
      Fixed: ch5 IP reassigned from 192.168.254.7 to 192.168.254.5 via ISAPI PUT
      (2026-02-08). NVR accepted change (statusCode=1/OK). Ch5 now shows
      netUnreachable (camera physically disconnected, not IP conflict).
      Previous name "SCREEN PRINT" auto-reset to "IPCamera 05" by NVR.

  - id: GAP-OP-041
    discovered_by: "LOOP-CAMERA-BASELINE-20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "Infisical project: infrastructure (prod)"
    description: |
      Shop infrastructure credentials (NVR, iDRAC, Dell switch) were referenced
      in SHOP_SERVER_SSOT.md and CAMERA_SSOT.md at Infisical paths
      /spine/shop/nvr/*, /spine/shop/idrac/*, /spine/shop/switch/* but all three
      paths were empty. Agent could not diagnose camera IP conflict without
      asking the user for credentials interactively. SSOT docs implied secrets
      existed when they did not.
    severity: medium
    status: fixed
    fixed_in: "LOOP-CAMERA-BASELINE-20260208"
    notes: |
      Fixed: all three credential sets stored in Infisical (6 keys total):
      /spine/shop/nvr (NVR_ADMIN_USER, NVR_ADMIN_PASSWORD),
      /spine/shop/idrac (IDRAC_ADMIN_USER, IDRAC_ADMIN_PASSWORD),
      /spine/shop/switch (SWITCH_ADMIN_USER, SWITCH_ADMIN_PASSWORD).

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-BACKUP-STABILIZATION-20260208
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-042
    discovered_by: "LOOP-BACKUP-STABILIZATION-20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "ops/bindings/backup.inventory.yaml"
    description: |
      No SSH key authentication existed from pve or infra-core to the NAS
      (Synology 918+ at house, Tailscale). Blocked all offsite backup sync.
      The backup binding had an enabled offsite target (vm-200-docker-host-offsite)
      and NAS directories existed, but no sync mechanism or SSH trust was ever
      established. backup.status reported no_matches — a false signal that
      appeared like a path problem when the real issue was no connectivity.
    severity: high
    status: fixed
    fixed_in: "LOOP-BACKUP-STABILIZATION-20260208"
    notes: |
      Fixed: pve RSA key and infra-core ed25519 key added to
      ronadmin@nas authorized_keys. Verified bidirectional SSH from both hosts.
      Offsite rsync script and app backup scripts both use this trust path.

  - id: GAP-OP-043
    discovered_by: "LOOP-BACKUP-STABILIZATION-20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "ops/bindings/backup.inventory.yaml"
    description: |
      Vaultwarden backup stopped when the service migrated from proxmox-home
      (LXC 102) to infra-core (VM 204) on 2026-02-05. No backup cron was
      re-established on the new host. Last NAS backup was 2026-02-05 02:45,
      creating a 3-day gap (Feb 5-8) where vaultwarden had zero backups.
      The migration playbook had no post-migration backup verification step.
      Additionally, the old host had no backup cron — the mechanism was unknown.
    severity: high
    status: fixed
    fixed_in: "LOOP-BACKUP-STABILIZATION-20260208"
    notes: |
      Fixed: /usr/local/bin/vaultwarden-backup.sh on infra-core creates daily
      tar.gz of vw-data dir, rsyncs to NAS. Cron at 02:45 daily. First backup
      verified (1.5MB, synced to NAS successfully). Lesson: service migration
      playbooks MUST include a backup continuity step — verify backup mechanism
      is re-established on the new host before closing the migration.

  - id: GAP-OP-044
    discovered_by: "LOOP-BACKUP-STABILIZATION-20260208"
    discovered_at: "2026-02-08"
    type: duplicate-truth
    doc: "ops/bindings/backup.inventory.yaml"
    description: |
      The backup binding had vm-200-docker-host-offsite enabled=true with a NAS
      path that existed but was always empty. backup.status reported this as
      no_matches, which appeared to be a path misconfiguration. In reality, no
      sync mechanism (rsync, scp, Hyper Backup pull) was ever wired between pve
      and NAS. The enabled=true flag + existing NAS directory created a false
      impression that offsite backup was configured but broken, when it had never
      been set up at all.
    severity: medium
    status: fixed
    fixed_in: "LOOP-BACKUP-STABILIZATION-20260208"
    notes: |
      Fixed: vm-200-docker-host-offsite disabled (186GB exceeds shop upload
      bandwidth). New vm-offsite-critical target covers VMs 204-210 via
      /usr/local/bin/vzdump-offsite-sync.sh on pve (cron 09:00, rsync with
      1500 KB/s bwlimit over Tailscale). Lesson: never enable a backup target
      in the binding without a corresponding sync mechanism. enabled=true must
      mean "this is actively running and verifiable."

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-MEDIA-AGENT-WORKBENCH-20260208
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-045
    discovered_by: "LOOP-MEDIA-AGENT-WORKBENCH-20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "ops/agents/ (expected directory) + generate-context.sh"
    description: |
      No agent discovery mechanism exists. ops/agents/ directory is absent.
      generate-context.sh injects CLI tools but not domain-specific agents.
      agents_verify.sh exists but is not wired into spine.verify. No routing
      rules map problem domains (media, infra, finance, etc.) to specialized
      agents. A fresh Claude Code session asking about a media issue has no
      path to discover the media agent in workbench/agents/media/.
    severity: high
    status: fixed
    fixed_in: "LOOP-AGENT-DISCOVERY-GOVERNANCE-20260208"
    notes: |
      Fixed: (1) ops/bindings/agents.registry.yaml created with routing rules,
      (2) generate-context.sh injects "Available Agents" section,
      (3) D49 agent discovery lock wired into drift-gate.sh,
      (4) agents_verify.sh rewritten for contract model,
      (5) AGENTS_GOVERNANCE.md updated. media-agent registered as first entry.

# ─────────────────────────────────────────────────────────────
# Gap Types
# ─────────────────────────────────────────────────────────────
#
# stale-ssot      - Doc has outdated information
# missing-entry   - Expected entry doesn't exist
# agent-behavior  - Agent pattern that caused friction (not a doc issue)
# unclear-doc     - Doc exists but is ambiguous or incomplete
# duplicate-truth - Multiple docs claim authority for same thing
# runtime-bug     - Runtime implementation behavior violates governance intent
