# Operational Gaps - Runtime SSOT Discoveries
#
# These are NOT extraction gaps (see docs/core/AGENTIC_GAP_MAP.md)
# These are issues agents discover DURING work that are out of scope to fix immediately
#
# Review periodically, create cleanup loops for open gaps
#
# Status: authoritative
# Last verified: 2026-02-07

version: 1
updated: "2026-02-08T03:00Z"

gaps:
  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-INFRA-VM-RESTRUCTURE-20260206
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-001
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: stale-ssot
    doc: "ops/bindings/docker.compose.targets.yaml"
    description: |
      Still lists cloudflared, pihole, infisical (secrets) under docker-host
      after Phase 1 migration to infra-core. Services are now on VM 204.
    severity: low
    status: fixed
    fixed_in: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    notes: "Moved cloudflared/pihole/secrets to infra-core target, added vaultwarden"

  - id: GAP-OP-002
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "docs/governance/SERVICE_REGISTRY.yaml"
    description: |
      No infisical entry existed before migration. Agent had to discover
      the stack by inspecting Docker containers on docker-host.
    severity: medium
    status: fixed
    fixed_in: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    notes: "Entry added during migration work"

  - id: GAP-OP-003
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: agent-behavior
    doc: null
    description: |
      Agent guessed path ~/stacks/infisical/ instead of consulting
      docker.compose.targets.yaml binding first. SSOT had the correct
      path (~/stacks/secrets). This is a pattern issue, not a data issue.
    severity: low
    status: fixed
    fixed_in: "gap-sweep-20260208"
    notes: "Documented 'consult bindings first' in SESSION_PROTOCOL.md (Trace truth) and docs/brain/rules.md (Entry Points table)"

  - id: GAP-OP-004
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: duplicate-truth
    doc: "ops/bindings/infra.placement.policy.yaml"
    description: |
      Canonical infra-core/observability services were intended for shop Proxmox
      (pve) but relocation commands relied on manifest values without a
      canonical site/hypervisor placement lock. This allowed wrong-instance
      execution risk when target metadata drifted.
    severity: high
    status: fixed
    fixed_in: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    notes: "Added infra placement policy SSOT + D37 lock + command-time enforcement in relocation/provision scripts"

  - id: GAP-OP-005
    discovered_by: "LOOP-INFRA-VM-RESTRUCTURE-20260206"
    discovered_at: "2026-02-07"
    type: stale-ssot
    doc: "ops/bindings/ssh.targets.yaml"
    description: |
      Hypervisor SSH target IDs are distinct (`pve`, `proxmox-home`) but runtime
      host identity is ambiguous: `proxmox-home` currently reports hostname `pve`.
      This creates decision-risk during relocation/provisioning because agents and
      operators can confuse shop/home hypervisors.
    severity: high
    status: fixed
    fixed_in: "LOOP-GOV-SPINE-SEAL-20260207"
    notes: |
      Guardrail added: infra.hypervisor.identity capability + mutating infra
      preconditions now fail on hostname/machine-id ambiguity. Host-level fix
      applied: proxmox-home canonical hostname updated from `pve` to
      `proxmox-home` via capability `infra.hypervisor.hostname.set`
      (receipt: RCAP-20260207-113039__infra.hypervisor.hostname.set__Rxrbe80548).

  - id: GAP-OP-006
    discovered_by: "LOOP-GOV-SPINE-SEAL-20260207"
    discovered_at: "2026-02-07"
    type: runtime-bug
    doc: "ops/plugins/infra/bin/infra-relocation-preflight"
    description: |
      Preflight host scan silently skipped service source hosts because it used
      an invalid yq expression (`.services[].from_host // empty`) and suppressed
      parser errors. The command also reported missing SSH targets/unreachable
      hosts as informational output instead of a failing gate, allowing
      decision flow to continue without required host evidence.
    severity: high
    status: fixed
    fixed_in: "LOOP-GOV-SPINE-SEAL-20260207"
    notes: |
      Fixed by correcting host extraction, deriving required hosts from active
      VM/service states, and enforcing hard failure on required host/SSOT
      mismatches.

  - id: GAP-OP-007
    discovered_by: "LOOP-GOV-SPINE-SEAL-20260207"
    discovered_at: "2026-02-07"
    type: runtime-bug
    doc: "ops/plugins/infra/bin/infra-relocation-service-transition"
    description: |
      Service and relocation state transitions allowed out-of-order decision
      updates (cutover/migrated) with warning-only behavior. This permitted
      metadata progression even when relocation state had not reached cutover.
    severity: medium
    status: fixed
    fixed_in: "LOOP-GOV-SPINE-SEAL-20260207"
    notes: |
      Transition scripts now hard-fail when relocation state and service/vm
      status are misaligned, and cutover state transition includes an execute-time
      vm readiness gate.

  - id: GAP-OP-008
    discovered_by: "LOOP-GOV-SPINE-SEAL-20260207"
    discovered_at: "2026-02-07"
    type: runtime-bug
    doc: "surfaces/verify/drift-gate.sh"
    description: |
      Top-level `spine.verify` can pass while `infra.hypervisor.identity` fails.
      During active relocation this produces a green global health signal even
      when hypervisor identity ambiguity remains unresolved.
    severity: high
    status: fixed
    fixed_in: "LOOP-GOV-SPINE-SEAL-20260207"
    notes: |
      Added D39 gate (`surfaces/verify/d39-infra-hypervisor-identity-lock.sh`)
      and wired it into `drift-gate.sh`. During active relocation states,
      `spine.verify` now fails if hypervisor identity invariants are violated.

  # ─────────────────────────────────────────────────────────────
  # Discovered during soak-window parallel execution 2026-02-07
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-009
    discovered_by: "soak-window-parallel-execution-20260207"
    discovered_at: "2026-02-07"
    type: stale-ssot
    doc: "surfaces/verify/contracts-gate.sh"
    description: |
      contracts-gate.sh (T5) referenced contracts at docs/ (e.g. docs/RECEIPTS_CONTRACT.md)
      but canonical files live at docs/core/ (e.g. docs/core/RECEIPTS_CONTRACT.md).
      Path mismatch caused ops verify to fail at foundation gate. The files exist;
      the gate paths were stale.
    severity: high
    status: fixed
    fixed_in: "soak-window-parallel-execution-20260207"
    notes: "Fixed contracts-gate.sh paths: docs/ → docs/core/ for all three contracts"

  - id: GAP-OP-010
    discovered_by: "soak-window-parallel-execution-20260207"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "ops/bindings/ssh.targets.yaml"
    description: |
      media-stack host has docker.compose.targets.yaml binding (ssh_target: media-stack)
      but no corresponding entry in ssh.targets.yaml. Prevents governed SSH access
      for media stack RCA work.
    severity: medium
    status: fixed
    fixed_in: "soak-window-parallel-execution-20260207"
    notes: "Added media-stack target (100.117.1.53, root, shop/pve) to ssh.targets.yaml"

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-HOST-CANONICALIZATION-20260207
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-011
    discovered_by: "LOOP-HOST-CANONICALIZATION-20260207"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "surfaces/verify/drift-gate.sh"
    description: |
      Hidden-root coverage missing from active drift gates. host.drift.audit
      passes while unmanaged hidden roots and a secret-bearing legacy file
      (~/.config/ronny-ops/env.sh) exist at home root. No gate enforces
      hidden-root inventory or forbidden pattern scanning.
    severity: high
    status: fixed
    fixed_in: "LOOP-HOST-CANONICALIZATION-20260207"
    notes: "D41 hidden-root governance lock + D42 code path case lock + env.sh deleted + backups quarantined"

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-INFRA-CADDY-AUTH-20260207 pre-work
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-012
    discovered_by: "LOOP-INFRA-CADDY-AUTH-20260207"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "Infisical project: infrastructure (prod)"
    description: |
      Required Authentik bootstrap secrets are not present in Infisical
      infrastructure/prod: AUTHENTIK_SECRET_KEY and AUTHENTIK_DB_PASSWORD.
      This blocks safe execute of staged caddy-auth stack deployment.
    severity: high
    status: fixed
    fixed_in: "LOOP-INFRA-CADDY-AUTH-20260207"
    notes: "Created in infrastructure/prod at /spine/vm-infra/caddy-auth (no root-path duplicates)."

  - id: GAP-OP-013
    discovered_by: "LOOP-INFRA-CADDY-AUTH-20260207"
    discovered_at: "2026-02-07"
    type: duplicate-truth
    doc: "Infisical project: infrastructure (prod)"
    description: |
      Infrastructure project still stores legacy keys at root path `/` (flat
      namespace). This risks future collisions with new VM-infra secrets unless
      all new infra keys are constrained under `/spine/*` namespaces.
    severity: medium
    status: fixed
    fixed_in: "secrets-namespace-migration-20260207"
    notes: |
      Enforcement added: secrets.namespace.status + secrets.namespace.policy freeze.
      Migration map staged at ops/staged/SECRETS_NAMESPACE_MIGRATION_MAP.md.
      P1 complete for /spine/platform/security (9 root duplicates removed) and
      P2 complete for /spine/network/edge (10 root duplicates removed) and
      P3 complete for /spine/storage/nas (6 root duplicates removed) and
      P4 complete for /spine/integrations/commerce-mail (9 root duplicates removed) and
      P5 complete for /spine/services/* (11 root duplicates removed) and
      P6 complete for /spine/ai/providers (4 root duplicates removed).
      Root key count reduced from baseline 49 -> 0 (49 removed).
      Evidence: ops/staged/SECRETS_NAMESPACE_P1_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P2_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P3_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P4_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P5_EXECUTION_20260207.md,
      ops/staged/SECRETS_NAMESPACE_P6_EXECUTION_20260207.md,
      RCAP-20260207-171547__secrets.p2.root_cleanup.execute__Rchmg11028,
      RCAP-20260207-180752__secrets.namespace.status__R4k6j64294.

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-GOV-CLI-TOOL-DISCOVERY-20260207
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-014
    discovered_by: "LOOP-GOV-CLI-TOOL-DISCOVERY-20260207"
    discovered_at: "2026-02-07"
    type: agent-behavior
    doc: null
    description: |
      Agent could not discover qrencode despite it being registered in
      maker.tools.inventory.yaml. The tool is domain-scoped (maker plugin)
      with no cross-domain discovery path. Session protocol, brain context,
      and rules all lack tool discovery guidance. Agent searched workbench
      repo, MCP catalogs, and file contents but never found the binding.
    severity: medium
    status: fixed
    fixed_in: "LOOP-GOV-CLI-TOOL-DISCOVERY-20260207"
    notes: |
      Fixed by: cli.tools.inventory.yaml (cross-domain catalog),
      generate-context.sh (tool injection into agent context),
      D44 drift gate (discovery chain validation),
      SESSION_PROTOCOL.md + rules.md updates (agent guidance).

  # ─────────────────────────────────────────────────────────────
  # Discovered during OL_HOME_BASELINE_FINISH audit 2026-02-07
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-015
    discovered_by: "OL_HOME_BASELINE_FINISH"
    discovered_at: "2026-02-07"
    type: runtime-bug
    doc: "docs/governance/MINILAB_SSOT.md"
    description: |
      PVE node-name mismatch on proxmox-home: hostname changed from `pve` to
      `proxmox-home` (GAP-OP-005 fix) but PVE node was not migrated. All VM/LXC
      configs live under /etc/pve/nodes/pve/ while PVE tools look under
      /etc/pve/nodes/proxmox-home/. Breaks qm list, pct list, pct exec, vzdump
      jobs. VMs running pre-rename continue but are unmanageable. VM 101 (immich)
      is stopped and cannot be restarted.
    severity: critical
    status: fixed
    fixed_in: "gap-sweep-20260208"
    notes: "Fixed: hostname reverted to pve (matching PVE node name). qm list, pct list, vzdump all functional. Tailscale hostname remains proxmox-home (independent). Exception recorded in naming.policy.yaml."

  - id: GAP-OP-017
    discovered_by: "LOOP-GOV-CLAUDE-ENTRYPOINT-LOCK-20260208"
    discovered_at: "2026-02-08"
    type: duplicate-truth
    doc: "~/.claude/CLAUDE.md"
    description: |
      Claude home config (CLAUDE.md, commands/ctx.md, settings.json, settings.local.json)
      contains standalone governance sections (Authority Order, Immutable Invariants, etc.)
      and uppercase ~/Code/ path references. No Claude-equivalent of D32 (Codex instruction
      source lock) exists. Runtime scripts reference .brain/ paths but no .brain/ directory
      exists — actual files live at docs/brain/. SESSION_PROTOCOL.md also references .brain/.
    severity: medium
    status: fixed
    fixed_in: "LOOP-GOV-CLAUDE-ENTRYPOINT-LOCK-20260208"
    notes: |
      Fixed by: CLAUDE.md rewritten to redirect shim, path case canonicalized across
      all governed Claude files, .brain/ references replaced with docs/brain/ in runtime
      scripts and SESSION_PROTOCOL.md, D46 + D47 drift gates added, agent.entrypoint.lock.yaml
      binding created, host-claude-entrypoint-lock plugin with status/enforce/execute modes.

  # ─────────────────────────────────────────────────────────────
  # Discovered during PVE baseline SSH audit 2026-02-07
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-018
    discovered_by: "OL_SHOP_BASELINE_FINISH"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "/etc/pve/jobs.cfg"
    description: |
      VM 204 (infra-core) is not included in the vzdump backup job on pve.
      The job covers VMs 200-203 but was created before VM 204 existed.
      infra-core runs cloudflared, pihole, infisical, vaultwarden, caddy,
      and authentik — all critical services with no hypervisor-level backup.
    severity: high
    status: fixed
    fixed_in: "LOOP-INFRA-SSOT-ALIGNMENT-20260208"
    notes: |
      Fixed: added vmid 204 to vzdump job in /etc/pve/jobs.cfg via SSH.
      Job now covers 200,201,202,203,204. Add VM 205 when provisioned.

  - id: GAP-OP-019
    discovered_by: "OL_SHOP_BASELINE_FINISH"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: "pve crontab"
    description: |
      No ZFS scrub scheduled for the media pool on pve. Only the tank pool
      has a weekly scrub (`0 3 * * 0 zpool scrub tank`). The media pool's
      last scrub was CANCELED on 2026-01-11. Media pool is RAIDZ1 with 4x
      Seagate ST8000AS0002 (SMR archive drives) — higher risk profile than
      tank (RAIDZ2 enterprise SAS).
    severity: medium
    status: fixed
    fixed_in: "LOOP-INFRA-SSOT-ALIGNMENT-20260208"
    notes: |
      Fixed: added `0 4 * * 0 zpool scrub media` to pve crontab via SSH.
      Runs Sundays at 4am, offset from tank scrub (3am).

  # ─────────────────────────────────────────────────────────────
  # Discovered during LOOP-MEDIA-STACK-RCA-20260205 / ARCH-20260208
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-021
    discovered_by: "LOOP-MEDIA-STACK-RCA-20260205"
    discovered_at: "2026-02-07"
    type: agent-behavior
    doc: "ops/staged/MEDIA_RCA_DECISION_NOTE.md"
    description: |
      RCA decision note stated /opt/appdata/*.db files were "stale copies, not
      actively used." In reality these were active symlink targets created Dec 23,
      serving all 5 main databases. An agent following the decision note would have
      assumed the entire database layer was on NFS and may have attempted redundant
      or harmful migrations. The inaccuracy persisted for 2 days before correction.
    severity: medium
    status: fixed
    fixed_in: "LOOP-MEDIA-STACK-ARCH-20260208"
    notes: |
      Root cause: initial RCA SSH session saw files at /opt/appdata/ and assumed
      copies without checking symlink state (ls -la vs ls -l). Fixed: decision note
      corrected. Lesson: always use `ls -la` or `file` command to distinguish
      symlinks from regular files during NFS topology discovery.

  - id: GAP-OP-022
    discovered_by: "LOOP-MEDIA-STACK-ARCH-20260208"
    discovered_at: "2026-02-08"
    type: agent-behavior
    doc: null
    description: |
      Phase A symlink migration broke trailarr and posterizarr because their
      compose services lacked the /opt/appdata:/opt/appdata bind mount that
      radarr/sonarr/lidarr/prowlarr/jellyfin already had (Dec 23 setup).
      The playbook did not include a pre-check for container mount visibility.
      Failure mode: sqlite3.OperationalError with no mention of symlinks,
      making diagnosis non-obvious.
    severity: medium
    status: fixed
    fixed_in: "LOOP-MEDIA-STACK-ARCH-20260208"
    notes: |
      Fixed: added /opt/appdata:/opt/appdata volume to trailarr and posterizarr
      in docker-compose.yml on media-stack, containers recreated via compose up -d.
      Lesson codified in docs/brain/lessons/MEDIA_STACK_LESSONS.md.
      Future mitigation: any symlink migration playbook MUST include a
      pre-flight step: `docker inspect <service> --format '{{range .Mounts}}...'`
      to verify the symlink target path is visible inside the container.

  - id: GAP-OP-023
    discovered_by: "LOOP-MEDIA-STACK-ARCH-20260208"
    discovered_at: "2026-02-08"
    type: missing-entry
    doc: "ops/staged/MEDIA_STACK_ARCH_PHASE_A_PLAYBOOK_20260207.md"
    description: |
      Phase A playbook listed 5 target services but missed sonarr logs.db (2.2MB,
      constant writes) which was also on NFS. The playbook was written from the
      ARCH scope doc which only listed "databases STILL on NFS" but omitted logs.db
      files for services whose main .db was already symlinked. Discovery chain:
      scope doc partial → playbook incomplete → extra target found during execution.
    severity: low
    status: fixed
    fixed_in: "LOOP-MEDIA-STACK-ARCH-20260208"
    notes: |
      sonarr logs.db migrated during Phase A execution. Lesson: when scoping
      NFS-to-local migrations, enumerate ALL .db files per service (not just the
      main database). Use: find /mnt/docker/volumes/<service> -name '*.db' -type f

  - id: GAP-OP-024
    discovered_by: "LOOP-MEDIA-STACK-ARCH-20260208"
    discovered_at: "2026-02-08"
    type: agent-behavior
    doc: "ops/staged/MEDIA_STACK_ARCH_PHASE_A_PLAYBOOK_20260207.md"
    description: |
      Phase A playbook listed introskipper.db path as
      /mnt/docker/volumes/jellyfin/config/data/introskipper.db but actual path was
      /mnt/docker/volumes/jellyfin/config/data/introskipper/introskipper.db (extra
      directory level). Path was assumed from scope doc without SSH verification.
      Similarly, trailarr had a logs/logs.db not mentioned in the playbook.
    severity: low
    status: fixed
    fixed_in: "LOOP-MEDIA-STACK-ARCH-20260208"
    notes: |
      Correct paths discovered via `find` during execution. Lesson: playbooks for
      remote file operations MUST include a preflight `find` or `ls -la` step to
      confirm actual paths before mutation. Never trust doc-assumed paths for
      file-level operations.

  - id: GAP-OP-016
    discovered_by: "OL_HOME_BASELINE_FINISH"
    discovered_at: "2026-02-07"
    type: missing-entry
    doc: null
    description: |
      No naming governance policy exists. Hostname, PVE node name, Tailscale
      hostname, ssh.targets.yaml ID, and DEVICE_IDENTITY_SSOT entries are all
      managed independently with no canonical mapping or rename procedure.
      This caused GAP-OP-015 (hostname changed without PVE node migration)
      and has caused repeated confusion (media-stack site attribution, hypervisor
      identity ambiguity).
    severity: high
    status: fixed
    fixed_in: "LOOP-NAMING-GOVERNANCE-20260207"
    notes: |
      Fixed by: naming.policy.yaml (canonical names + surface authority + rename
      procedure + allowed exceptions) + D45 naming consistency lock gate
      (cross-file verification wired into drift-gate.sh). Also fixed vault kind
      misclassification (lxc→vm) in placement policy and DEVICE_IDENTITY_SSOT.

  # ─────────────────────────────────────────────────────────────
  # Discovered during gap sweep 2026-02-08
  # ─────────────────────────────────────────────────────────────

  - id: GAP-OP-020
    discovered_by: "gap-sweep-20260208"
    discovered_at: "2026-02-08"
    type: runtime-bug
    doc: "ops/commands/loops.sh"
    description: |
      ops loops list --open crashes with jq error at open_loops.jsonl:121.
      Close records use "id" key but the parser expects "loop_id", producing
      "Cannot index object with null". Additionally, open_loops.jsonl has
      54 open entries with massive duplication and 3 close records that never
      filter out closed loops. The loops subsystem is the spine's work-tracking
      surface and currently fails on every invocation.
    severity: high
    status: fixed
    fixed_in: "gap-sweep-20260208"
    notes: |
      Fixed: loops.sh jq filter now normalizes close records, deduplicates by loop_id,
      and applies close status. open_loops.jsonl compacted from 121 to 61 lines.
      6 completed loops closed. 9 open loops remain.

# ─────────────────────────────────────────────────────────────
# Gap Types
# ─────────────────────────────────────────────────────────────
#
# stale-ssot      - Doc has outdated information
# missing-entry   - Expected entry doesn't exist
# agent-behavior  - Agent pattern that caused friction (not a doc issue)
# unclear-doc     - Doc exists but is ambiguous or incomplete
# duplicate-truth - Multiple docs claim authority for same thing
# runtime-bug     - Runtime implementation behavior violates governance intent
